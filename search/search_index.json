{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Docker, Virtual Machines & Networks","text":"<p>Welcome! Use the sidebar to navigate to different sections of this project.</p>"},{"location":"#docker","title":"Docker","text":"<ul> <li>Docker Build</li> <li>Docker Networking</li> <li>Docker Macvlan Network</li> <li>Communication Between Inside &amp; Outside Docker</li> <li>Docker Container Interfaces</li> <li>Docker Engine on Ubuntu</li> <li>Using tarballs (.tgz)</li> <li>Docker Compose</li> <li>Docker Volumes</li> <li>Docker Space Optimization</li> </ul>"},{"location":"#virtualization","title":"Virtualization","text":"<ul> <li>VM-to-VM Communication</li> <li>Fix Slow Ubuntu VM Boot</li> <li>Recovering VM</li> </ul>"},{"location":"#tools","title":"Tools","text":"<ul> <li>SSH Setup</li> <li>Matlab Setup</li> <li>Matlab_Docker</li> <li>Wine on Linux</li> <li>SQL</li> <li>[PostgreSQL] (PostgreSQL.md)</li> <li>Matlab Data Sources</li> <li>Remote Port Forwarding SSH</li> <li>NAT_Adapter_Iptables_Forwarding.md</li> <li>Bridged_Adapter</li> <li>File Transfer Host To Remote</li> <li>Systemctl Systemd</li> <li>Ubuntu Python Packages</li> <li>Connecting DB IEDs PLCs</li> <li>Routes</li> </ul>"},{"location":"#scada","title":"SCADA","text":"<ul> <li>ScadaLTS</li> </ul>"},{"location":"#os","title":"OS","text":"<ul> <li>Mac Notes</li> </ul>"},{"location":"#system-administration","title":"System Administration","text":"<ul> <li>Linux System Administration</li> <li>Networking Fundamentals</li> <li>Troubleshooting Tools</li> </ul>"},{"location":"Bridged_Adapter/","title":"Bridged Adapter","text":""},{"location":"Bridged_Adapter/#how-do-you-know-that-enp0s3-is-the-bridged-adapter","title":"How do you know that enp0s3 is the bridged adapter?","text":"<p>Excellent question \u2014 and one that a lot of people get tripped up on when mixing VirtualBox/VMware networking modes with Docker networking.</p> <p>Let\u2019s go step by step to make it crystal clear why we can conclude that <code>enp0s3</code> is the interface tied to the bridged adapter in your Ubuntu VM setup.</p>"},{"location":"Bridged_Adapter/#step-1-how-virtualbox-or-other-hypervisors-name-interfaces","title":"\ud83e\udde9 Step 1. How VirtualBox (or other hypervisors) name interfaces","text":"<p>When you create a VM in VirtualBox, each \u201cAdapter\u201d (NAT, Bridged, Host-only, etc.) is exposed inside the guest OS (Ubuntu) as a network interface \u2014 typically named:</p> <pre><code>enp0s3, enp0s8, enp0s9, etc.\n</code></pre> <p>The first adapter (Adapter 1) almost always appears as <code>enp0s3</code> inside the VM. So if Adapter 1 in VirtualBox is configured as Bridged Adapter, <code>enp0s3</code> is the bridged interface.</p>"},{"location":"Bridged_Adapter/#step-2-confirming-with-your-vms-ip","title":"\ud83e\udde9 Step 2. Confirming with your VM\u2019s IP","text":"<p>You ran:</p> <pre><code>ip addr show enp0s3\n</code></pre> <p>and got:</p> <pre><code>inet 192.168.30.60/24 brd 192.168.30.255 scope global enp0s3\n</code></pre> <p>Your Remote Desktop network and host network are also on the <code>192.168.30.0/24</code> subnet. \u27a1\ufe0f That means <code>enp0s3</code> is connected to the same LAN as your host and remote desktop \u2014 typical of a Bridged Adapter connection.</p> <p>If it were a NAT adapter, you\u2019d see a different subnet, like <code>10.0.2.x</code> (the default NAT range VirtualBox uses).</p> <p>\u2705 Therefore: The fact that <code>enp0s3</code> has <code>192.168.30.60</code> \u2014 a LAN IP \u2014 means it\u2019s bridged to your physical network card.</p>"},{"location":"Bridged_Adapter/#step-3-how-you-used-it-in-docker","title":"\ud83e\udde9 Step 3. How you used it in Docker","text":"<p>You created the Docker macvlan using:</p> <pre><code>docker network create -d macvlan \\\n  --subnet=192.168.40.0/24 \\\n  --gateway=192.168.40.1 \\\n  -o parent=enp0s3 \\\n  plc-macvlan-40\n</code></pre> <p>The <code>-o parent=enp0s3</code> tells Docker:</p> <p>\u201cAttach this macvlan network to the same Layer 2 segment as <code>enp0s3</code>.\u201d</p> <p>Since <code>enp0s3</code> is your bridged adapter to the real network, each container\u2019s IP (e.g., <code>192.168.40.201</code>) appears as a separate device on that LAN (from the VM\u2019s perspective).</p>"},{"location":"Bridged_Adapter/#step-4-how-you-verified-it-works","title":"\ud83e\udde9 Step 4. How you verified it works","text":"<p>You were able to access PLC containers from outside (your host):</p> <pre><code>http://192.168.30.60:8081/dashboard\n</code></pre> <p>That\u2019s only possible because:</p> <ul> <li><code>enp0s3</code> connects your Ubuntu VM directly to the same physical network as your host and remote desktop (bridged).</li> <li>iptables DNAT forwards traffic from <code>192.168.30.60</code> (on enp0s3) \u2192 PLC containers (on macvlan).</li> </ul> <p>If <code>enp0s3</code> were a NAT adapter, your host could not reach <code>192.168.30.60:8081</code> directly.</p>"},{"location":"Bridged_Adapter/#summary","title":"\u2705 Summary","text":"Interface Source Meaning <code>enp0s3</code> First network adapter in VirtualBox This is the Bridged Adapter connected to your LAN <code>192.168.30.60</code> IP address assigned to <code>enp0s3</code> Same subnet as your Remote Desktop and host \u2014 confirms it\u2019s bridged Docker parent <code>-o parent=enp0s3</code> Macvlan attaches to this physical NIC <code>192.168.40.0/24</code> macvlan subnet Separate subnet for Docker containers iptables rules Map 8081\u20138084 to 192.168.40.201\u2013204 Enable access from LAN to macvlan network via enp0s3"},{"location":"Bridged_Adapter/#tldr","title":"\ud83e\udde0 TL;DR","text":"<p>You know <code>enp0s3</code> is the bridged adapter because:</p> <ol> <li>It\u2019s the first adapter (VirtualBox convention).</li> <li>It has an IP in your LAN subnet (<code>192.168.30.x</code>).</li> <li>Traffic from your host can reach it directly.</li> <li>Your iptables DNAT setup works only because it\u2019s bridged.</li> </ol> <p>If you want, I can show you exactly how to verify this conclusively with a one-liner using:</p> <pre><code>ip route show default\n</code></pre> <p>Would you like me to include that check?</p>"},{"location":"Bridged_Adapter/#on-my-own-host-machine-where-i-access-the-plc-browsers-using-http19216830608081dashboard-i-do-not-see-enp0s3-or-any-interface-that-matches-the-subnet-of-the-vm-on-the-remote-desktop-explain-further-how-my-host-machine-is-able-to-access-the-ubuntu-vms-ip-and-the-docker-container-running-in-the-ubuntu-vm-when-there-is-no-corresponding-interface-or-correct-me-if-there-is-a-corresponding-interface-to-access-the-1921683060-on-enp0s3-plc-container","title":"On my own host machine where I access the plc browsers using http://192.168.30.60:8081/dashboard , I do not see enp0s3 or any interface that matches the subnet of the VM on the Remote Desktop , explain further how my host machine is able to access the ubuntu vm's ip and the docker container running in the Ubuntu vm when there is no corresponding interface, or correct me if there is a corresponding interface to access the 192.168.30.60 (on enp0s3) \u2192 PLC container","text":"<pre><code>Last login: Fri Oct 17 17:38:12 on ttys032\nsreedevirajavelu@iTrust-9000003511 ~ % ifconfig\nlo0: flags=8049&lt;UP,LOOPBACK,RUNNING,MULTICAST&gt; mtu 16384\n    options=1203&lt;RXCSUM,TXCSUM,TXSTATUS,SW_TIMESTAMP&gt;\n    inet 127.0.0.1 netmask 0xff000000\n    inet6 ::1 prefixlen 128 \n    inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\ngif0: flags=8010&lt;POINTOPOINT,MULTICAST&gt; mtu 1280\nstf0: flags=0&lt;&gt; mtu 1280\nanpi0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=400&lt;CHANNEL_IO&gt;\n    ether e6:59:2d:4a:00:b0\n    media: none\n    status: inactive\nanpi1: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=400&lt;CHANNEL_IO&gt;\n    ether e6:59:2d:4a:00:b1\n    media: none\n    status: inactive\nen3: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=400&lt;CHANNEL_IO&gt;\n    ether e6:59:2d:4a:00:90\n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: none\n    status: inactive\nen4: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=400&lt;CHANNEL_IO&gt;\n    ether e6:59:2d:4a:00:91\n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: none\n    status: inactive\nen1: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=460&lt;TSO4,TSO6,CHANNEL_IO&gt;\n    ether 36:e3:5e:f0:6d:80\n    media: autoselect &lt;full-duplex&gt;\n    status: inactive\nen2: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=460&lt;TSO4,TSO6,CHANNEL_IO&gt;\n    ether 36:e3:5e:f0:6d:84\n    media: autoselect &lt;full-duplex&gt;\n    status: inactive\nbridge0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=63&lt;RXCSUM,TXCSUM,TSO4,TSO6&gt;\n    ether 36:e3:5e:f0:6d:80\n    Configuration:\n        id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0\n        maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200\n        root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0\n        ipfilter disabled flags 0x0\n    member: en1 flags=3&lt;LEARNING,DISCOVER&gt;\n            ifmaxaddr 0 port 8 priority 0 path cost 0\n    member: en2 flags=3&lt;LEARNING,DISCOVER&gt;\n            ifmaxaddr 0 port 9 priority 0 path cost 0\n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: &lt;unknown type&gt;\n    status: inactive\nap1: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=6460&lt;TSO4,TSO6,CHANNEL_IO,PARTIAL_CSUM,ZEROINVERT_CSUM&gt;\n    ether b6:9a:14:e0:e6:d7\n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: autoselect (none)\n    status: inactive\nen0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=6460&lt;TSO4,TSO6,CHANNEL_IO,PARTIAL_CSUM,ZEROINVERT_CSUM&gt;\n    ether 4a:e2:04:eb:3b:e2\n    inet6 fe80::871:f226:16fb:5018%en0 prefixlen 64 secured scopeid 0xb \n    inet 10.34.11.15 netmask 0xffffe000 broadcast 10.34.31.255\n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: autoselect\n    status: active\nawdl0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=6460&lt;TSO4,TSO6,CHANNEL_IO,PARTIAL_CSUM,ZEROINVERT_CSUM&gt;\n    ether 7e:e4:55:0f:aa:48\n    inet6 fe80::7ce4:55ff:fe0f:aa48%awdl0 prefixlen 64 scopeid 0xd \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: autoselect\n    status: active\nllw0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=400&lt;CHANNEL_IO&gt;\n    ether 7e:e4:55:0f:aa:48\n    inet6 fe80::7ce4:55ff:fe0f:aa48%llw0 prefixlen 64 scopeid 0xe \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: autoselect (none)\nutun0: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1500\n    inet6 fe80::e4aa:ba08:b55a:fee%utun0 prefixlen 64 scopeid 0xf \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\nutun1: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1380\n    inet6 fe80::6ff9:4e20:6ac1:7bb7%utun1 prefixlen 64 scopeid 0x10 \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\nutun2: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 2000\n    inet6 fe80::c335:87af:5360:d8cd%utun2 prefixlen 64 scopeid 0x11 \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\nutun3: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1000\n    inet6 fe80::ce81:b1c:bd2c:69e%utun3 prefixlen 64 scopeid 0x12 \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\nutun4: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1500\n    inet 200.10.30.154 --&gt; 200.10.30.153 netmask 0xfffffffc\nsreedevirajavelu@iTrust-9000003511 ~ % \n</code></pre> <p>Excellent, detailed observation \u2014 and this is one of those subtle but really interesting networking questions where you\u2019re absolutely right to notice:</p> <p>\u201cWait, I don\u2019t see any 192.168.30.x interface on my host \u2014 so how am I even reaching 192.168.30.60?\u201d</p> <p>Let\u2019s unpack exactly why you can access the Ubuntu VM (192.168.30.60) and its PLC containers even though your host has no visible interface on that subnet.</p>"},{"location":"Bridged_Adapter/#the-key-is-the-remote-desktop-rdp-network-path","title":"\ud83e\udde9 The key is the Remote Desktop (RDP) network path","text":"<p>Here\u2019s the hierarchy you described:</p> <pre><code>[Your MacBook or Host Machine]  \u2190 (RDP connection) \u2192  [Remote Desktop: 192.168.30.5]\n       \u2502                                               \u2502\n       \u2502                                               \u2514\u2500\u2500 (Ubuntu VM: 192.168.30.60)\n       \u2502                                                        \u251c\u2500\u2500 (Docker macvlan: 192.168.40.0/24)\n       \u2502                                                        \u2514\u2500\u2500 iptables DNAT forwarding 8081\u20138084\n</code></pre>"},{"location":"Bridged_Adapter/#whats-actually-happening","title":"\ud83e\udde0 What\u2019s actually happening","text":"<p>You\u2019re not accessing 192.168.30.60 directly from your Mac\u2019s local interface. Instead, your RDP (Remote Desktop Protocol) session to <code>192.168.30.5</code> acts like a portal into that internal network (192.168.30.0/24).</p> <p>Your Mac likely has:</p> <ul> <li>An active VPN or corporate tunnel interface (e.g., <code>utun4</code> showing <code>200.10.30.154</code>)</li> <li>That VPN provides routing to internal subnets like <code>192.168.30.0/24</code>.</li> </ul> <p>So when you go to:</p> <pre><code>http://192.168.30.60:8081/dashboard\n</code></pre> <p>\u2192 the traffic goes through your VPN or RDP tunnel, into the Remote Desktop network, and reaches the Ubuntu VM.</p>"},{"location":"Bridged_Adapter/#why-you-dont-see-19216830x-in-ifconfig","title":"\ud83e\uddf1 Why you don\u2019t see 192.168.30.x in <code>ifconfig</code>","text":"<p>Your Mac only lists its local interfaces \u2014 those physically present or logically connected.</p> <p>But your connection to <code>192.168.30.60</code> doesn\u2019t require a local interface on that subnet because:</p> <ul> <li>There\u2019s a route (via VPN, RDP tunnel, or NAT) that sends traffic to that subnet indirectly.</li> <li>macOS knows: \u201cif I need to reach 192.168.30.60, send packets via my VPN interface (utun4).\u201d</li> </ul> <p>You can confirm this routing path using:</p> <pre><code>netstat -rn | grep 192.168.30\n</code></pre> <p>or</p> <pre><code>route get 192.168.30.60\n</code></pre> <p>\u2705 You\u2019ll likely see something like:</p> <pre><code>route to: 192.168.30.60\ndestination: 192.168.30.0\ngateway: 200.10.30.153\ninterface: utun4\n</code></pre> <p>That shows your packets go via <code>utun4</code> \u2014 your VPN/tunnel interface \u2014 not a local interface like <code>en0</code>.</p>"},{"location":"Bridged_Adapter/#how-the-access-chain-actually-works","title":"\ud83d\udd04 How the access chain actually works","text":"<p>Let\u2019s trace your connection when you visit <code>http://192.168.30.60:8081/dashboard</code>:</p> <ol> <li>Your browser on the host \u2192 sends TCP packets to 192.168.30.60:8081</li> <li>macOS routing \u2192 sees that 192.168.30.0/24 is reachable via <code>utun4</code> (VPN or remote tunnel)</li> <li>Traffic enters the remote LAN (the same network where your Remote Desktop 192.168.30.5 lives)</li> <li>VM\u2019s bridged adapter enp0s3 (192.168.30.60) receives the traffic</li> <li>iptables DNAT in the Ubuntu VM forwards:</li> </ol> <p><code>192.168.30.60:8081 \u2192 192.168.40.201:8080</code></p> <p>and so on for the other PLCs. 6. Docker macvlan interface passes it to the correct PLC container 7. Response travels back \u2192 through the same path \u2192 appears in your browser.</p> <p>So even though your host doesn\u2019t have a <code>192.168.30.x</code> interface, it can still reach it via a routed or tunneled network.</p>"},{"location":"Bridged_Adapter/#analogy","title":"\ud83e\udded Analogy","text":"<p>Think of it like this:</p> <p>You can visit a friend in another city without living there \u2014 because the highway (your VPN or RDP network) connects your location to theirs.</p> <p>Your host\u2019s local \u201caddress\u201d (10.x.x.x) doesn\u2019t matter \u2014 the route takes care of the connection.</p>"},{"location":"Bridged_Adapter/#summary_1","title":"\u2705 Summary","text":"Observation Explanation No 192.168.30.x interface on host Host doesn\u2019t need one \u2014 routing or VPN handles connectivity Can access <code>192.168.30.60:8081</code> Because traffic goes through VPN or RDP network that knows that subnet Ubuntu VM\u2019s <code>enp0s3</code> = 192.168.30.60 Bridged to the same LAN as the Remote Desktop Docker macvlan <code>192.168.40.x</code> Secondary subnet for PLC containers iptables DNAT Forwards ports 8081\u20138084 \u2192 specific PLC IPs Your host access works Due to routed connectivity through VPN or RDP\u2019s internal network <p>Would you like me to diagram this path visually \u2014 showing how the traffic travels from your Mac\u2019s interface (utun4) \u2192 Remote Desktop network (192.168.30.0/24) \u2192 Ubuntu VM \u2192 PLC container? It would help make this chain very intuitive.</p> <pre><code>Last login: Fri Oct 17 17:39:55 on ttys032\nsreedevirajavelu@iTrust-9000003511 ~ % netstat -rn | grep 192.168.30\n\n192.168.30         200.10.30.153      UGSc                utun4       \nsreedevirajavelu@iTrust-9000003511 ~ % \n</code></pre> <pre><code>sreedevirajavelu@iTrust-9000003511 ~ % route get 192.168.30.60\n\n   route to: 192.168.30.60\ndestination: 192.168.30.60\n    gateway: static-tu-200-10-30-153.superredes.net\n  interface: utun4\n      flags: &lt;UP,GATEWAY,HOST,DONE,WASCLONED,IFSCOPE,IFREF&gt;\n recvpipe  sendpipe  ssthresh  rtt,msec    rttvar  hopcount      mtu     expire\n       0         0         0         0         0         0      1500         0 \nsreedevirajavelu@iTrust-9000003511 ~ % \n</code></pre>"},{"location":"CommunicationBetweenInsideAndOutsideDocker/","title":"Communication Inside & Outside Docker","text":"<p>what does this mean and should I try connecting to this ip address if I have an sql database in a docker container on Macbook and I'm trying to connect to it from a python script inside an ubuntu vm in virtual box</p> <p>sreedevirajavelu@srees-MacBook-Air ~ % ipconfig getifaddr en0</p> <p>10.34.11.15 sreedevirajavelu@srees-MacBook-Air ~ % </p> <p>The IP address <code>10.34.11.15</code> is your MacBook\u2019s local network IP address on the <code>en0</code> interface (usually Wi-Fi).</p>"},{"location":"CommunicationBetweenInsideAndOutsideDocker/#what-this-means-for-your-setup","title":"What this means for your setup:","text":"<ul> <li>Your MacBook\u2019s Docker container MySQL is running on <code>localhost</code> inside your MacBook.</li> <li>Your Ubuntu VM in VirtualBox is a separate machine from the MacBook host.</li> <li>The VM needs to reach the Docker container through your MacBook\u2019s network IP (10.34.11.15) if networking between the VM and the Mac is set up properly.</li> </ul>"},{"location":"CommunicationBetweenInsideAndOutsideDocker/#can-you-connect-to-the-mysql-docker-container-on-ip-10341115-from-the-ubuntu-vm","title":"Can you connect to the MySQL Docker container on IP <code>10.34.11.15</code> from the Ubuntu VM?","text":"<p>It depends on the VirtualBox network mode:</p> <ol> <li> <p>NAT (default):</p> </li> <li> <p>The VM is behind a NAT and cannot directly access host IP like <code>10.34.11.15</code>.</p> </li> <li> <p>You would need to set up port forwarding in VirtualBox to forward the host port 3306 to the VM or use other tricks.</p> </li> <li> <p>Bridged Adapter:</p> </li> <li> <p>The VM gets an IP on the same LAN as your MacBook.</p> </li> <li> <p>VM can directly access <code>10.34.11.15</code> on port 3306 if firewall allows.</p> </li> <li> <p>Host-only Adapter:</p> </li> <li> <p>VM and host can communicate using a special virtual network IP (not the LAN IP).</p> </li> <li>You\u2019d check the IP of the host on that virtual network interface.</li> </ol>"},{"location":"CommunicationBetweenInsideAndOutsideDocker/#what-should-you-do","title":"What should you do?","text":"<ul> <li> <p>Check your VM\u2019s network mode in VirtualBox settings.</p> </li> <li> <p>If it\u2019s NAT, either:</p> </li> <li> <p>Switch to Bridged Adapter mode for the VM to be on the same LAN as the Mac, or</p> </li> <li> <p>Set up port forwarding in VirtualBox (host port 3306 \u2192 VM port 3306).</p> </li> <li> <p>Try to ping the MacBook\u2019s IP (<code>10.34.11.15</code>) from inside the Ubuntu VM:</p> </li> </ul> <pre><code>ping 10.34.11.15\n</code></pre> <p>If ping works and firewall allows, you can try to connect to MySQL at <code>10.34.11.15:3306</code> from your Python script running in the VM.</p>"},{"location":"CommunicationBetweenInsideAndOutsideDocker/#summary","title":"Summary","text":"Your Mac IP VM network type Access from VM to MySQL container? <code>10.34.11.15</code> NAT No direct access without port forwarding <code>10.34.11.15</code> Bridged Adapter Yes, direct access if firewall allows VirtualBox host-only IP Host-only Adapter Access via that network IP, not <code>10.34.11.15</code> <p>If you want, I can guide you how to set up VirtualBox network so the VM can access your Docker MySQL on the Mac!</p>"},{"location":"Connecting_DB_IEDs_PLCs/","title":"Connecting DB IEDs PLCs","text":"<p>Below is the exact step-by-step way to dockerize your first IED (ST_IED) using the networking architecture you selected.</p> <p>\u2705 After this, you will be able to repeat the process for GT, EG, HRSG by copying 1 folder and changing: \u2013 the Python script filename \u2013 the Modbus port \u2013 the MACVLAN IP</p> <p>Everything else stays identical.</p>"},{"location":"Connecting_DB_IEDs_PLCs/#part-1-prepare-the-first-ied-for-dockerizing-st-ied","title":"\u2705 PART 1 \u2014 Prepare the first IED for Dockerizing (ST IED)","text":"<p>We will create this structure:</p> <pre><code>st_ied/\n\u2502\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 ST_IED_pg.py\n</code></pre>"},{"location":"Connecting_DB_IEDs_PLCs/#1-create-requirementstxt","title":"\u2705 1. Create <code>requirements.txt</code>","text":"<p><code>requirements.txt</code>:</p> <pre><code>psycopg2\npymodbus\n</code></pre>"},{"location":"Connecting_DB_IEDs_PLCs/#2-create-the-dockerfile","title":"\u2705 2. Create the Dockerfile","text":"<p><code>Dockerfile</code>:</p> <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\n# Install Postgres client dependencies\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    libpq-dev gcc \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt ./\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python3\", \"ST_IED_pg.py\"]\n</code></pre> <p>\u2705 This Dockerfile is reusable for all IEDs. You only replace the Python filename inside <code>CMD</code>.</p>"},{"location":"Connecting_DB_IEDs_PLCs/#3-modify-your-st_ied-python-script-to-use-environment-variables","title":"\u2705 3. Modify your ST_IED Python script to use environment variables","text":"<p>Replace your <code>establish_connection()</code> function with this:</p> <pre><code>import os\n\ndef establish_connection():\n    conn = psycopg2.connect(\n        host=os.getenv(\"DB_HOST\", \"ccp-multi-ied-multi-plc-pg\"),\n        port=os.getenv(\"DB_PORT\", \"5432\"),\n        user=os.getenv(\"DB_USER\", \"root\"),\n        password=os.getenv(\"DB_PASS\", \"root\"),\n        database=os.getenv(\"DB_NAME\", \"powerplant\")\n    )\n    return conn\n</code></pre> <p>\u2705 No more hardcoded IP \u2705 Container resolves Postgres by container name \u2705 DB host changes don\u2019t affect your script anymore</p>"},{"location":"Connecting_DB_IEDs_PLCs/#part-2-create-the-docker-compose-entry-for-st-ied","title":"\u2705 PART 2 \u2014 Create the docker-compose entry for ST IED","text":"<p>Since you already created the two networks:</p> <p>\u2705 <code>ied_pg_net</code> (for DB access) \u2705 <code>plc-macvlan-40</code> (for PLC access)</p> <p>Your docker-compose for one IED looks like this:</p> <p><code>docker-compose.yml</code>:</p> <pre><code>version: \"3.9\"\n\nservices:\n  st_ied:\n    build: ./st_ied\n    container_name: st_ied\n    networks:\n      ied_pg_net: {}                 # bridge network for DB\n      plc-macvlan-40:\n        ipv4_address: 192.168.40.210  # example IP\n    environment:\n      DB_HOST: ccp-multi-ied-multi-plc-pg\n      DB_PORT: \"5432\"\n      DB_USER: root\n      DB_PASS: root\n      DB_NAME: powerplant\n    ports:\n      - \"504:504\"   # expose for debugging (optional)\n      - \"50001:50001\"  # TCP listener (optional)\n\nnetworks:\n  ied_pg_net:\n    external: true\n\n  plc-macvlan-40:\n    external: true\n</code></pre>"},{"location":"Connecting_DB_IEDs_PLCs/#why-this-works","title":"\u2705 Why this works","text":"<p>\u2714 ST IED joins <code>ied_pg_net</code>, so it can reach PostgreSQL by:</p> <pre><code>host = \"ccp-multi-ied-multi-plc-pg\"\n</code></pre> <p>\u2714 ST IED joins <code>plc-macvlan-40</code> with a static IP:</p> <pre><code>192.168.40.210\n</code></pre> <p>\u2714 PLC (at e.g. 192.168.40.202) can reach the IED via Modbus at:</p> <pre><code>192.168.40.210:504\n</code></pre>"},{"location":"Connecting_DB_IEDs_PLCs/#part-3-build-run-your-first-dockerized-ied","title":"\u2705 PART 3 \u2014 Build + Run your first dockerized IED","text":"<p>Build:</p> <pre><code>docker compose build st_ied\n</code></pre> <p>Run:</p> <pre><code>docker compose up -d st_ied\n</code></pre> <p>Check its IP inside the macvlan network:</p> <pre><code>docker inspect st_ied | grep IPAddress\n</code></pre> <p>Check logs:</p> <pre><code>docker logs -f st_ied\n</code></pre> <p>\u2705 If you see <code>[ST] Initial Modbus context values\u2026</code> \u2192 the IED is working \u2705 If you see <code>[ST: DB sync] Updated Modbus reg\u2026</code> \u2192 DB polling works \u2705 PLC should now read registers</p>"},{"location":"Connecting_DB_IEDs_PLCs/#part-4-how-to-dockerize-the-remaining-ieds","title":"\u2705 PART 4 \u2014 How to dockerize the remaining IEDs","text":"<p>Now simply repeat this exact same folder pattern:</p> <pre><code>eg_ied/\n\u2502\u2500\u2500 Dockerfile\n\u2502\u2500\u2500 requirements.txt\n\u2502\u2500\u2500 EG_IED_pg.py\n\ngt_ied/\n\u2502\u2500\u2500 Dockerfile\n\u2502\u2500\u2500 requirements.txt\n\u2502\u2500\u2500 GT_IED_pg.py\n\nhrsg_ied/\n\u2502\u2500\u2500 Dockerfile\n\u2502\u2500\u2500 requirements.txt\n\u2502\u2500\u2500 HRSG_IED_pg.py\n</code></pre> <p>And add each service to docker-compose:</p> <p>\u2705 EG IED Example</p> <pre><code>  eg_ied:\n    build: ./eg_ied\n    container_name: eg_ied\n    networks:\n      ied_pg_net: {}\n      plc-macvlan-40:\n        ipv4_address: 192.168.40.211\n    environment:\n      DB_HOST: ccp-multi-ied-multi-plc-pg\n      DB_PORT: \"5432\"\n      DB_USER: root\n      DB_PASS: root\n      DB_NAME: powerplant\n    ports:\n      - \"505:505\"\n</code></pre> <p>\u2705 GT IED</p> <pre><code>  gt_ied:\n    build: ./gt_ied\n    container_name: gt_ied\n    networks:\n      ied_pg_net: {}\n      plc-macvlan-40:\n        ipv4_address: 192.168.40.212\n    environment:\n      DB_HOST: ccp-multi-ied-multi-plc-pg\n      DB_PORT: \"5432\"\n      DB_USER: root\n      DB_PASS: root\n      DB_NAME: powerplant\n    ports:\n      - \"502:502\"\n</code></pre> <p>\u2705 HRSG IED</p> <pre><code>  hrsg_ied:\n    build: ./hrsg_ied\n    container_name: hrsg_ied\n    networks:\n      ied_pg_net: {}\n      plc-macvlan-40:\n        ipv4_address: 192.168.40.213\n    environment:\n      DB_HOST: ccp-multi-ied-multi-plc-pg\n      DB_PORT: \"5432\"\n      DB_USER: root\n      DB_PASS: root\n      DB_NAME: powerplant\n    ports:\n      - \"503:503\"\n      - \"50000:50000\"\n</code></pre>"},{"location":"Connecting_DB_IEDs_PLCs/#part-5-final-networking-diagram","title":"\u2705 PART 5 \u2014 Final Networking Diagram","text":"<pre><code>         (ied_pg_net)\n  +------------------------------+\n  |    Postgres (container)     |\n  +------------------------------+\n               |\n               |\n         +-----+------+\n         |   IEDs     | (all also join plc-macvlan-40)\n         +------------+\n          |    |    |\n    192.168.40.210  (ST)\n    192.168.40.211  (EG)\n    192.168.40.212  (GT)\n    192.168.40.213  (HRSG)\n          |\n          |  (physical MACVLAN)\n   +------+------------------+\n   |          PLC           |\n   |   192.168.40.202       |\n   +------------------------+\n</code></pre> <p>\u2705 PLC \u2194 IED on MACVLAN subnet \u2705 IED \u2194 Postgres on shared bridge network \u2705 PLCs cannot reach PostgreSQL (secure)</p>"},{"location":"Connecting_DB_IEDs_PLCs/#if-you-want","title":"\u2705 If you want:","text":"<p>I can generate the full production-ready docker-compose containing:</p> <p>\u2705 All 4 IEDs \u2705 Naming conventions \u2705 Restart policies \u2705 Healthchecks \u2705 Logs mounted to local files \u2705 Automatic DB wait-for initialization</p> <p>Just say: \u201cGenerate full multi-IED docker-compose.\u201d</p>"},{"location":"DockerBuild/","title":"Docker Build","text":""},{"location":"DockerBuild/#building-docker-images","title":"Building Docker images","text":"<p><code>docker build --network host --tag database .</code></p> <p>What each part does:</p> <ul> <li> <p><code>docker build</code>: command to build a Docker image</p> </li> <li> <p><code>--network host</code>: uses host's network stack during the build process (allows the build to access network resources as if it were running directly on the host machine)</p> </li> <li> <p><code>--tag database</code>: tags (names) the resulting image as \"database\"</p> </li> <li> <p><code>.</code>: uses current directory as the build context (where Docker looks for the Dockerfile and any files it needs to copy)</p> </li> <li> <p><code>--network host</code>: this flag is often used when the build process needs to download packages or access network resources that might be blocked by Docker's default networking.</p> </li> </ul>"},{"location":"DockerEngine_Ubuntu/","title":"Docker Engine on Ubuntu","text":"<p>Steps to install Docker Engine on ubuntu 24.04.2 :</p> <ul> <li>https://docs.docker.com/engine/install/ubuntu/</li> </ul> <p>Check ubuntu version using:</p> <ul> <li><code>lsb_release -a</code></li> <li><code>cat /etc/os-release</code></li> </ul> <p></p> <p>Check CPU architecture - <code>uname -m</code></p> <ul> <li>x86_64 \u2192 64-bit Intel/AMD (amd64) \u2705 supported</li> <li>aarch64 \u2192 64-bit ARM (arm64) \u2705 supported</li> <li>armv7l \u2192 32-bit ARM \u274c not supported by Docker Engine</li> <li>s390x, ppc64le \u2192 supported but only on special hardware</li> </ul> <p>On a VirtualBox VM on a MacBook, you\u2019ll almost certainly see x86_64 if your Mac is Intel-based, or aarch64 if your MacBook is Apple Silicon (M1/M2/M3).</p> <p></p> <p>Steps taken from https://docs.docker.com/engine/install/ubuntu/ </p> <p>Run the following command to uninstall all conflicting packages:</p> <pre><code>for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n</code></pre> <p>Install using the apt repository</p> <p>Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker apt repository. Afterward, you can install and update Docker from the repository.</p> <ol> <li>Set up Docker's apt repository.</li> </ol> <pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n</code></pre> <ol> <li>Install the Docker packages</li> </ol> <p>To install the latest version, run:</p> <pre><code>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>Note: The Docker service starts automatically after installation. To verify that Docker is running, use:</p> <p><code>sudo systemctl status docker</code></p> <p>Some systems may have this behavior disabled and will require a manual start:</p> <p><code>sudo systemctl start docker</code></p>"},{"location":"DockerEngine_Ubuntu/#issues-and-fixes","title":"Issues and Fixes","text":"<p><code>docker --version</code> works but </p> <p>for <code>docker ps</code> gives permission denied error meaning current user ubuntu does not have permission to access the Docker socket </p> <p>Solution: </p> <ol> <li>Check the socket permissions </li> </ol> <p><code>ls -l /var/run/docker.sock</code></p> <p>If you see something like: <code>srw-rw---- 1 root docker 0 Oct 10 13:20 /var/run/docker.sock</code></p> <p>That means only users in the docker group can use it.</p> <ol> <li>Add your user to the docker group</li> </ol> <p><code>sudo usermod -aG docker $USER</code></p> <p>Then log out and log back in - the new group membership only takes effect after re-login</p>"},{"location":"Docker_Compose/","title":"Docker Compose","text":""},{"location":"Docker_Compose/#difference-between-docker-compose-up-with-hyphen-and-docker-compose-up-d-no-hyphen-between-docker-and-compose","title":"Difference between docker-compose up (with hyphen) and docker compose up -d (no hyphen between docker and compose )","text":""},{"location":"Docker_Compose/#docker-compose-up-with-hyphen","title":"docker-compose up (with hyphen)","text":"<ul> <li>uses old Docker Compose (v1) which is a python based tool</li> <li>starts containers in the foreground, so you will see all logs from all containers in your terminal</li> <li>if you stop the terminal (Ctrl C) , the containers are also stopped unless you add the -d (detached)</li> <li>this command is still supported, but considered legacy - Docker now recommends the plugin-based syntax  </li> </ul>"},{"location":"Docker_Compose/#docker-compose-up-d-no-hyphen-between-docker-compose","title":"docker compose up -d (no hyphen between docker &amp; compose)","text":"<ul> <li>uses new Docker Compose (v2), which is now part of the docker CLI as plugin</li> <li>-d means detached mode:</li> <li>containers start in the background. </li> <li>you get your terminal back immediately</li> <li>logs are not streamed to terminal but you can view it with <code>docker compose logs -f</code></li> </ul>"},{"location":"Docker_Container_Interfaces/","title":"Docker Container Interfaces","text":""},{"location":"Docker_Container_Interfaces/#docker-container-interfaces","title":"Docker Container Interfaces","text":"<ul> <li><code>ip a</code> inside the docker container of an IED after <code>docker exec -it ds1cb1 bash</code>:</li> </ul> <p>Different use cases of the 2 network interfaces:</p>"},{"location":"Docker_Container_Interfaces/#1-eth0-1001624","title":"1. <code>eth0 : 10.0.1.6/24</code>","text":"<p>Default interface created when the container is attached to a Docker network (bridge, overlay, or custom)</p> <p>Use case:</p> <ul> <li>Used for container-to-container communication inside the same Docker network</li> <li>Other containers on the same network (subnet <code>10.0.1.0/24</code>) can reach this container via <code>10.0.1.6</code></li> <li>Not directly routable from the host machine unless you expose ports (via <code>-p host_port:container_port</code>)</li> </ul> <p>Example:</p> <ul> <li>If you have another container on the same Docker network, it can <code>ping 10.0.1.6</code> or connect directly to its services.</li> </ul>"},{"location":"Docker_Container_Interfaces/#2-eth1-17224101124","title":"2. <code>eth1 : 172.24.10.11/24</code>","text":"<p>A second interface \u2014 likely because you attached this container to another Docker network (such as with <code>--network</code> flag or via Docker Compose multiple networks)</p> <p>Use case:</p> <ul> <li>Provides connectivity to containers/services on the <code>172.24.10.0/24</code> subnet</li> <li>Might be used for external integration (such as simulating a \"backend network\" while <code>eth0</code> handles a \"frontend network\")</li> <li>Useful in multi-network setups, where a container acts as a bridge between 2 networks</li> </ul> <p>Example:</p> <ul> <li>If you are running a database container on the <code>172.24.10.0/24</code> network, this container can talk to it via <code>172.24.10.x</code></li> </ul>"},{"location":"Docker_Container_Interfaces/#which-ip-can-you-use-from-the-host-machine","title":"Which IP can you use from the host machine?","text":"<ul> <li>Normally, you don't use these internal container IPs from the host.</li> <li>Instead:</li> <li>Use localhost (127.0.0.1) on the host with port mappings (-p 8080:80)</li> <li>If using macvlan/host networks, the container may be directly reachable on one of these IPs (<code>10.0.1.6</code> or <code>172.24.10.11</code>) from the host or even other machines on the LAN.</li> </ul>"},{"location":"Docker_Container_Interfaces/#summary","title":"Summary:","text":"<ul> <li><code>eth0 (10.0.1.6)</code> : main Docker network, container-to-container communication.</li> <li><code>eth1 (172.24.10.0)</code>: secondary Docker network, used if container needs to talk to another isolated group of services.</li> <li>From the host, you usually connect via <code>localhost</code>+ published ports, unless you intentionally configured a special network (macvlan, host, overlay).</li> </ul> <p>To check if these IPs are reachable directly from host or internal-only :</p> <p>...to be continued...</p>"},{"location":"Docker_Macvlan_Network/","title":"Docker Macvlan Network","text":""},{"location":"Docker_Macvlan_Network/#docker-macvlan-network","title":"Docker Macvlan Network","text":"<p>A macvlan nerwork allows each Docker container to appear on the same Layer 2 network as your host (Ubuntu VM).</p> <ul> <li> <p>Each container gets its own unique IP address on your physical LAN (e.g. 192.168.30.x)</p> </li> <li> <p>The containers can be reached directly by other devices on that LAN - without NAT.</p> </li> </ul> <p>Because of that, the macvlan network must be bound to your host's physical network interface (the one actually connected to your LAN or VM bridge).</p>"},{"location":"Docker_Macvlan_Network/#2-choosing-the-correct-interface-to-create-mavvlan-network","title":"\ud83e\udde9 2\ufe0f\u20e3 Choosing the correct interface to create Mavvlan network","text":"<p>ip a output</p> <pre><code>ubuntu@pgt:~$ ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 08:00:27:62:7e:2a brd ff:ff:ff:ff:ff:ff\n    inet 192.168.30.60/24 brd 192.168.30.255 scope global enp0s3\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fe62:7e2a/64 scope link \n       valid_lft forever preferred_lft forever\n3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 52:18:4f:1d:0b:aa brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5018:4fff:fe1d:baa/64 scope link \n       valid_lft forever preferred_lft forever\nubuntu@pgt:~$ \n</code></pre> <p>From your ip a output:</p> <p>2: enp0s3:  <pre><code>inet 192.168.30.60/24\n</code></pre> <p>\u2705 enp0s3 is UP, has a valid IPv4 (192.168.30.60/24), and is your main interface connected to the external network.</p> <p>That makes enp0s3 the correct interface to use for a macvlan network.</p> <p>The others:</p> <p>lo \u2014 loopback (ignore)</p> <p>docker0 \u2014 Docker\u2019s internal bridge (not physical, so not usable for macvlan)</p> <p>So yes, you should use enp0s3 for your macvlan setup.</p> <p></p> <p>There are three private IPv4 address ranges defined by RFC 1918. These are reserved for internal networks only (they are not routable on the public Internet).</p> Private Range CIDR Typical Use <code>10.0.0.0</code> \u2013 <code>10.255.255.255</code> <code>/8</code> Large enterprise LANs <code>172.16.0.0</code> \u2013 <code>172.31.255.255</code> <code>/12</code> Corporate or internal subnets <code>192.168.0.0</code> \u2013 <code>192.168.255.255</code> <code>/16</code> Home, office, and VM networks <p>ubuntu@pgt:~$ ip route</p> <p>default via 192.168.30.1 dev enp0s3 proto static </p> <p>172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown </p> <p>192.168.30.0/24 dev enp0s3 proto kernel scope link src 192.168.30.60 </p> <p>ubuntu@pgt:~$ </p> <p>Create Docker macvlan network </p> <p><code>docker network create -d macvlan --subnet=192.168.30.0/24 --gateway=192.168.30.1 -o parent=enp0s3 plc-macvlan</code></p> <p>Persist Docker macvlan network:</p>"},{"location":"Docker_Macvlan_Network/#accessing-docker-macvlan-network-from-host-machine","title":"Accessing docker macvlan network from host machine:","text":"<ol> <li>How Docker macvlan works</li> </ol> <p>When you create a Docker network using the macvlan driver, Docker gives each container its own virtual MAC address and attaches it directly to the physical network via the VM\u2019s NIC (e.g., enp0s3).</p> <p>Each container looks like a separate device on the LAN \u2014 it can communicate directly with:</p> <p>Other containers on the same macvlan network</p> <p>Other physical machines on the same LAN</p> <p>But here\u2019s the catch \ud83d\udc47</p> <p>The host interface (the Ubuntu VM itself) is not automatically connected to this macvlan network.</p> <p>This is by design \u2014 Linux macvlan interfaces cannot talk to their parent interface (and vice versa).</p> <p>So the VM\u2019s main NIC (enp0s3) cannot directly send or receive packets to/from containers on the macvlan network.</p>"},{"location":"Docker_Macvlan_Network/#question","title":"Question:","text":"<p>How do I create and persist a linux virtual network interface on the ubuntu VM so that I can access the docker containers which I create on the docker macvlan network inside the ubuntu vm on the host, these are the details of how I created the docker macvlan network and the docker container on the ubuntu vm create docker macvlan network on ubuntu vm (which I have sshed into):</p> <ul> <li> <p><code>docker network create -d macvlan --subnet=192.168.30.0/24 --gateway=192.168.30.1 -o parent=enp0s3 plc-macvlan</code></p> </li> <li> <p>create and run the docker container for plc in ubuntu vm : </p> </li> </ul> <p><code>docker run -d --name st_plc--net plc-macvlan --ip 192.168.30.201 --privileged openplc:v3</code></p> <p>I want to access this website on my host machine : http://192.168.30.201:8080/login</p> <p>Perfect \u2014 you\u2019ve explained this clearly, and your goal is exactly right \ud83d\udc4f</p> <p>You already have:</p> <p>The Ubuntu VM running with IP 192.168.30.60 on interface enp0s3</p> <p>A Docker macvlan network (plc-macvlan) attached to that interface</p> <p>A PLC container with IP 192.168.30.201 on that macvlan</p> <p>Now you want to access that container\u2019s web UI (OpenPLC) from your host machine (e.g., your physical computer running VirtualBox, VMware, or similar).</p> <p>Let\u2019s go step-by-step and make the setup both functional and persistent.</p> <p>\ud83e\udde0 Why This Is Needed As you already know:</p> <p>A macvlan network gives containers their own MAC addresses and puts them directly on the LAN.</p> <p>But the Ubuntu VM itself (the host of the containers) can\u2019t communicate with them, because Linux doesn\u2019t allow the parent interface (enp0s3) to talk to its macvlan children.</p> <p>Step 1: Create the macvlan interface (temporary test)</p> <p>Run these inside your Ubuntu VM (SSH session):</p> <pre><code># Create the macvlan0 interface attached to enp0s3\nsudo ip link add macvlan0 link enp0s3 type macvlan mode bridge\n\n# Assign a free IP in the same subnet (different from 192.168.30.60 and 192.168.30.1)\nsudo ip addr add 192.168.30.254/24 dev macvlan0\n\n# Bring it up\nsudo ip link set macvlan0 up\n</code></pre> <p>Context: You are running a Docker container inside an Ubuntu VM, and that container is attached to a macvlan network (plc-macvlan). By design, a macvlan network isolates container traffic - the host (your VM) cannot communicate with containers on that macvlan unless you explicitly add a macvlan on the host side too.  </p> <p>So the 3 commands above make the VM itself a part of the same network as the container.</p> <ol> <li> <p><code>sudo ip link add macvlan0 link enp0s3 type macvlan mode bridge</code></p> </li> <li> <p>creates a new virtual network interface called <code>macvlan0</code></p> </li> <li> <p>this interface is attached to your physical network interface <code>enp0s3</code> (the one the VM uses to access your LAN)</p> </li> <li> <p>the type macvlan creates a separate interface with its own MAC address </p> </li> <li> <p><code>mode bridge</code> means that traffic on <code>enp0s3</code> will be bridged with <code>macvlan0</code> - both can send/receive packets in the same Layer 2 domain.</p> </li> </ol> <p>Why is it needed:</p> <ul> <li> <p>Docker's <code>macvlan</code> driver puts containers in the same L2 domain but isolates them from the host.</p> </li> <li> <p>By creating <code>macvlan0</code>, your VM becomes reachable to and from containers in that macvlan network.</p> </li> <li> <p><code>sudo ip addr add 192.168.30.254/24 dev macvlan0</code></p> </li> <li> <p>Assigns an IP address (192.168.30.254) to the new <code>macvlan0</code> interface.</p> </li> <li> <p>The <code>/24</code> subnet mask means it is part of the same 192.168.30.x network as your VM (<code>192.168.30.254</code>) and containers (like <code>192.168.30.201</code>).</p> </li> <li> <p>it acts as the gateway for your VM to communicate with containers</p> </li> <li> <p>without an IP, the VM can't send packets directly through macvlan0</p> </li> <li> <p><code>sudo ip link set macvlan0 up</code></p> </li> <li> <p>brings the macvlan0 interface online (equivalent to 'turning it on')</p> </li> <li> <p>by default, new interfaces are down</p> </li> <li> <p>this command activates it so it can actually pass network traffic</p> </li> </ul> <p>After these steps: </p> <p>VM can now <code>ping</code> and access containers on the plc-macvlan network e.g. 192.168.30.201</p> <p><code>ping -I macvlan0 192.168.30.201</code></p> <p>To persist the macvlan interface created above:</p> <p>By default, <code>ip link add</code> changes are temporary and vanish after reboot.</p> <p>To persist it across reboots, choose one of these methods:</p> <p>Option 1: create a startup script + systemd service </p> <p>Step 1:</p> <ol> <li>create the script:</li> </ol> <p><code>sudo nano /usr/local/sbin/setup-macvlan.sh</code></p> <p>Paste:</p> <pre><code>#!/bin/bash\nip link add macvlan0 link enp0s3 type macvlan mode bridge\nip addr add 192.168.30.254/24 dev macvlan0\nip link set macvlan0 up\n</code></pre> <ol> <li>Make it executable:</li> </ol> <p><code>sudo chmod +x /usr/local/sbin/setup-macvlan.sh</code></p> <p>Step 2:</p> <ol> <li>Create the systemd service file</li> </ol> <p><code>sudo nano /etc/systemd/system/setup-macvlan.service</code></p> <ol> <li>Paste this:</li> </ol> <pre><code>[Unit]\nDescription=Setup macvlan0 network interface\nAfter=network.target\n\n[Service]\nType=oneshot\nExecStart=/usr/local/sbin/setup-macvlan.sh\nRemainAfterExit=yes\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Step 3: Reload and enable the service</p> <p><code>sudo systemctl daemon-reload</code></p> <p>Enable it to run automatically at reboot</p> <p><code>sudo systemctl enable setup-macvlan.service</code></p> <p>Test it immediately without rebooting:</p> <p><code>sudo systemctl start setup-macvlan.service</code></p> <p>Step 4: verify it worked </p> <p>Check if the interface is up: <code>ip addr show macvlan0</code></p> <p>And verify service status: <code>sudo systemctl status setup-macvlan.service</code></p>"},{"location":"Docker_Macvlan_Network/#to-make-docker-container-reachable-from-vms-default-interface","title":"To make docker container reachable from VM's default interface :","text":"<pre><code>sudo ip link add macvlan-host link enp0s3 type macvlan mode bridge\nsudo ip addr add 192.168.30.250/24 dev macvlan-host\nsudo ip link set macvlan-host up\n</code></pre> <p>1\ufe0f\u20e3 Container is running and reachable from the VM From your last command: ping -I macvlan0 192.168.30.201 You get: 64 bytes from 192.168.30.201 ... \u2705 This means inside the VM, the container is reachable via the macvlan0 interface. The container is up and responding to pings from the VM itself. 2\ufe0f\u20e3 Container is not reachable from the VM\u2019s default interface When you just run: ping 192.168.30.201 You get: Destination Host Unreachable This happens because Docker macvlan creates an isolated network, and the host VM\u2019s default interface (enp0s3) cannot talk to the macvlan interface directly. This is a known limitation of macvlan: A host cannot communicate with a container on a macvlan network by default. 3\ufe0f\u20e3 How to reach the container from the host machine (your physical machine or host VM) Since your VM is attached as Bridged Adapter, the host machine is on the same subnet (192.168.30.0/24) as your container. To reach the container: Option A: Add a macvlan sub-interface on the host machine You need to create a macvlan interface on your host machine, attached to the same physical network as the VM. Give it an IP in the same subnet (but not conflicting with gateway, VM, or container IPs). Example on host: sudo ip link add macvlan-host link  type macvlan mode bridge sudo ip addr add 192.168.30.250/24 dev macvlan-host sudo ip link set macvlan-host up Then ping: ping 192.168.30.201 <p>TROUBLESHOOTING :</p> <pre><code>ping -I macvlan0 192.168.30.201   # works\nping 192.168.30.201               # fails\n</code></pre> <p>1\ufe0f\u20e3 What\u2019s wrong currently From docker inspect plc-macvlan: \"Subnet\": \"192.168.30.0/24\", \"Gateway\": \"192.168.30.1\", \"Options\": { \"parent\": \"enp0s3\" } Your macvlan network for containers is on the same subnet as your VM\u2019s default interface (192.168.30.0/24). Your VM\u2019s IP is 192.168.30.60. Your containers have IPs like 192.168.30.201. Linux cannot send packets from the parent interface (enp0s3) to macvlan child interfaces on the same subnet. This is a known limitation of macvlan: Host \u2194 Macvlan containers on the same L2 subnet is blocked unless you use a separate \u201cmacvlan-host\u201d interface with a different subnet or routing.</p> <p>2\ufe0f\u20e3 What you need to change Solution: put your macvlan containers on a different subnet, e.g., 192.168.40.0/24. Then: Use a \u201chost macvlan interface\u201d (macvlan-host) on the VM in the same subnet, e.g., 192.168.40.254. Enable IP forwarding + iptables NAT to allow the VM and host to reach the container network.</p> <p>I will have 4 different docker containers for 4 plcs on the ubuntu vm hence using the same url and the same ssh method of accessing every plc will not make sense, how can I use the unique ip address of each plc container to access the browser</p> <p>http://localhost:8080/dashboard</p> <p>ssh -L 8080:192.168.40.201:8080 ubuntu@192.168.30.60</p> <p>Perfect \u2014 that\u2019s exactly the right way to think about this. Since you\u2019ll be running 4 PLC containers, each with a unique IP (e.g., 192.168.40.201, 192.168.40.202, etc.) on the macvlan network, you want to be able to access each PLC\u2019s web interface directly by its IP instead of tunneling everything to localhost:8080. Let\u2019s go step by step \ud83d\udc47 \ud83e\udde0 Why the problem happens The reason you can\u2019t currently access 192.168.40.x IPs from your Remote Desktop host (or from your own PC) is that macvlan networks isolate traffic from the parent host interface. That means: The Ubuntu VM can talk to the containers \u2705 But the Remote Desktop host (the VM\u2019s hypervisor host) cannot \u274c Nor can your own machine (accessing via Remote Desktop or SSH) \u274c So, even though the containers have unique IPs, those IPs exist only inside the VM\u2019s internal macvlan network.</p> <p>\ud83e\udde9 To fix this \u2014 you have two design options: Option 1: Add a second adapter (NAT or Host-only) + iptables forwarding This approach lets you access each PLC by its unique IP directly \u2014 without using SSH tunneling or localhost. Step 1: Add another network adapter to your Ubuntu VM In your VM settings (in VirtualBox, VMware, etc.): Keep the Bridged Adapter (this is your current enp0s3). Add a second adapter: Type: NAT (or Host-only) Name: automatically created, e.g. enp0s8 in Ubuntu. This will give your VM a second IP (say, 10.0.2.15 for NAT or 192.168.56.101 for host-only). Step 2: Enable IP forwarding Inside the Ubuntu VM: sudo sysctl -w net.ipv4.ip_forward=1</p> <p>To persist it: echo \"net.ipv4.ip_forward=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Step 3: Forward each PLC port range Let\u2019s say you have these PLC containers: PLC Container IP    Web Port    External Port PLC1    192.168.40.201  8080    8081 PLC2    192.168.40.202  8080    8082 PLC3    192.168.40.203  8080    8083 PLC4    192.168.40.204  8080    8084</p> <p>Add forwarding rules: sudo iptables -t nat -A PREROUTING -p tcp -d  --dport 8081 -j DNAT --to-destination 192.168.40.201:8080 sudo iptables -t nat -A PREROUTING -p tcp -d  --dport 8082 -j DNAT --to-destination 192.168.40.202:8080 sudo iptables -t nat -A PREROUTING -p tcp -d  --dport 8083 -j DNAT --to-destination 192.168.40.203:8080 sudo iptables -t nat -A PREROUTING -p tcp -d  --dport 8084 -j DNAT --to-destination 192.168.40.204:8080 <p>sudo iptables -t nat -A POSTROUTING -j MASQUERADE Replace  with the Ubuntu VM\u2019s default bridged IP (the one reachable from your Remote Desktop). Step 4: Access from Remote Desktop or your own PC Now you can open, from Remote Desktop or your own machine: http://:8081/login \u2192 PLC1 http://:8082/login \u2192 PLC2 http://:8083/login \u2192 PLC3 http://:8084/login \u2192 PLC4 \u2705 Each one maps uniquely to the right container. To persist the iptables rules: sudo apt install iptables-persistent sudo netfilter-persistent save Option 2: Bridge the macvlan directly to the Remote Desktop network This is more complex but allows direct IP access (e.g., you could ping 192.168.40.201 directly from your Remote Desktop host). However: You must ensure the Remote Desktop\u2019s LAN supports the 192.168.40.0/24 subnet. The Remote Desktop\u2019s physical adapter must allow macvlan bridge mode. If multiple VMs use the same network, you may have IP conflicts. If your VM is on a corporate or managed network, this method is not recommended \u2014 Option 1 (NAT + iptables) is safer and works everywhere. Summary Goal    Recommended Solution Access PLC browser from Remote Desktop  Use iptables port forwarding from VM IP to container IP Access PLC browser from your own host PC    Use same forwarding \u2014 just access VM\u2019s bridged IP Keep separate PLC dashboards    Assign unique external ports (8081\u20138084) Keep setup persistent   Use iptables-persistent or a startup script Would you like me to give you the exact command sequence and a ready-to-use startup script that automatically sets up this forwarding for all 4 PLCs whenever the VM boots?"},{"location":"Docker_Networks/","title":"Docker Networking","text":""},{"location":"Docker_Networks/#docker-networking-tutorial-bridge-none-host-ipvlan-macvlan-overlay","title":"Docker Networking Tutorial (Bridge - None - Host - IPvlan - Macvlan - Overlay)","text":"<p>Reference: https://youtu.be/fBRgw5dyBd4?si=LodjJDx4AdnjYOS6</p>"},{"location":"Docker_Networks/#docker-bridge","title":"Docker Bridge","text":"<ul> <li>2 segments that the bridge connects is:</li> <li>local host e.g. your laptop, or linux server where you run your Docker containers</li> <li>virtual network created by Docker</li> <li></li> </ul>"},{"location":"Docker_Networks/#when-you-install-docker-for-the-first-time-it-will-create-a-default-bridge-network-on-the-host-slightly-different-on-macos-as-it-will-run-al-containers-inside-a-linux-vm-virtualization-framework","title":"When you install Docker for the first time, it will create a default bridge network on the host (slightly different on MacOS as it will run al containers inside a Linux VM, virtualization framework)","text":"<ul> <li>all containers you create on that host will get an IP address on that range</li> </ul> <p>On a Linux host, Docker Engine directly creates a docker0 bridge network interface in the host\u2019s network stack. That\u2019s the familiar default bridge you see with ifconfig or ip a (usually 172.17.0.1/16). </p> <p>On a Mac (macOS) or Windows, things are different:</p> <p>Docker Desktop does not create a native docker0 bridge interface on your MacBook\u2019s network stack.</p> <p>Instead, Docker Desktop runs all containers inside a lightweight Linux VM (on macOS, this is managed using Apple\u2019s HyperKit or now a Virtualization Framework).</p> <p>Inside that VM, Docker does create a docker0 bridge (just like on Linux). But this bridge is inside the VM, not on macOS itself.</p> <p>From your MacBook\u2019s perspective, you won\u2019t see docker0 if you run ifconfig or ip a. Instead, Docker Desktop sets up a special networking layer that forwards traffic between your MacBook and the VM.</p> <p>So to answer directly:</p> <p>\ud83d\udc49 No, Docker Desktop does not create a default bridge network on your MacBook\u2019s host network stack. \ud83d\udc49 Yes, it does create a default bridge network inside the Linux VM where your containers run.</p> <p>If you run: <code>docker network ls</code> you\u2019ll still see bridge, host, and none listed, because those networks exist in the Docker VM environment, even though your Mac itself doesn\u2019t expose them as interfaces.</p> <p>Do you want me to also explain how you can check the actual IP range of the bridge network inside Docker Desktop on your Mac?</p>"},{"location":"Docker_Networks/#containers-created-in-the-default-bridge-network","title":"Containers created in the default bridge network","text":"<ul> <li>can communicate with each other</li> <li>default bridge network has restrictions and is not recommended for production, better to use user-defined bridge network.</li> <li>Default bridge network: Cannot use container DNS from host </li> <li>In default bridge network, DNS is not supported. Also cannot use DNS to send requests between containers.</li> </ul>"},{"location":"Docker_Networks/#user-defined-bridge-network","title":"User defined bridge network**:","text":"<ul> <li>Can use DNS to send requests to containers. </li> <li>This only applies to communications inside the bridge network between containers. You still will NOT be able to use the container DNS from the host.</li> </ul>"},{"location":"Docker_Networks/#host-mode-networking-option","title":"Host mode - networking option","text":"<ul> <li>container will not get its own IP and instead share the same networking namespace as the host where you run the container</li> <li>will appear as if you were running a regular application on that host</li> <li>thus any application running on a different server will be able to access the container using the host's IP address</li> </ul>"},{"location":"Docker_Networks/#ipvlan-network","title":"IPvlan network:","text":"<ul> <li>Traditionally, to expose a container to the outside world we used bridge network. But it adds additional complexity and performance penalty. Packet needs to go through additional hop, need to map ports from the container to the host to expose it to other applications.  </li> <li>Does not use a bridge for isolation and is associated directly with the Linux network interface. No need for port mappings in these scenarios.</li> </ul>"},{"location":"Docker_Networks/#macvlan-network","title":"MacVLAN network:","text":"<ul> <li>Some legacy applications and those that monitor network traffic expect to be directly connected to the physical network.</li> <li>In this case, can use the MacVLAN network driver to assign a MAC address to each container's virtual network interface.</li> <li>It will appear as a physical network interface directly connected to the physical network.</li> <li>to create it, must specify the subnet that the host uses, the gateway, and the parent network interface</li> <li>compare the parent MAC address (parent interface) -&gt; e.g. ip addr show ens33</li> <li>MAC address of the container -&gt; ssh into it and do <code>ip addr</code></li> </ul>"},{"location":"Docker_Networks/#difference-between-ipvlan-and-macvlan","title":"Difference between IPVLAN and MACVLAN:","text":"<ul> <li>if use IPVLAN: container will get the same MAC address as your host</li> <li>if use MACVLAN: container will have a different MAC address from host</li> </ul>"},{"location":"Docker_Networks/#overlay-network","title":"Overlay network","text":"<ul> <li>When you deploy your applications to production, you will need more than one physical or virtual server, each with a Docker daemon installed.</li> <li>The overlay network driver creates a distributed network among multiple Docker daemon hosts.</li> <li>This network sits on top of (overlays) the host-specific networks, allowing containers connected to it to communicate securely, especially when encryption is enabled.</li> <li>Most frequently, this type of network is used with Docker Swarm , but it is also possible to connect individual containers.</li> <li>Comment from youtuber: to manage containers at scale, especially in production, consider using Kubernetes.</li> <li>need to disable</li> <li>Example of usage of overlay network:</li> <li><code>2 ubuntu VMs</code></li> <li>find network interfaces on both VMs, e.g. ens33 on both VMs -&gt; disable this on the network interface on both VMs</li> <li><code>sudo ethtool -K ens33 tx-checksum-ip-generic off</code> (not persistent across restarts so might need to use something like a script using systemd service to automatically run it on boot)</li> <li>Even if you want to connect individual containers to overlay network, we still need to initialize the Docker Swarm</li> <li>on the first VM, run <code>docker swarm init</code></li> <li>will give a command you can execute on other VMs to join the Docker Swarm</li> <li>each VM must have docker installed</li> <li>manager and worker nodes (the VMs)</li> <li>on manager, create an overlay network and an attachable flag for individual containers to be able to use this network. Otherwise only swarm services will be able to use it.</li> <li>on the worker node (VM) if you do <code>docker network ls</code>, you will only see the overlay network when you start a container that uses the network (can start on the manager node)</li> <li>to verify that we can access containers on that overlay network deployed on different VMs, can SSH to the second container and use curl to send a request to the first container on the manager node</li> <li></li> </ul>"},{"location":"Docker_Networks/#overlay-networks","title":"Overlay Networks","text":"<ul> <li>Allow containers running on different Docker hosts (machines) to communicate with each other as if they are on the same local network, while keeping the communcation secure and isolated.</li> </ul> <p>Why normal Docker networks is not enough? - By default, Docker creates a bridge network on a single host (like bridge0 on Linux) - Containers connected to this bridge can talk to each other, but only on the same host.</p> <p>If you want:  - multiple physical/virtual servers - running containers that need to talk to each other - Need something that span across hosts</p>"},{"location":"Docker_Networks/#how-overlay-networks-work","title":"How overlay networks work?","text":"<ul> <li>Docker creates a virtual distributed network that spans all participating nodes (hosts).</li> <li>uses VXLAN tunneling (encapsulating packets inside UDP) so that container-to-container traffic can move across hosts transparently.</li> <li>network is managed by Docker swarm or Docker engine in swarm mode</li> </ul> <p>Example: - Host A has a container running web. - Host B has a container running db. - Both containers join the same overlay network called backend-net. - Now web can talk to db using the container name (db:3306), even though they are on different machines.</p>"},{"location":"Docker_Space_Optimization/","title":"Docker Space Optimization","text":"<p>\u2705 Good \u2014 <code>docker system prune -a</code> ran successfully and reclaimed 2.26 GB.</p> <p>This is expected: most of the 62 GB in <code>/var/lib/docker</code> is still being used by:</p> <ul> <li>Running containers \u2192 <code>/var/lib/docker/overlay2</code> layers</li> <li>Container logs \u2192 <code>/var/lib/docker/containers</code></li> </ul> <p>The biggest remaining culprit is usually logs from running containers, which can grow very large, especially for long-running PLC/IED/SCADA containers.</p>"},{"location":"Docker_Space_Optimization/#step-2-clear-container-logs-safe-does-not-stop-containers","title":"\u2705 Step 2 \u2014 Clear container logs (safe, does NOT stop containers)","text":"<p>Check which logs are large:</p> <pre><code>sudo du -sh /var/lib/docker/containers/*/*-json.log | sort -h\n</code></pre> <p>Then truncate all logs:</p> <pre><code>sudo sh -c 'truncate -s 0 /var/lib/docker/containers/*/*-json.log'\n</code></pre> <p>\u2705 This frees space immediately \u2705 Does not restart or break containers \u2705 Very safe for persistent workloads</p>"},{"location":"Docker_Space_Optimization/#step-3-optional-prune-unused-docker-volumes","title":"\u2705 Step 3 \u2014 Optional: prune unused Docker volumes","text":"<p>If you have volumes that are truly unused, you can run:</p> <pre><code>sudo docker volume prune\n</code></pre> <p>\u26a0\ufe0f Be careful \u2014 volumes may contain persistent databases or configs. Only delete if you are sure.</p>"},{"location":"Docker_Space_Optimization/#step-4-recheck-disk-usage","title":"\u2705 Step 4 \u2014 Recheck disk usage","text":"<p>After truncating logs and pruning:</p> <pre><code>df -h\nsudo du -h -d1 /var/lib/docker | sort -h\n</code></pre> <p>You should see several tens of GB freed and <code>/</code> no longer at 100%.</p> <p>If you want, I can give you a one-line set of commands to safely reclaim the maximum possible space from your Docker setup without deleting running containers or important volumes.</p> <p>Do you want me to do that?</p>"},{"location":"Docker_Space_Optimization/#question-how-should-i-truncate-logs-based-on-the-output-below","title":"Question: How should I truncate logs based on the output below?","text":"<p>'''</p> <p>ubuntu@pgt:~$ docker inspect --format='{{.HostConfig.LogConfig.Type}}' eg_plc               json-file ubuntu@pgt:~$ docker inspect --format='{{.HostConfig.LogConfig.Type}}' st_plc json-file ubuntu@pgt:~$ docker inspect --format='{{.HostConfig.LogConfig.Type}}' st_ied json-file ubuntu@pgt:~$ docker inspect --format='{{.HostConfig.LogConfig.Type}}' scadalts json-file ubuntu@pgt:~$ docker inspect --format='{{.HostConfig.LogConfig.Type}}' matlab_container json-file ubuntu@pgt:~$ docker inspect --format='{{.HostConfig.LogConfig.Type}}' mysql json-file ubuntu@pgt:~$  '''</p> <p>Perfect \u2014 all your containers are using the <code>json-file</code> logging driver, which means:</p> <p>\u2705 Logs are stored on the host at:</p> <pre><code>/var/lib/docker/containers/&lt;container_id&gt;/&lt;container_id&gt;-json.log\n</code></pre> <p>Truncating logs inside the container won\u2019t work, you need to do it on the host.</p>"},{"location":"Docker_Space_Optimization/#step-1-truncate-logs-for-all-containers","title":"\u2705 Step 1 \u2014 Truncate logs for all containers","text":"<p>Run this one-liner on the host:</p> <pre><code>sudo sh -c 'truncate -s 0 /var/lib/docker/containers/*/*-json.log'\n</code></pre> <p>Explanation:</p> <ul> <li><code>truncate -s 0</code> \u2192 sets file size to 0 bytes</li> <li><code>*/*-json.log</code> \u2192 matches all container log files</li> <li>Safe: does not stop containers or remove data</li> </ul>"},{"location":"Docker_Space_Optimization/#step-2-verify-space-reclaimed","title":"\u2705 Step 2 \u2014 Verify space reclaimed","text":"<pre><code>df -h\nsudo du -h -d1 /var/lib/docker/containers | sort -h\n</code></pre> <p>You should see <code>/var/lib/docker/containers</code> shrink significantly.</p>"},{"location":"Docker_Space_Optimization/#step-3-prevent-logs-from-growing-too-large-in-the-future","title":"\u2705 Step 3 \u2014 Prevent logs from growing too large in the future","text":"<p>Edit or create Docker daemon config:</p> <pre><code>sudo nano /etc/docker/daemon.json\n</code></pre> <p>Add:</p> <pre><code>{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"50m\",\n    \"max-file\": \"3\"\n  }\n}\n</code></pre> <p>Then restart Docker:</p> <pre><code>sudo systemctl restart docker\n</code></pre> <p>\u2705 Limits each container log to 50 MB, keeps 3 rotated files, prevents <code>/var/lib/docker</code> from filling up again.</p> <p>This will safely free tens of GB from your VM and prevent future log bloat.</p> <p>If you want, I can give you a single sequence of commands to reclaim all possible space in <code>/var/lib/docker</code> without touching running containers or important volumes.</p>"},{"location":"Docker_Space_Optimization/#changes-to-systemd-will-they-persist-when-vm-is-extracted-or-only-default-settings-persist","title":"Changes to Systemd - Will they persist when VM is extracted or only default settings persist?","text":"<p>ubuntu@pgt:~$ systemd-delta [OVERRIDDEN] /etc/tmpfiles.d/screen-cleanup.conf \u2192 /usr/lib/tmpfiles.d/screen-cleanup.conf</p> <p>--- /usr/lib/tmpfiles.d/screen-cleanup.conf     2021-02-23 10:18:41.000000000 +0000 +++ /etc/tmpfiles.d/screen-cleanup.conf 2024-09-11 14:24:28.000000000 +0000 @@ -1 +1,2 @@ -d /run/screen 0777 root utmp +# This file is generated by /var/lib/dpkg/info/screen.postinst upon package configuration +d /run/screen 1777 root utmp</p> <p>[OVERRIDDEN] /etc/systemd/system/sshd-keygen@.service.d/disable-sshd-keygen-if-cloud-init-active.conf \u2192 /usr/lib/systemd/system/sshd-keygen@.service.d/disable-sshd-keygen-if-cloud-init-active.conf</p> <p>Files /usr/lib/systemd/system/sshd-keygen@.service.d/disable-sshd-keygen-if-cloud-init-active.conf and /etc/systemd/system/sshd-keygen@.service.d/disable-sshd-keygen-if-cloud-init-active.conf are identi&gt;</p> <p>[EQUIVALENT] /etc/systemd/system/multipath-tools.service \u2192 /usr/lib/systemd/system/multipath-tools.service [MASKED]     /etc/systemd/system/systemd-networkd-wait-online.service \u2192 /usr/lib/systemd/system/systemd-networkd-wait-online.service [EXTENDED]   /usr/lib/systemd/system/netfilter-persistent.service \u2192 /etc/systemd/system/netfilter-persistent.service.d/iptables.conf [EXTENDED]   /usr/lib/systemd/system/rc-local.service \u2192 /usr/lib/systemd/system/rc-local.service.d/debian.conf [EXTENDED]   /usr/lib/systemd/system/systemd-localed.service \u2192 /usr/lib/systemd/system/systemd-localed.service.d/locale-gen.conf [EXTENDED]   /usr/lib/systemd/system/user@.service \u2192 /usr/lib/systemd/system/user@.service.d/timeout.conf</p> <p>8 overridden configuration files found. lines 1-21/21 (END)</p>"},{"location":"Docker_Space_Optimization/#question-suggest-a-full-checklist-to-verify-that-my-extracted-vm-preserves-all-docker-systemd-and-network-configurations-before-i-start-using-it-on-the-new-host","title":"Question: Suggest a full checklist to verify that my extracted VM preserves all Docker, systemd, and network configurations before I start using it on the new host.","text":"<p>Absolutely \u2014 here\u2019s a comprehensive checklist to verify your extracted Ubuntu VM so that Docker, systemd, and network configurations are preserved and the VM behaves as expected on the new host.</p>"},{"location":"Docker_Space_Optimization/#1-verify-vm-basics","title":"1\ufe0f\u20e3 Verify VM Basics","text":"<ul> <li>\u2705 Check disk space:</li> </ul> <pre><code>df -h\n</code></pre> <ul> <li>\u2705 Check hostname and networking:</li> </ul> <pre><code>hostname\nip a\nping 8.8.8.8\n</code></pre> <ul> <li>\u2705 Verify Ubuntu version:</li> </ul> <pre><code>lsb_release -a\nuname -a\n</code></pre> <ul> <li>\u2705 Ensure root and user accounts exist and have correct permissions.</li> </ul>"},{"location":"Docker_Space_Optimization/#2-verify-systemd-configuration","title":"2\ufe0f\u20e3 Verify systemd configuration","text":"<ul> <li>\u2705 Run <code>systemd-delta</code> to confirm overridden, masked, and extended unit files:</li> </ul> <pre><code>systemd-delta\n</code></pre> <ul> <li> <p>Compare output to source VM \u2014 all <code>[OVERRIDDEN]</code> and <code>[EXTENDED]</code> entries should match.</p> </li> <li> <p>\u2705 Check status of critical services (e.g., Docker, SSH, networking, custom PLC/IED services):</p> </li> </ul> <pre><code>systemctl status docker\nsystemctl status &lt;custom-service-name&gt;\n</code></pre> <ul> <li>\u2705 List all enabled services:</li> </ul> <pre><code>systemctl list-unit-files | grep enabled\n</code></pre> <ul> <li>\u2705 Confirm masked services are still masked:</li> </ul> <pre><code>systemctl list-unit-files | grep masked\n</code></pre> <ul> <li>\u2705 Check drop-in files for custom units:</li> </ul> <pre><code>ls -l /etc/systemd/system/&lt;unit&gt;.service.d/\n</code></pre>"},{"location":"Docker_Space_Optimization/#3-verify-docker-configuration-and-containers","title":"3\ufe0f\u20e3 Verify Docker configuration and containers","text":"<ul> <li>\u2705 Check Docker daemon info:</li> </ul> <pre><code>docker info\n</code></pre> <ul> <li> <p>Confirm <code>Docker Root Dir</code> is correct and matches your source VM.</p> </li> <li> <p>\u2705 List all containers (running and stopped):</p> </li> </ul> <pre><code>docker ps -a\n</code></pre> <ul> <li>\u2705 List images:</li> </ul> <pre><code>docker images\n</code></pre> <ul> <li>\u2705 Verify Docker volumes:</li> </ul> <pre><code>docker volume ls\ndocker volume inspect &lt;volume_name&gt;\n</code></pre> <ul> <li>\u2705 Check logging driver:</li> </ul> <pre><code>docker inspect --format='{{.HostConfig.LogConfig.Type}}' &lt;container_name&gt;\n</code></pre> <ul> <li>\u2705 Optional: Run a container and confirm it can access networks and volumes as expected.</li> </ul>"},{"location":"Docker_Space_Optimization/#4-verify-docker-networks","title":"4\ufe0f\u20e3 Verify Docker networks","text":"<ul> <li>\u2705 List networks:</li> </ul> <pre><code>docker network ls\n</code></pre> <ul> <li>\u2705 Inspect important networks (especially macvlan for PLC/IED):</li> </ul> <pre><code>docker network inspect &lt;network_name&gt;\n</code></pre> <ul> <li>Check that IP ranges, subnets, and gateway settings match the source VM.</li> </ul>"},{"location":"Docker_Space_Optimization/#5-verify-network-configuration-outside-docker","title":"5\ufe0f\u20e3 Verify network configuration outside Docker","text":"<ul> <li>\u2705 List system network interfaces:</li> </ul> <pre><code>ip a\n</code></pre> <ul> <li>\u2705 Check routes:</li> </ul> <pre><code>ip route\n</code></pre> <ul> <li>\u2705 Check <code>netplan</code> or <code>networkd</code> configuration:</li> </ul> <pre><code>cat /etc/netplan/*.yaml\nsystemctl status systemd-networkd\n</code></pre> <ul> <li>\u2705 Verify firewall rules:</li> </ul> <pre><code>sudo iptables -L -n -v\nsudo ufw status\n</code></pre>"},{"location":"Docker_Space_Optimization/#6-verify-critical-configuration-files","title":"6\ufe0f\u20e3 Verify critical configuration files","text":"<ul> <li>\u2705 Custom systemd service files:</li> </ul> <pre><code>ls -l /etc/systemd/system/\n</code></pre> <ul> <li>\u2705 Network-related files:</li> </ul> <pre><code>ls -l /etc/network/\nls -l /etc/systemd/network/\n</code></pre> <ul> <li>\u2705 Docker configuration:</li> </ul> <pre><code>cat /etc/docker/daemon.json\n</code></pre> <ul> <li>\u2705 Any custom scripts or applications used in PLC/IED services.</li> </ul>"},{"location":"Docker_Space_Optimization/#7-test-runtime-functionality","title":"7\ufe0f\u20e3 Test runtime functionality","text":"<ul> <li>\u2705 Start Docker containers and ensure they start without errors:</li> </ul> <pre><code>docker-compose up -d  # if using docker-compose\ndocker start &lt;container_name&gt;\n</code></pre> <ul> <li>\u2705 Test service connectivity (PLC \u2194 IED, MATLAB, SCADA, database):</li> </ul> <pre><code>docker exec -it &lt;container&gt; ping &lt;other-container&gt;\n</code></pre> <ul> <li>\u2705 Test any systemd services that should auto-start:</li> </ul> <pre><code>systemctl restart &lt;custom-service&gt;\nsystemctl status &lt;custom-service&gt;\n</code></pre> <ul> <li>\u2705 Optionally, run your usual workflow (PLC simulation, MATLAB scripts, SCADA dashboards) to ensure the VM works as intended.</li> </ul>"},{"location":"Docker_Space_Optimization/#8-backup-for-safety","title":"8\ufe0f\u20e3 Backup for safety","text":"<ul> <li>Before making major changes, consider taking a snapshot of the extracted VM in VirtualBox.</li> <li>This way you can revert if anything doesn\u2019t behave as expected.</li> </ul>"},{"location":"Docker_Space_Optimization/#summary","title":"\u2705 Summary","text":"<p>Following this checklist ensures:</p> <ul> <li>All systemd overrides, masks, and extensions are preserved.</li> <li>Docker containers, images, volumes, and networks behave as expected.</li> <li>Network settings and connectivity match the source VM.</li> <li>Critical workflows and simulations will run after extraction.</li> </ul> <p>If you want, I can make a condensed \u201cone-shot\u201d set of commands that you can run on the extracted VM to verify all of these points automatically and quickly.</p> <p>Do you want me to do that?</p>"},{"location":"Docker_Volumes/","title":"Docker Volumes","text":""},{"location":"Docker_Volumes/#what-is-a-volume-in-docker","title":"What is a Volume in Docker","text":"<p>Think of Docker container as a small isolated computer - when you stop or delete it, everything inside disappears (its 'disk' is temporary).</p> <p>A Docker volume is like pluggin in an external hard drive or shared folder to that container.</p> <p>It lets you:</p> <ol> <li>Keep data even after container is stopped or removed.</li> <li>Share files between real machine (the host) and the container.</li> </ol> <p>There are 2 main types of volume mounts:</p> <ul> <li>Bind mount (links a folder from your host machine to the container)</li> <li>Named volume (managed entirely by Docker - stored somewhere under <code>/var/lib/docker/volumes</code>)</li> </ul> <p>Explanation for use of volumes:</p> <ul> <li>Left side (<code>./sql</code>) -&gt; a folder on your computer at the same level as your docker-compose.yml file</li> <li>Right side (<code>/docker-entrypoint-initdb.d</code>) -&gt; a special folder inside the container that MySQL checks when it first starts.</li> </ul> <p>When MySQL starts for the first time, it looks in <code>/docker-entrypoint-initdb.d</code> and automatically runs any <code>.sql</code> files it finds there - this is how it initializes your database.</p> <p>When Docker starts the MySQL container, it copies that folder into the container.</p> <p>MySQL executes your CCP_data.sql automatically - creating the tables, inserting initial data, etc.</p> <p>This mount does not store your database data (the rows &amp; tables you later insert). It only gives MySQL initialization scripts.</p> <p>Once MySQL is running, the database data is stored inside the container's temporary filesystem. If you delete the container, the data is lost.</p> <p>This is called a bind mount (it directly links your folder <code>./sql</code> from the host to the continer).</p> <p>Question:  does the special folder docker-entrypoint-initdb.d exist in every docker container ? if not where does it come from ?</p> <p>No, the folder /docker-entrypoint-initdb.d does not exist in every Docker container. It exists only in certain official database images (like MySQL, PostgreSQL, MongoDB, etc.) because the image creator chose to include it in the image\u2019s startup logic. </p> <p>Each image defines its own conventions about where to put configuration or initialization files.</p> <p>So when you use:</p> <pre><code>    volumes:\n      - ./sql:/docker-entrypoint-initdb.d\n</code></pre> <p>you are not telling Docker anything special - you are telling the MySQL container: When you run your startup script, here is the folder where you can find initialization files.</p>"},{"location":"Docker_Volumes/#content-of-docker-composeyml-file-to-use-postgresql-database-as-docker-container","title":"Content of docker-compose.yml file to use postgresql database as docker container","text":"<p>This version does not persist volume and data will vanish if you remove the container.</p> <pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: ccp-multi-ied-multi-plc\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: powerplant\n    ports:\n      - \"5432:5432\"\n    volumes:\n    - ./postgres-sql:/docker-entrypoint-initdb.d\n\n</code></pre> <p>Use of  - volumes: - ./sql:/docker-entrypoint-initdb.d</p> <ul> <li> <p>This mounts your host directory ./sql into the container\u2019s /docker-entrypoint-initdb.d directory.</p> </li> <li> <p>docker-entrypoint-initdb.d is a special folder: on first startup of the container, MySQL runs all .sql or .sh scripts there to initialize the DB.</p> </li> <li> <p>After initialization, new runs don\u2019t automatically re-run scripts unless you remove the container and volume.</p> </li> </ul> <p>\u2757 This is not where live database data is stored. Live data is in /var/lib/mysql. Since you\u2019re not mounting that, the data lives in a Docker-managed anonymous volume.</p> <p>So: Changing this bind mount will not remove your MySQL data. It only affects which init scripts would run on container creation.</p>"},{"location":"Docker_Volumes/#content-of-docker-composeyml-file-to-use-postgresql-database-as-docker-container-with-persistent-volume","title":"Content of docker-compose.yml file to use postgresql database as docker container (with persistent volume)","text":"<pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: ccp-multi-ied-multi-plc-pg\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: powerplant\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data   # &lt;-- persistent volume for DB data\n      - ./postgres-sql:/docker-entrypoint-initdb.d  # &lt;-- init scripts\n\nvolumes:\n  postgres_data:   # &lt;-- named volume definition\n</code></pre>"},{"location":"Docker_Volumes/#use-of-volumes-inside-postgresql-docker-composeyml","title":"Use of Volumes inside PostgreSQL docker-compose.yml","text":"<pre><code>volumes:\n- postgres_data:/var/lib/postgresql/data # persistent volume for DB data\n- ./postgres-sql:/docker-entrypoint-initdb.d # init scripts\n\nvolumes:\n  postgres_data: # named volume definition \n</code></pre> <p>Explanation of each line:</p> <ol> <li> <p><code>-postgres_data:/var/lib/postgresql/data</code></p> </li> <li> <p>left side (postgres_data) -&gt; a named volume, managed by Docker</p> </li> <li> <p>you don't see it as a folder on your host - Docker stores it internally (under /var/lib/docker/volumes/...)</p> </li> <li> <p>right side (/var/lib/postgresql/data) -&gt; the main folder inside the container where the PostgreSQL stores all its databases.</p> </li> </ol> <p>This means your PostgreSQL data (tables, records, etc.) is stored in the <code>postgres_data</code> volume not inside the temporary cotainer filesystem.</p> <p>Even if you:</p> <ul> <li> <p>stop the container (docker compose down)</p> </li> <li> <p>rebuild it</p> </li> <li> <p>restart it</p> </li> </ul> <p>the data remains intact, because the volume is persistent.</p> <p>If you ever want to delete the data, you explicitly run: <code>docker compose down -v</code></p> <ol> <li> <p><code>- postgres-sql:/docker-entrypoint-initdb.d</code></p> </li> <li> <p>it mounts your host folder <code>postgres-sql/</code> (next to your <code>docker-compose.yml</code>) into the container's <code>/docker-entrypoint-initdb.d</code> folder</p> </li> <li> <p>PostgreSQL automatically runs any <code>.sql</code> files there the first time the database is initialized (before any data exists).</p> </li> </ol> <p>So if you have <code>postgres-sql/CCP_data.sql</code>, it will create your tables and insert the initial values when the container is started for the first time.</p> <ol> <li>section at the bottom:</li> </ol> <pre><code>volumes:\n  postgres_data:\n</code></pre> <p>This declares the named volume used above, so Docker knows it should create and manage it.</p>"},{"location":"Docker_Volumes/#question-how-can-i-ensure-that-these-3-containers-always-stay-running-before-i-stop-and-remove-the-containers-how-can-i-verify-whether-the-data-in-the-containers-are-persisted-ubuntupgt-docker-ps-grep-ccp-d38cf688db43-postgres16-docker-entrypoints-2-weeks-ago-up-37-minutes-00005432-5432tcp-5432-5432tcp-ccp-multi-ied-multi-plc-pg-ubuntupgt-docker-ps-grep-scada-ubuntupgt-docker-ps-a-grep-scada-017a83b809c7-scadaltsscadaltslatest-catalinash-run-2-days-ago-exited-143-45-minutes-ago-scadalts-ubuntupgt-docker-ps-a-grep-mysql-8ddf0f1a81bf-mysqlmysql-server57-entrypointsh-mysq-2-days-ago-exited-0-45-minutes-ago-mysql-ubuntupgt","title":"Question: how can I ensure that these 3 containers always stay running ? before I stop and remove the containers how can I verify whether the data in the containers are persisted ? ubuntu@pgt:~$ docker ps | grep ccp d38cf688db43 postgres:16 \"docker-entrypoint.s\u2026\" 2 weeks ago Up 37 minutes 0.0.0.0:5432-&gt;5432/tcp, [::]:5432-&gt;5432/tcp ccp-multi-ied-multi-plc-pg ubuntu@pgt:~$ docker ps | grep scada ubuntu@pgt:~$ docker ps -a | grep scada 017a83b809c7 scadalts/scadalts:latest \"catalina.sh run\" 2 days ago Exited (143) 45 minutes ago scadalts ubuntu@pgt:~$ docker ps -a | grep mysql 8ddf0f1a81bf mysql/mysql-server:5.7 \"/entrypoint.sh mysq\u2026\" 2 days ago Exited (0) 45 minutes ago mysql ubuntu@pgt:~$","text":"<p>Here is the safe and correct workflow to ensure your 3 containers always stay running WITHOUT LOSING DATA.</p> <p>We will do it in 3 steps:</p> <p>\u2705 Step 1 \u2014 Check if your containers already use volumes \u2705 Step 2 \u2014 Back up data (if needed) \u2705 Step 3 \u2014 Recreate with restart policies (<code>--restart unless-stopped</code>)</p>"},{"location":"Docker_Volumes/#step-1-check-whether-your-containers-use-volumes","title":"\u2705 Step 1 \u2014 Check whether your containers use volumes","text":"<p>Before removing anything, you MUST confirm that:</p> <p>\u2705 the data lives in Docker volumes (safe) \u274c or the data is stored inside the container filesystem (unsafe \u2014 will be lost if container is deleted)</p>"},{"location":"Docker_Volumes/#11-check-volumes-for-each-container","title":"\u2705 1.1 Check volumes for each container","text":""},{"location":"Docker_Volumes/#scadalts","title":"ScadaLTS","text":"<pre><code>docker inspect scadalts | grep -i mount -A 10\n</code></pre>"},{"location":"Docker_Volumes/#mysql-57","title":"MySQL 5.7","text":"<pre><code>docker inspect mysql | grep -i mount -A 10\n</code></pre>"},{"location":"Docker_Volumes/#matlab-container","title":"MATLAB container","text":"<pre><code>docker inspect matlab_container | grep -i mount -A 10\n</code></pre>"},{"location":"Docker_Volumes/#what-you-want-to-see","title":"\u2705 What you want to see:","text":"<p>Example (this is good):</p> <pre><code>\"Mounts\": [\n    {\n        \"Type\": \"volume\",\n        \"Name\": \"mysql_data\",\n        \"Destination\": \"/var/lib/mysql\"\n    }\n]\n</code></pre> <p>If you see <code>\"Type\": \"volume\"</code> \u2192 your data is safe. If instead you see <code>\"Type\": \"bind\"</code> \u2192 data is also safe (stored on host). If you see empty Mounts: \u2192 dangerous, all data is inside the container and will be lost if removed.</p> <p>Let me know what your actual outputs are.</p>"},{"location":"Docker_Volumes/#step-2-back-up-the-data-if-your-volumes-are-missing","title":"\u2705 Step 2 \u2014 Back up the data (if your volumes are missing)","text":"<p>If Mounts is empty, you must back up first.</p>"},{"location":"Docker_Volumes/#mysql-backup","title":"\u2705 MySQL backup","text":"<pre><code>docker start mysql\ndocker exec -it mysql mysqldump -u root -p --all-databases &gt; mysql_backup.sql\n</code></pre>"},{"location":"Docker_Volumes/#scadalts-backup","title":"\u2705 ScadaLTS backup","text":"<p>It stores data inside:</p> <ul> <li><code>/usr/local/tomcat/webapps/Scada-LTS/WEB-INF/classes</code></li> <li><code>/opt/scadalts</code> depending on version</li> </ul> <p>You can copy the data out:</p> <pre><code>docker cp scadalts:/usr/local/tomcat/webapps/Scada-LTS ./scadalts_backup\n</code></pre>"},{"location":"Docker_Volumes/#matlab-container_1","title":"\u2705 MATLAB container","text":"<p>Only needed if you stored data inside the container:</p> <pre><code>docker cp matlab_container:/home/matlab ./matlab_backup\n</code></pre>"},{"location":"Docker_Volumes/#step-3-make-containers-always-running","title":"\u2705 Step 3 \u2014 Make containers always running","text":"<p>Once volume safety is confirmed, you can recreate the containers with:</p> <p>\u2705 auto-restart \u2705 persistent data volumes \u2705 correct ports</p>"},{"location":"Docker_Volumes/#31-apply-restart-policies-without-recreating-if-possible","title":"\u2705 3.1 Apply restart policies without recreating (if possible)","text":"<p>You can add restart policies even to existing containers:</p> <pre><code>docker update --restart unless-stopped scadalts\ndocker update --restart unless-stopped mysql\ndocker update --restart unless-stopped matlab_container\n</code></pre>"},{"location":"Docker_Volumes/#check-theyre-applied","title":"\u2705 Check they\u2019re applied","text":"<pre><code>docker inspect scadalts | grep RestartPolicy -A 3\n</code></pre> <p>Expected:</p> <pre><code>\"RestartPolicy\": {\n    \"Name\": \"unless-stopped\",\n    \"MaximumRetryCount\": 0\n}\n</code></pre> <p>This avoids deleting the containers at all. Data stays untouched.</p>"},{"location":"Docker_Volumes/#32-ensure-they-start-now","title":"\u2705 3.2 Ensure they start now","text":"<p>Start them manually:</p> <pre><code>docker start scadalts\ndocker start mysql\ndocker start matlab_container\n</code></pre>"},{"location":"Docker_Volumes/#33-reboot-test","title":"\u2705 3.3 Reboot test","text":"<pre><code>sudo reboot\n</code></pre> <p>Then run:</p> <pre><code>docker ps -a\n</code></pre> <p>All three should show: Up X minutes (restart policy: unless-stopped)</p>"},{"location":"Docker_Volumes/#summary-safest-path-for-you","title":"\u2705 Summary \u2014 safest path for you","text":"<p>\u2705 Check existing volumes (prevent data loss) \u2705 Add restart policies using <code>docker update</code> \u2705 No need to delete or recreate containers \u2705 Reboot VM \u2192 containers auto-start</p> <p>Reply with the output of:</p> <pre><code>docker inspect scadalts | grep -i mount -A 10\ndocker inspect mysql | grep -i mount -A 10\ndocker inspect matlab_container | grep -i mount -A 10\n</code></pre> <p>I will tell you exactly whether your data is safe and what to do next.</p>"},{"location":"Docker_Volumes/#output-of-commands-above","title":"Output of commands above","text":"<p>based on the output below is my data safe in all 3 containers if I stop and restart them?</p> <pre><code>ubuntu@pgt:~$ docker inspect scadalts | grep -i mount -A 10\n        \"MountLabel\": \"\",\n        \"ProcessLabel\": \"\",\n        \"AppArmorProfile\": \"docker-default\",\n        \"ExecIDs\": null,\n        \"HostConfig\": {\n            \"Binds\": [\n                \"scadalts_scadalts-data:/usr/local/tomcat/webapps/Scada-LTS:rw\"\n            ],\n            \"ContainerIDFile\": \"\",\n            \"LogConfig\": {\n                \"Type\": \"json-file\",\n--\n        \"Mounts\": [\n            {\n                \"Type\": \"volume\",\n                \"Name\": \"scadalts_scadalts-data\",\n                \"Source\": \"/var/lib/docker/volumes/scadalts_scadalts-data/_data\",\n                \"Destination\": \"/usr/local/tomcat/webapps/Scada-LTS\",\n                \"Driver\": \"local\",\n                \"Mode\": \"rw\",\n                \"RW\": true,\n                \"Propagation\": \"\"\n            }\nubuntu@pgt:~$ docker inspect mysql | grep -i mount -A 10\n        \"MountLabel\": \"\",\n        \"ProcessLabel\": \"\",\n        \"AppArmorProfile\": \"docker-default\",\n        \"ExecIDs\": null,\n        \"HostConfig\": {\n            \"Binds\": [\n                \"scadalts_mysql-data:/var/lib/mysql:rw\",\n                \"/home/ubuntu/scadaLTS/scadalts_dump.sql:/docker-entrypoint-initdb.d/scadalts_dump.sql:rw\"\n            ],\n            \"ContainerIDFile\": \"\",\n            \"LogConfig\": {\n--\n        \"Mounts\": [\n            {\n                \"Type\": \"bind\",\n                \"Source\": \"/home/ubuntu/scadaLTS/scadalts_dump.sql\",\n                \"Destination\": \"/docker-entrypoint-initdb.d/scadalts_dump.sql\",\n                \"Mode\": \"rw\",\n                \"RW\": true,\n                \"Propagation\": \"rprivate\"\n            },\n            {\n                \"Type\": \"volume\",\nubuntu@pgt:~$ docker inspect matlab_container | grep -i mount -A 10\n        \"MountLabel\": \"\",\n        \"ProcessLabel\": \"\",\n        \"AppArmorProfile\": \"docker-default\",\n        \"ExecIDs\": null,\n        \"HostConfig\": {\n            \"Binds\": [\n                \"/home/ubuntu/matlab_projects:/home/matlab/projects\",\n                \"/home/ubuntu/database_toolbox:/usr/local/MATLAB/R2025a/toolbox/database\",\n                \"/home/ubuntu/matlab_licenses:/usr/local/MATLAB/R2025a/licenses\",\n                \"/home/ubuntu/matlab_local:/usr/local/MATLAB/R2025a/toolbox/local\"\n            ],\n--\n        \"Mounts\": [\n            {\n                \"Type\": \"bind\",\n                \"Source\": \"/home/ubuntu/matlab_projects\",\n                \"Destination\": \"/home/matlab/projects\",\n                \"Mode\": \"\",\n                \"RW\": true,\n                \"Propagation\": \"rprivate\"\n            },\n            {\n                \"Type\": \"bind\",\nubuntu@pgt:~$ \n</code></pre> <p>\u2705 Yes \u2014 all your data is safe in all 3 containers. Here is the breakdown, container by container, so you can be confident nothing will be lost even if you recreate or restart them.</p>"},{"location":"Docker_Volumes/#1-scada-lts-container-data-is-safe","title":"\u2705 1. SCADA-LTS Container \u2014 Data is SAFE","text":"<p>You have this mount:</p> <pre><code>\"Type\": \"volume\",\n\"Name\": \"scadalts_scadalts-data\",\n\"Destination\": \"/usr/local/tomcat/webapps/Scada-LTS\"\n</code></pre> <p>\u2705 This is a Docker-managed volume \u2705 All SCADA-LTS application files, configs, DB files (for embedded DB, if any), and runtime data are stored in:</p> <pre><code>/var/lib/docker/volumes/scadalts_scadalts-data/_data\n</code></pre> <p>This will persist across container removal, recreation, and reboots.</p> <p>\u2705 SCADALTS data is safe.</p>"},{"location":"Docker_Volumes/#2-mysql-container-data-is-safe","title":"\u2705 2. MySQL Container \u2014 Data is SAFE","text":"<p>You have two mounts:</p>"},{"location":"Docker_Volumes/#mysql-data-directory-volume-critical-one","title":"\u2705 MySQL data directory volume (critical one)","text":"<pre><code>\"Type\": \"volume\",\n\"Name\": \"scadalts_mysql-data\",\n\"Destination\": \"/var/lib/mysql\"\n</code></pre> <p>This means your actual MySQL database files (<code>.frm</code>, <code>.ibd</code>, logs, binary logs, etc.) are safely stored in a Docker volume.</p> <p>\u2705 This is the most important part \u2014 your database content is protected.</p>"},{"location":"Docker_Volumes/#sql-dump-bind-mount","title":"\u2705 SQL dump bind mount","text":"<pre><code>\"Type\": \"bind\",\n\"Source\": \"/home/ubuntu/scadaLTS/scadalts_dump.sql\"\n</code></pre> <p>This is only for initial database import and is not overwritten. It does not affect or replace your real MySQL data.</p> <p>\u2705 Your MySQL database is safe and future-proof.</p>"},{"location":"Docker_Volumes/#3-matlab-container-data-is-safe","title":"\u2705 3. MATLAB Container \u2014 Data is SAFE","text":"<p>Everything important is mounted via bind mounts:</p> <pre><code>/home/ubuntu/matlab_projects \u2192 /home/matlab/projects\n/home/ubuntu/database_toolbox \u2192 /usr/local/MATLAB/...\n/home/ubuntu/matlab_licenses \u2192 /usr/local/MATLAB/...\n/home/ubuntu/matlab_local \u2192 /usr/local/MATLAB/...\n</code></pre> <p>\u2705 All your projects, toolbox files, and license files are stored on the host filesystem.</p> <p>None of your work is inside the container\u2019s internal filesystem.</p> <p>\u2705 MATLAB data is safe.</p>"},{"location":"Docker_Volumes/#final-summary-you-are-safe-to-restart-recreate-or-apply-restart-policies","title":"\u2705 \u2705 Final Summary \u2014 You are SAFE to Restart, Recreate, or Apply Restart Policies","text":"Container Uses Docker Volume? Uses Bind Mount? Is Data Safe? SCADA-LTS \u2705 Yes \u274c No \u2705 Safe MySQL \u2705 Yes (DB data) \u2705 Yes (SQL file only) \u2705 Safe MATLAB \u274c No volume \u2705 All data in bind mounts \u2705 Safe <p>\ud83d\udc49 All 3 containers store their important data outside the container filesystem. So even if you delete the containers (<code>docker rm</code>), your data stays intact.</p>"},{"location":"Docker_Volumes/#next-steps-safe-to-do-now","title":"\u2705 Next Steps (Safe to Do Now)","text":"<p>You can now safely apply restart policies:</p> <pre><code>docker update --restart unless-stopped scadalts\ndocker update --restart unless-stopped mysql\ndocker update --restart unless-stopped matlab_container\n</code></pre> <p>Then:</p> <pre><code>docker start scadalts\ndocker start mysql\ndocker start matlab_container\n</code></pre> <p>After reboot:</p> <pre><code>docker ps -a\n</code></pre> <p>All three should show \u201cUp \u2026\u201d \u2014 meaning they auto-started.</p> <p>If you want, I can also help you:</p> <p>\u2705 convert this setup into a docker-compose.yml \u2705 simplify all services into a clean, reproducible stack \u2705 automatically restore on reboot</p> <p>Just let me know.</p>"},{"location":"Docker_tgz/","title":"Using tarballs (.tgz)","text":""},{"location":"Docker_tgz/#what-is-a-docker-tgz-file","title":"What is a Docker .tgz file ?","text":"<p>In the Docker context, a <code>.tgz</code> file is a compressed archive of a Docker image created with:</p> <pre><code>  docker save -o image.tar &lt;image:tag&gt;\n\n  # then gzip it\n\n  tar czf image.tgz image.tar\n</code></pre> <p>Or directly:</p> <p><code>docker save &lt;image:tag&gt; | gzip &gt; image.tgz</code></p> <p>Inside the <code>.tgz</code>: - it is basically a tarball of JSON metadata and filesystem layer tarballs</p>"},{"location":"Docker_tgz/#to-make-modifications-to-docker-tgz-file","title":"To make modifications to Docker .tgz file","text":"<ul> <li>load the <code>.tgz</code> file into Docker (<code>docker load</code>)</li> <li>run a container from it</li> <li>make changes</li> <li><code>docker commit</code></li> <li><code>docker save</code></li> </ul>"},{"location":"Docker_tgz/#do-changes-to-tgz-persist","title":"Do changes to .tgz persist?","text":"<ul> <li>if you only change inside the running container from image, these changes don't go back into the .tgz. They live in the container's writable layer.</li> <li>To persist changes into a new <code>.tgz</code>:</li> <li>Run a container from the image</li> <li>Make modifications</li> <li>Commit the container -&gt; new image   <code>docker commit &lt;container-id&gt; myimage:modified</code></li> <li>Save it back:   <code>docker save myimage:modified | gzip &gt; myimage-modified.tgz</code></li> </ul> <p>## How .tgz files work in practice  - <code>.tgz</code> = portable snapshot of a Docker image  - you can transfer it, then use docker load &lt; my image.tgz to load it into Docker on another host  -  mostly for air-gapped environments (no Docker Hub access) or backups.</p>"},{"location":"Docker_tgz/#to-load-the-tgz-into-docker","title":"To load the .tgz into Docker","text":"<p><code>docker load &lt; myimage.tgz</code></p> <ul> <li>makes the image available locally so you can run containers from it</li> </ul>"},{"location":"Docker_tgz/#to-create-a-tgz-from-an-existing-image","title":"To create a .tgz from an existing image","text":"<p><code>docker save myimage:tag | gzip &gt; myimage.tgz</code> - now you can copy over myimage.tgz to another machine and docker load it there</p> <p>sreedevirajavelu@srees-MacBook-Air pgtwin_7ss % docker load -i 7ss_db_docker_20250218.tgz  unrecognized image format</p> <p>That error means the file 7ss_db_docker_20250218.tgz is not in the expected Docker save/load format. Let\u2019s debug step by step:</p> <ol> <li>What docker load expects <code>docker load -i file.tgz</code> only works if the archive was created using:</li> </ol> <p><code>docker save -o image_name.tgz image:tag</code></p> <p>This produces a tarball with manifest.json, repositories, and layer directories inside.</p> <p>If your .tgz was created another way (e.g., tar czf on a Docker context or folder), docker load will reject it.</p> <ol> <li>Check the contents of your .tgz</li> </ol> <p>Run on your MacBook: tar -tzf 7ss_db_docker_20250218.tgz | head -20 If you see manifest.json and layer directories \u2192 \u2705 this is a docker save image, and something else is wrong. If you see Dockerfile, requirements.txt, Panda-db/, etc. \u2192 \u274c this is just the build context, not an image.</p> <ol> <li>If it\u2019s a Docker context (not an image)</li> </ol> <p>You cannot load it directly. Instead you need to build: tar -xzf 7ss_db_docker_20250218.tgz -C ./extracted/ cd extracted docker build -t my-image:latest .</p> <ol> <li>If it is a valid docker save image</li> </ol> <p>Try re-loading: gunzip -c 7ss_db_docker_20250218.tgz | docker load (some tools produce gzip-compressed vs tar-compressed files differently).</p> <p>sreedevirajavelu@srees-MacBook-Air pgtwin_7ss % tar -tzf 7ss_db_docker_20250218.tgz | head -20</p> <p>7ss_db/</p> <p>7ss_db/docker-sql/</p> <p>7ss_db/docker-sql/pgtv4_pp_7ss_db_20240302.sql</p> <p>7ss_db/docker-sql/my.cnf</p> <p>7ss_db/docker-sql/build_fixed_max_conn.sh</p> <p>7ss_db/docker-sql/build.sh</p> <p>7ss_db/docker-sql/pandapower_db_initial.sql</p> <p>7ss_db/docker-sql/prepare_max_connections.py</p> <p>7ss_db/docker-sql/README.md</p> <p>7ss_db/docker-sql/pandapower_db_structure.sql</p> <p>7ss_db/docker-sql/Dockerfile</p> <p>7ss_db/docker-sql/run_db.sh</p> <p>sreedevirajavelu@srees-MacBook-Air pgtwin_7ss % </p>"},{"location":"File_Transfer_Host_To_Remote/","title":"File Transfer Host To Remote","text":""},{"location":"File_Transfer_Host_To_Remote/#question","title":"Question:","text":"<p>I have ssh-ed into an ubuntu vm at 192.168.30.60 using vpn tunnel I want to transfer a folder from my local host into the ubuntu vm, how can I do this?</p> <p>Perfect \u2014 since you already SSH-ed into the Ubuntu VM at <code>192.168.30.60</code> via a VPN tunnel, there are several reliable ways to transfer a folder from your local host (Mac) to that remote Ubuntu VM.</p>"},{"location":"File_Transfer_Host_To_Remote/#option-1-use-scp-simple-and-recommended","title":"\ud83d\udfe2 Option 1 \u2014 Use <code>scp</code> (Simple and Recommended)","text":"<p><code>scp</code> (secure copy) is the easiest way to transfer files or folders over SSH.</p>"},{"location":"File_Transfer_Host_To_Remote/#to-copy-a-folder-from-your-local-mac-ubuntu-vm","title":"\ud83d\udcc1 To copy a folder from your local Mac \u2192 Ubuntu VM","text":"<pre><code>scp -r /path/to/local/folder ubuntu@192.168.30.60:/home/ubuntu/\n</code></pre> <p>Example:</p> <pre><code>scp -r ~/Documents/myproject ubuntu@192.168.30.60:/home/ubuntu/\n</code></pre> <ul> <li><code>-r</code> \u2192 recursive (for directories)</li> <li><code>ubuntu</code> \u2192 username on the VM</li> <li><code>/home/ubuntu/</code> \u2192 destination path inside the VM</li> </ul> <p>It will prompt for your SSH password or use your SSH key automatically if already set up.</p>"},{"location":"File_Transfer_Host_To_Remote/#option-2-use-rsync-faster-and-resumable","title":"\ud83d\udfe3 Option 2 \u2014 Use <code>rsync</code> (faster and resumable)","text":"<p>If the folder is large or you might re-transfer updates, use <code>rsync</code>:</p> <pre><code>rsync -avz ~/Documents/myproject ubuntu@192.168.30.60:/home/ubuntu/\n</code></pre> <ul> <li><code>-a</code> \u2192 archive mode (preserve permissions, etc.)</li> <li><code>-v</code> \u2192 verbose</li> <li><code>-z</code> \u2192 compress during transfer (good for VPNs)</li> </ul> <p>If interrupted, you can rerun the same command \u2014 it resumes efficiently.</p>"},{"location":"File_Transfer_Host_To_Remote/#option-3-if-youre-already-sshed-in-using-terminal","title":"\ud83d\udfe0 Option 3 \u2014 If you\u2019re already SSH\u2019ed in (using terminal)","text":"<p>If you are inside the SSH session and want to pull files from your local machine, you need to open a new terminal on your local Mac and run the <code>scp</code> or <code>rsync</code> command from your Mac, not from within SSH.</p> <p>(SSH connections are one-way for file transfer; the copy command must be run from the source side.)</p>"},{"location":"File_Transfer_Host_To_Remote/#option-4-using-visual-tools-optional","title":"\ud83d\udd35 Option 4 \u2014 Using Visual Tools (Optional)","text":"<p>If you prefer GUI methods:</p> <ul> <li>Use VS Code Remote SSH extension \u2192 open the Ubuntu VM and drag files.</li> <li> <p>Or use FileZilla:</p> </li> <li> <p>Protocol: SFTP</p> </li> <li>Host: <code>192.168.30.60</code></li> <li>Username: <code>ubuntu</code></li> <li>Password: (your SSH password)</li> <li>Port: <code>22</code></li> </ul>"},{"location":"File_Transfer_Host_To_Remote/#tip","title":"\ud83e\udde0 Tip","text":"<p>You can verify the transfer:</p> <pre><code>ssh ubuntu@192.168.30.60\nls -l /home/ubuntu/\n</code></pre> <p>You should see your folder there.</p> <p>Would you like me to show you how to copy the folder in reverse (from the Ubuntu VM \u2192 your Mac) as well?</p>"},{"location":"File_Transfer_Host_To_Remote/#question_1","title":"Question:","text":"<p>Based on the details below, how should I transfer the files from the local folder to the remote VM at  192.168.30.60</p> <p>path to local folder :</p> <p>/Users/sreedevirajavelu/Desktop/POWER PLANT TWIN/ccp_multi_IED_multi_plc_database_pg</p> <p>at remote VM:</p> <p>ubuntu@pgt:~$ whoami</p> <p>ubuntu</p> <p>Perfect \ud83d\udc4d You\u2019ve already got all the info needed. Here\u2019s exactly how to transfer your local folder to your remote Ubuntu VM at <code>192.168.30.60</code>.</p>"},{"location":"File_Transfer_Host_To_Remote/#command-to-use-from-your-local-host-terminal","title":"\u2705 Command to use (from your local host terminal):","text":"<pre><code>scp -r \"/Users/sreedevirajavelu/Desktop/POWER PLANT TWIN/ccp_multi_IED_multi_plc_database_pg\" ubuntu@192.168.30.60:/home/ubuntu/\n</code></pre>"},{"location":"File_Transfer_Host_To_Remote/#explanation","title":"\ud83e\udde0 Explanation:","text":"<ul> <li><code>scp</code> \u2192 Secure copy over SSH</li> <li><code>-r</code> \u2192 Recursively copy directories and their contents</li> <li><code>\"...\"</code> \u2192 Quotes around the path because your folder name contains spaces (<code>POWER PLANT TWIN</code>)</li> <li><code>ubuntu@192.168.30.60</code> \u2192 Username and target host</li> <li><code>/home/ubuntu/</code> \u2192 Destination path on the VM (your home directory)</li> </ul>"},{"location":"File_Transfer_Host_To_Remote/#to-verify-after-copying","title":"\ud83d\udee0 To verify after copying:","text":"<p>Once the command completes, SSH into your VM (if you aren\u2019t already) and run:</p> <pre><code>ls /home/ubuntu/\n</code></pre> <p>You should see:</p> <pre><code>ccp_multi_IED_multi_plc_database_pg\n</code></pre>"},{"location":"File_Transfer_Host_To_Remote/#if-youre-connected-via-vpn","title":"\u26a0\ufe0f If you\u2019re connected via VPN:","text":"<p>Make sure your VPN tunnel allows SSH/SCP traffic to <code>192.168.30.60</code>. If <code>scp</code> hangs or times out, you can test connectivity first:</p> <pre><code>ssh ubuntu@192.168.30.60\n</code></pre> <p>If SSH works, <code>scp</code> will work too.</p> <p>Would you like to copy it to a specific directory inside the VM (e.g., <code>/opt/projects/</code> or <code>/srv/plc/</code>) instead of your home folder? I can adjust the command accordingly.</p>"},{"location":"Fix_Slow_UbuntuVM_Boot/","title":"Fix slow booting for ubuntu vm based on output of these commands to find what was slowing down the booting time","text":"<pre><code> systemd-analyze\n systemd-analyze blame\n</code></pre> <p>Top delay:</p> <p>2min 258ms systemd-networkd-wait-online.service</p> <p>This service alone adds over 2 minutes to your boot. That\u2019s practically your entire userspace delay.</p> <p>\ud83d\udd0d What is systemd-networkd-wait-online.service?</p> <p>It waits for the network to be fully up (like getting a DHCP lease or full connection). Used mostly on servers to ensure critical services only start after network is available. In most VMs and desktops, it's not necessary and can be safely disabled.</p> <p>\u2705 Fix: Disable the Wait-Online Service</p> <p>Run: - sudo systemctl disable systemd-networkd-wait-online.service - sudo systemctl mask systemd-networkd-wait-online.service</p> <p>\ud83d\udca1 mask ensures it cannot be accidentally started again.</p> <p>If you also see or use NetworkManager, you can disable its wait service too:</p> <p>sudo systemctl disable NetworkManager-wait-online.service</p> <p>sudo systemctl mask NetworkManager-wait-online.service</p> <p>\ud83d\udd01 Then reboot your VM: sudo reboot \ud83e\uddea After reboot:</p> <p>Check again: systemd-analyze You should see your boot time drop from 2+ minutes \u2192 ~15 seconds or less.</p> <p>\ud83d\udd01 To Undo the Effects of Disabling and Masking the Wait-Online Service If you previously ran:</p> <ul> <li> <p>sudo systemctl disable systemd-networkd-wait-online.service</p> </li> <li> <p>sudo systemctl mask systemd-networkd-wait-online.service</p> </li> </ul> <p>You can restore the service like this:</p> <p>\u2705 Step 1: Unmask the service</p> <ul> <li>sudo systemctl unmask systemd-networkd-wait-online.service</li> </ul> <p>\u2705 Step 2: Re-enable the service at boot</p> <ul> <li>sudo systemctl enable systemd-networkd-wait-online.service \u2705 Step 3 (Optional): Start it immediately (not needed for reboot use)</li> </ul> <p>sudo systemctl start systemd-networkd-wait-online.service</p> <p>\ud83d\udd01 Same steps apply if you disabled NetworkManager-wait-online.service: Just replace the service name in each command. Example:</p> <p>sudo systemctl unmask NetworkManager-wait-online.service</p> <p>sudo systemctl enable NetworkManager-wait-online.service</p> <p>\ud83e\uddea Confirm it's active again:</p> <p>systemctl status systemd-networkd-wait-online.service or systemctl is-enabled systemd-networkd-wait-online.service</p> <p>You should see:</p> <p><code>enabled</code></p> <p>\u2705 Summary: Rollback Commands</p>"},{"location":"Fix_Slow_UbuntuVM_Boot/#unmask-and-re-enable-systemd-networkd-wait-online","title":"Unmask and re-enable systemd-networkd-wait-online","text":"<p>sudo systemctl unmask systemd-networkd-wait-online.service</p> <p>sudo systemctl enable systemd-networkd-wait-online.service</p>"},{"location":"Fix_Slow_UbuntuVM_Boot/#optional-also-for-networkmanager-wait-online-if-previously-masked","title":"Optional: Also for NetworkManager-wait-online (if previously masked)","text":"<p>sudo systemctl unmask NetworkManager-wait-online.service</p> <p>sudo systemctl enable NetworkManager-wait-online.service</p>"},{"location":"Linux_System_Administration/","title":"Linux System Administration","text":""},{"location":"Linux_System_Administration/#linux-system-administration","title":"Linux System Administration","text":"<p>Perfect, Sree Devi \u2014 this is exactly the right area to master before your Meta SRE interview. The interviewer will not just ask you definitions, but test how you think, observe, and troubleshoot in a live Linux environment.</p> <p>Let\u2019s go step-by-step through the core Linux system administration concepts you mentioned \u2014 what each is, how it works, and how to use it in interview scenarios \ud83d\udc47</p>"},{"location":"Linux_System_Administration/#1-linux-processes","title":"\ud83e\udde9 1. Linux Processes","text":""},{"location":"Linux_System_Administration/#what-they-are","title":"\ud83d\udd39 What They Are","text":"<p>A process is an instance of a running program. Every time you run a command or application, Linux creates a process with a PID (Process ID).</p> <p>Each process has:</p> <ul> <li>A parent process (PPID) \u2014 the process that spawned it.</li> <li>A user who owns it.</li> <li>A state (running, sleeping, stopped, zombie).</li> <li>CPU/memory usage info.</li> </ul>"},{"location":"Linux_System_Administration/#key-commands","title":"\ud83d\udd39 Key Commands","text":""},{"location":"Linux_System_Administration/#ps-process-status","title":"<code>ps</code> \u2014 process status","text":"<p>Shows currently running processes.</p> <pre><code>ps aux        # show all processes with detailed info\nps -ef        # alternative format, common in system admin\nps -u sree    # show processes by specific user\n</code></pre> <p>Example output fields:</p> <pre><code>USER  PID  %CPU  %MEM  VSZ  RSS  TTY  STAT  START  TIME  COMMAND\n</code></pre> <ul> <li>%CPU / %MEM \u2014 how much CPU/memory process uses</li> <li>STAT \u2014 process state (R = running, S = sleeping, Z = zombie)</li> </ul> <p>\u2705 Interview tip: If asked \u201chow to find what\u2019s consuming high CPU?\u201d, say: \u201cI\u2019d run <code>ps aux --sort=-%cpu | head</code> or use <code>top</code> for live view.\u201d</p>"},{"location":"Linux_System_Administration/#top-dynamic-view-of-running-processes","title":"<code>top</code> \u2014 dynamic view of running processes","text":"<p>Real-time system monitor (press <code>q</code> to quit).</p> <pre><code>top\n</code></pre> <p>Look for:</p> <ul> <li>%CPU, %MEM usage by each process</li> <li>load average (top right): indicates overall system load</li> <li>Tasks summary (running, sleeping, stopped, zombie)</li> </ul> <p>Useful keys in top:</p> <ul> <li><code>P</code> \u2013 sort by CPU</li> <li><code>M</code> \u2013 sort by memory</li> <li><code>k</code> \u2013 kill process (enter PID)</li> <li><code>1</code> \u2013 show per-CPU usage</li> </ul> <p>\u2705 Scenario example: \u201cServer feels slow \u2014 I\u2019d open <code>top</code> to check which process spikes CPU or memory, then investigate that PID with <code>ps -fp &lt;pid&gt;</code>.\u201d</p>"},{"location":"Linux_System_Administration/#process-management-commands","title":"\ud83d\udd39 Process Management Commands","text":"<pre><code>kill &lt;pid&gt;           # terminate a process\nkill -9 &lt;pid&gt;        # force kill (SIGKILL)\npkill nginx          # kill all nginx processes\nsystemctl restart nginx   # restart a service safely\n</code></pre>"},{"location":"Linux_System_Administration/#2-systemd-service-and-process-manager","title":"\u2699\ufe0f 2. systemd \u2014 Service and Process Manager","text":""},{"location":"Linux_System_Administration/#what-it-is","title":"\ud83d\udd39 What It Is","text":"<p><code>systemd</code> is the init system responsible for starting and managing services after boot.</p> <p>Services are called units, and they are defined in files ending with <code>.service</code> under <code>/etc/systemd/system</code> or <code>/lib/systemd/system</code>.</p>"},{"location":"Linux_System_Administration/#common-commands","title":"\ud83d\udd39 Common Commands","text":"<pre><code>systemctl status nginx        # view service status\nsystemctl start nginx         # start a service\nsystemctl stop nginx          # stop a service\nsystemctl restart nginx       # restart a service\nsystemctl enable nginx        # enable on boot\nsystemctl disable nginx       # disable on boot\nsystemctl list-units --type=service   # list active services\n</code></pre> <p>\u2705 Interview tip: \u201cIf a web app doesn\u2019t respond, I\u2019d check if its service is active using <code>systemctl status &lt;service&gt;</code>, review its logs using <code>journalctl -u &lt;service&gt;</code>.\u201d</p>"},{"location":"Linux_System_Administration/#3-logs-and-journalctl","title":"\ud83e\uddfe 3. Logs and journalctl","text":""},{"location":"Linux_System_Administration/#where-logs-live","title":"\ud83d\udd39 Where Logs Live","text":"<p>Linux logs are in <code>/var/log</code>. Common ones:</p> <pre><code>/var/log/syslog        # system messages (Ubuntu/Debian)\n/var/log/messages      # general logs (CentOS/RHEL)\n/var/log/auth.log      # login attempts, sudo usage\n/var/log/nginx/error.log  # web server logs\n</code></pre>"},{"location":"Linux_System_Administration/#journalctl","title":"\ud83d\udd39 <code>journalctl</code>","text":"<p>Used with <code>systemd</code> to view logs.</p> <pre><code>journalctl -xe                   # view latest logs with explanations\njournalctl -u nginx.service      # view logs for specific service\njournalctl --since \"1 hour ago\"  # filter by time\njournalctl -f                    # follow logs in real time (like tail -f)\n</code></pre> <p>\u2705 Scenario example: \u201cWhen debugging a failed service restart, I\u2019d run <code>systemctl status</code> and <code>journalctl -u &lt;service&gt;</code> to find detailed failure logs.\u201d</p>"},{"location":"Linux_System_Administration/#4-permissions-and-ownership","title":"\ud83d\udd10 4. Permissions and Ownership","text":""},{"location":"Linux_System_Administration/#linux-permission-model","title":"\ud83d\udd39 Linux Permission Model","text":"<p>Every file has:</p> <ul> <li>Owner</li> <li>Group</li> <li>Others</li> </ul> <p>Permissions:</p> <ul> <li>r \u2013 read</li> <li>w \u2013 write</li> <li>x \u2013 execute</li> </ul>"},{"location":"Linux_System_Administration/#commands","title":"\ud83d\udd39 Commands","text":"<pre><code>ls -l                 # view permissions\nchmod 755 filename    # set permissions (rwxr-xr-x)\nchown user:group file # change ownership\nsudo                  # run command as superuser\n</code></pre>"},{"location":"Linux_System_Administration/#example","title":"\ud83d\udd39 Example","text":"<pre><code>-rwxr-xr--  1 sree  devteam  1024 Oct 18 10:00 script.sh\n</code></pre> <p>\u2192 Owner can read/write/execute \u2192 Group can read/execute \u2192 Others can only read</p> <p>\u2705 Scenario example: \u201cA script fails with \u2018Permission denied\u2019. I\u2019d check permissions with <code>ls -l</code> and use <code>chmod +x script.sh</code> to make it executable.\u201d</p>"},{"location":"Linux_System_Administration/#5-resource-monitoring","title":"\ud83d\udcbb 5. Resource Monitoring","text":""},{"location":"Linux_System_Administration/#commands_1","title":"\ud83d\udd39 Commands","text":"Command Purpose <code>top</code> View CPU, memory, and load average in real time <code>free -h</code> Check memory usage (RAM, swap) <code>df -h</code> Check disk space usage <code>du -sh /var/log</code> Check folder size <code>uptime</code> Show system uptime and load average <code>vmstat</code> CPU, memory, I/O stats <code>iostat</code> Disk I/O performance <code>sar</code> Historical performance (requires <code>sysstat</code> package) <p>\u2705 Scenario: \u201cIf a service is lagging, I\u2019d check CPU/memory usage via <code>top</code>, disk space via <code>df -h</code>, and whether swap usage is high via <code>free -h</code>.\u201d</p>"},{"location":"Linux_System_Administration/#6-useful-diagnostic-tools","title":"\ud83d\udd0d 6. Useful Diagnostic Tools","text":""},{"location":"Linux_System_Administration/#lsof-list-open-files","title":"<code>lsof</code> \u2014 List Open Files","text":"<p>Shows what files or network sockets a process has open.</p> <pre><code>lsof -p &lt;pid&gt;           # files opened by specific process\nlsof -i :80             # processes using port 80\n</code></pre> <p>\u2705 Use Case: \u201cPort 8080 is in use \u2014 I\u2019d run <code>lsof -i :8080</code> to find which process occupies it.\u201d</p>"},{"location":"Linux_System_Administration/#netstat-or-ss-network-connections","title":"<code>netstat</code> or <code>ss</code> \u2014 Network Connections","text":"<p>Show network sockets and listening ports.</p> <pre><code>netstat -tulnp          # list all listening ports with process IDs\nss -tuln                # modern alternative to netstat\n</code></pre> Option Meaning <code>-t</code> TCP connections <code>-u</code> UDP connections <code>-l</code> Listening ports <code>-n</code> Show numeric addresses <code>-p</code> Show PID/program name <p>\u2705 Scenario: \u201cTo verify if Nginx is listening on port 443, I\u2019d use <code>ss -tulnp | grep 443</code>.\u201d</p>"},{"location":"Linux_System_Administration/#7-load-average","title":"\ud83d\udcca 7. Load Average","text":""},{"location":"Linux_System_Administration/#what-it-means","title":"\ud83d\udd39 What It Means","text":"<p>Appears at the top of <code>top</code> or via <code>uptime</code>:</p> <pre><code>load average: 1.24, 0.87, 0.75\n</code></pre> <p>These are average runnable processes over 1, 5, and 15 minutes.</p> <p>Interpretation:</p> <ul> <li>If you have 4 CPU cores and a load average of 1.0 \u2192 system is under 25% load.</li> <li>Load average \u2248 number of processes waiting for CPU.</li> <li>High load (&gt; number of cores) means CPU is bottlenecked or too many I/O waits.</li> </ul> <p>\u2705 Scenario example: \u201cIf load average stays above 8 on a 4-core server, I\u2019d check CPU utilization in <code>top</code> and disk I/O using <code>iostat</code> or <code>vmstat</code> to see what\u2019s causing the wait.\u201d</p>"},{"location":"Linux_System_Administration/#8-interview-demonstration-example","title":"\ud83e\udde0 8. Interview Demonstration Example","text":"<p>Question: A user reports the web service is down. What\u2019s your first step? Answer Walkthrough:</p> <ol> <li>Check if the service is active: <code>systemctl status nginx</code></li> <li>If inactive, restart it: <code>systemctl restart nginx</code></li> <li>If it fails, view logs: <code>journalctl -u nginx -xe</code></li> <li>Verify port 80 is listening: <code>ss -tulnp | grep 80</code></li> <li>Check disk space: <code>df -h</code></li> <li>Check memory &amp; CPU: <code>top</code></li> <li>Confirm no permission or config file issues: <code>ls -l /etc/nginx/nginx.conf</code></li> </ol> <p>That\u2019s the kind of structured, root-cause-first reasoning Meta\u2019s Lead SRE will look for.</p> <p>Would you like me to create a hands-on practice guide next \u2014 with example tasks (like \u201csimulate high CPU\u201d, \u201canalyze logs\u201d, \u201cfix service failure\u201d) and commands you can try on your own Linux VM or WSL? That would help you practice interactively before your interview.</p>"},{"location":"Linux_System_Administration/#explain-the-use-and-how-to-use-each-of-the-commands-you-mentioned-here","title":"Explain the use and how to use each of the commands you mentioned here","text":"<p>\u2705 Answer Outline: Check system resource usage (top, free -m, iostat, vmstat). Inspect web server logs (/var/log/nginx/access.log, /var/log/syslog). Test connectivity (ping, curl, traceroute) and DNS (dig, nslookup). Use netstat -tulpen or ss to check open ports and connections. If containerized, check container CPU/memory with docker stats. Isolate if it\u2019s network latency, disk I/O, CPU load, or app issue.</p> <p>Excellent follow-up, Sree Devi \ud83d\udc4f \u2014 these commands are exactly what an SRE at Meta (or anywhere) uses daily to troubleshoot production systems.</p> <p>Let\u2019s go step-by-step through each command, with:</p> <ul> <li>\u2705 What it does</li> <li>\u2699\ufe0f How to use it (with examples)</li> <li>\ud83e\udde0 How to interpret output in an interview scenario</li> </ul>"},{"location":"Linux_System_Administration/#1-system-resource-usage-commands","title":"\ud83e\udde9 1. System Resource Usage Commands","text":""},{"location":"Linux_System_Administration/#top-monitor-processes-and-resource-usage-in-real-time","title":"<code>top</code> \u2014 Monitor processes and resource usage in real time","text":"<p>\u2705 Purpose: See which processes are consuming CPU, memory, and how the system load changes dynamically.</p> <pre><code>top\n</code></pre> <p>Key output fields:</p> <ul> <li>%CPU, %MEM \u2192 show process resource usage</li> <li>load average: \u2192 number of runnable processes (system load)</li> <li>Tasks: \u2192 running/sleeping/zombie processes</li> </ul> <p>Interactive keys:</p> <ul> <li><code>P</code> \u2192 sort by CPU</li> <li><code>M</code> \u2192 sort by memory</li> <li><code>k</code> \u2192 kill a process (enter PID)</li> <li><code>1</code> \u2192 show per-CPU stats</li> </ul> <p>Example: If the system feels slow, look for a process consuming &gt;90% CPU or memory. You might say in the interview:</p> <p>\u201cI\u2019d run <code>top</code> to check for processes consuming high CPU or memory and use <code>k</code> to kill runaway ones.\u201d</p>"},{"location":"Linux_System_Administration/#free-m-show-memory-usage","title":"<code>free -m</code> \u2014 Show memory usage","text":"<p>\u2705 Purpose: Display total, used, and available RAM.</p> <pre><code>free -m\n</code></pre> <ul> <li><code>-m</code> \u2192 show memory in megabytes (<code>-h</code> for human-readable)</li> </ul> <p>Output example:</p> <pre><code>              total    used    free   shared  buff/cache  available\nMem:           7972    2564    1504      120        3904       5120\nSwap:          2048       0    2048\n</code></pre> <p>Interpretation:</p> <ul> <li><code>used</code> \u2192 memory actively in use</li> <li><code>buff/cache</code> \u2192 memory used for caching (reclaimable)</li> <li><code>available</code> \u2192 memory apps can still use</li> </ul> <p>\u2705 \u201cIf memory usage is near 100% and swap is active, it\u2019s likely a memory leak or overloaded system.\u201d</p>"},{"location":"Linux_System_Administration/#iostat-monitor-disk-io-inputoutput-performance","title":"<code>iostat</code> \u2014 Monitor disk I/O (input/output performance)","text":"<p>\u2705 Purpose: Identify I/O bottlenecks \u2014 slow disks or too many reads/writes.</p> <p>Install (if missing):</p> <pre><code>sudo apt install sysstat\n</code></pre> <p>Run:</p> <pre><code>iostat -x 2 3\n</code></pre> <ul> <li><code>-x</code> \u2192 extended stats</li> <li><code>2 3</code> \u2192 update every 2 seconds, 3 times</li> </ul> <p>Key columns:</p> <ul> <li><code>%util</code> \u2192 how busy the disk is (close to 100% = bottleneck)</li> <li><code>r/s</code>, <code>w/s</code> \u2192 read/write requests per second</li> <li><code>await</code> \u2192 average wait time per I/O request</li> </ul> <p>\u2705 \u201cIf <code>%util</code> &gt; 90%, the disk is saturated \u2014 I\u2019d check which process is writing heavily using <code>iotop</code> or <code>lsof</code>.\u201d</p>"},{"location":"Linux_System_Administration/#vmstat-system-performance-overview-cpu-memory-io","title":"<code>vmstat</code> \u2014 System performance overview (CPU, memory, I/O)","text":"<p>\u2705 Purpose: Gives a quick view of CPU usage, swap activity, and system processes.</p> <pre><code>vmstat 2 5\n</code></pre> <ul> <li>Every 2 seconds, show stats 5 times.</li> </ul> <p>Key columns:</p> <ul> <li><code>r</code> \u2192 runnable processes</li> <li><code>b</code> \u2192 processes in uninterruptible sleep (I/O wait)</li> <li><code>si/so</code> \u2192 swap in/out (should be 0 ideally)</li> <li><code>us</code>/<code>sy</code> \u2192 CPU time spent in user/system mode</li> <li><code>wa</code> \u2192 time spent waiting on I/O</li> </ul> <p>\u2705 \u201cIf I see high <code>wa</code> (I/O wait) in <code>vmstat</code>, I know disk I/O is the bottleneck, not CPU.\u201d</p>"},{"location":"Linux_System_Administration/#2-inspect-logs","title":"\ud83e\uddfe 2. Inspect Logs","text":""},{"location":"Linux_System_Administration/#check-web-server-and-system-logs","title":"Check web server and system logs","text":"<p>Default paths:</p> <pre><code>/var/log/nginx/access.log\n/var/log/nginx/error.log\n/var/log/syslog\n</code></pre> <p>Common commands:</p> <pre><code>tail -n 50 /var/log/nginx/error.log      # show last 50 lines\ntail -f /var/log/nginx/access.log        # live log updates\ngrep \"500\" /var/log/nginx/access.log     # filter HTTP 500 errors\nless /var/log/syslog                     # browse system logs\n</code></pre> <p>\u2705 Interview answer example: \u201cIf the app returns 500 errors, I\u2019d check <code>/var/log/nginx/error.log</code> and use <code>grep</code> to filter out error patterns.\u201d</p>"},{"location":"Linux_System_Administration/#3-test-connectivity-and-dns","title":"\ud83c\udf10 3. Test Connectivity and DNS","text":""},{"location":"Linux_System_Administration/#ping-check-if-a-host-is-reachable","title":"<code>ping</code> \u2014 Check if a host is reachable","text":"<p>\u2705 Purpose: Test basic network connectivity using ICMP.</p> <pre><code>ping google.com\n</code></pre> <ul> <li>Sends ICMP echo requests; reports response time and packet loss.</li> <li><code>Ctrl + C</code> to stop.</li> </ul> <p>Output example:</p> <pre><code>64 bytes from 8.8.8.8: icmp_seq=1 ttl=115 time=14.3 ms\n</code></pre> <p>\u2705 \u201cIf ping fails, it could be a DNS issue, routing problem, or firewall blocking ICMP.\u201d</p>"},{"location":"Linux_System_Administration/#curl-test-https-connectivity","title":"<code>curl</code> \u2014 Test HTTP(S) connectivity","text":"<p>\u2705 Purpose: Simulate web requests and see responses (great for web app debugging).</p> <pre><code>curl -I https://example.com\n</code></pre> <ul> <li><code>-I</code> \u2192 fetch headers only</li> <li><code>-v</code> \u2192 verbose mode</li> <li><code>-L</code> \u2192 follow redirects</li> </ul> <p>Example:</p> <pre><code>curl -v http://localhost:8080\n</code></pre> <p>\u2705 \u201cIf the app returns 500, I\u2019d curl the endpoint to check if backend is reachable or misconfigured.\u201d</p>"},{"location":"Linux_System_Administration/#traceroute-trace-network-path-to-a-destination","title":"<code>traceroute</code> \u2014 Trace network path to a destination","text":"<p>\u2705 Purpose: Identify where packets are delayed or dropped along a route.</p> <pre><code>sudo apt install traceroute\ntraceroute google.com\n</code></pre> <p>Shows each router hop with latency times.</p> <p>Interpretation: If delay starts increasing after a certain hop, that\u2019s where the problem lies (network congestion or bad route).</p> <p>\u2705 \u201cIf latency is high, I\u2019d use <code>traceroute</code> to pinpoint where in the path the delay starts.\u201d</p>"},{"location":"Linux_System_Administration/#dig-dns-lookup-utility","title":"<code>dig</code> \u2014 DNS lookup utility","text":"<p>\u2705 Purpose: Query DNS servers to resolve domain names.</p> <pre><code>dig google.com\n</code></pre> <p>Output fields:</p> <ul> <li>ANSWER SECTION \u2192 IP address (A or AAAA record)</li> <li>Query time: \u2192 how long it took</li> <li>SERVER: \u2192 which DNS server replied</li> </ul> <p>Example:</p> <pre><code>dig @8.8.8.8 example.com +short\n</code></pre> <p>\u2705 \u201cIf DNS seems broken, I\u2019d run <code>dig</code> to see if the domain resolves and which DNS server responds.\u201d</p>"},{"location":"Linux_System_Administration/#nslookup-simpler-dns-tool","title":"<code>nslookup</code> \u2014 Simpler DNS tool","text":"<p>\u2705 Purpose: Same as <code>dig</code> but simpler format.</p> <pre><code>nslookup google.com\n</code></pre> <p>Output:</p> <pre><code>Server:  8.8.8.8\nAddress: 8.8.8.8#53\nName:    google.com\nAddress: 142.250.72.206\n</code></pre> <p>\u2705 \u201cI use <code>nslookup</code> or <code>dig</code> to confirm if DNS is returning the right IP before suspecting network issues.\u201d</p>"},{"location":"Linux_System_Administration/#4-check-open-ports-and-active-connections","title":"\ud83d\udd12 4. Check Open Ports and Active Connections","text":""},{"location":"Linux_System_Administration/#netstat-tulpen-show-active-network-ports-and-processes","title":"<code>netstat -tulpen</code> \u2014 Show active network ports and processes","text":"<p>\u2705 Purpose: See which processes are listening on which ports.</p> <pre><code>sudo netstat -tulpen\n</code></pre> Option Description <code>-t</code> TCP connections <code>-u</code> UDP connections <code>-l</code> Listening sockets <code>-p</code> Show PID/program <code>-e</code> Extended info <code>-n</code> Don\u2019t resolve hostnames <p>Output example:</p> <pre><code>Proto Recv-Q Send-Q Local Address  Foreign Address  State  PID/Program name\ntcp   0      0 0.0.0.0:80         0.0.0.0:*        LISTEN 1234/nginx\n</code></pre> <p>\u2705 \u201cTo verify if Nginx is running, I\u2019d use <code>netstat -tulpen | grep 80</code>.\u201d</p>"},{"location":"Linux_System_Administration/#ss-modern-replacement-for-netstat","title":"<code>ss</code> \u2014 Modern replacement for netstat","text":"<p>\u2705 Purpose: Faster, newer way to view sockets.</p> <pre><code>sudo ss -tulnp\n</code></pre> <p>Shows all listening ports and which service owns them.</p> <p>\u2705 \u201cI\u2019d use <code>ss -tulnp</code> instead of <code>netstat</code> on modern systems \u2014 it\u2019s faster and doesn\u2019t require extra packages.\u201d</p>"},{"location":"Linux_System_Administration/#5-container-resource-monitoring","title":"\ud83d\udc33 5. Container Resource Monitoring","text":""},{"location":"Linux_System_Administration/#docker-stats-monitor-container-cpumemory-usage","title":"<code>docker stats</code> \u2014 Monitor container CPU/memory usage","text":"<p>\u2705 Purpose: Real-time resource stats for running containers.</p> <pre><code>docker stats\n</code></pre> <p>Output example:</p> <pre><code>CONTAINER ID   NAME         CPU %     MEM USAGE / LIMIT\na1b2c3d4e5f6   nginx_web    2.35%     120MiB / 2GiB\nb7c8d9e0f1g2   db_service   10.52%    450MiB / 4GiB\n</code></pre> <p>\u2705 \u201cIf a containerized app is slow, I\u2019d check <code>docker stats</code> to see if a container is hitting its resource limits.\u201d</p>"},{"location":"Linux_System_Administration/#bonus-commands-for-containers","title":"Bonus Commands for Containers","text":"<pre><code>docker ps                       # list running containers\ndocker logs &lt;container_id&gt;       # view container logs\ndocker inspect &lt;container_id&gt;    # show container config/network info\ndocker exec -it &lt;container_id&gt; bash  # open shell inside container\n</code></pre>"},{"location":"Linux_System_Administration/#6-putting-it-all-together-interview-scenario-example","title":"\ud83e\udde0 6. Putting It All Together \u2014 Interview Scenario Example","text":"<p>Scenario: Users report your web app is slow.</p>"},{"location":"Linux_System_Administration/#your-investigation-flow-and-what-to-say-in-interview","title":"Your investigation flow (and what to say in interview):","text":"<ol> <li> <p>Check server load: <code>top</code> \u2192 CPU/memory usage    <code>free -m</code> \u2192 memory status    <code>iostat</code> / <code>vmstat</code> \u2192 I/O bottlenecks</p> </li> <li> <p>Check logs: <code>tail -f /var/log/nginx/error.log</code> <code>grep \"500\" /var/log/nginx/access.log</code></p> </li> <li> <p>Test connectivity: <code>ping app-server</code> \u2192 reachability    <code>curl -I http://localhost:8080</code> \u2192 check app response    <code>traceroute</code> \u2192 trace route if remote latency suspected</p> </li> <li> <p>Check DNS resolution: <code>dig app.company.com +short</code></p> </li> <li> <p>Verify open ports: <code>ss -tulnp | grep 8080</code> \u2192 ensure app is listening</p> </li> <li> <p>If containerized: <code>docker stats</code> \u2192 check resource use    <code>docker logs &lt;id&gt;</code> \u2192 application errors</p> </li> </ol> <p>\u2705 What the interviewer sees: You follow a logical, layered troubleshooting process \u2014 from system \u2192 network \u2192 app \u2192 container \u2014 just like a real SRE.</p> <p>Would you like me to create a practice lab script (like a guided \u201cbreak and fix\u201d Linux exercise) where you simulate an outage and use these commands to diagnose and fix it? That kind of hands-on simulation would prepare you exactly for the troubleshooting part of the Meta SRE interview.</p> <p>top vs htop </p>"},{"location":"Mac/","title":"Mac Notes","text":""},{"location":"Mac/#setting-up-ubuntu-vm-on-virtualbox-on-macbook-m4","title":"Setting up Ubuntu VM on VirtualBox on MacBook M4 :","text":"<p>https://www.youtube.com/watch?v=LjL_N0OZxvY</p>"},{"location":"Mac/#make-ubuntu-full-screen-in-virtualbox","title":"Make Ubuntu full screen in VirtualBox:","text":"<p>https://youtu.be/Fw8ppXeJ_GY?si=qu0ehtCFotNpaLcB - this tutorial also shows how to install guest additions</p>"},{"location":"Mac/#how-to-create-shared-folder-between-host-and-ubuntu-vm-on-virtualbox","title":"How to create shared folder between host and Ubuntu VM on VirtualBox:","text":"<p>https://youtu.be/j8Ne96h8UDg?si=aBwXLuINWmvPNp1l</p> <p>Fix slow booting for ubuntu vm based on output of these commands to find what was slowing down the booting time</p> <pre><code> systemd-analyze\n systemd-analyze blame\n</code></pre> <p></p> <p>Top delay:</p> <p>2min 258ms systemd-networkd-wait-online.service</p> <p>This service alone adds over 2 minutes to your boot. That\u2019s practically your entire userspace delay.</p> <p>\ud83d\udd0d What is systemd-networkd-wait-online.service?</p> <p>It waits for the network to be fully up (like getting a DHCP lease or full connection). Used mostly on servers to ensure critical services only start after network is available. In most VMs and desktops, it's not necessary and can be safely disabled.</p> <p>\u2705 Fix: Disable the Wait-Online Service</p> <p>Run: - sudo systemctl disable systemd-networkd-wait-online.service - sudo systemctl mask systemd-networkd-wait-online.service</p> <p>\ud83d\udca1 mask ensures it cannot be accidentally started again.</p> <p>If you also see or use NetworkManager, you can disable its wait service too:</p> <p>sudo systemctl disable NetworkManager-wait-online.service</p> <p>sudo systemctl mask NetworkManager-wait-online.service</p> <p>\ud83d\udd01 Then reboot your VM: sudo reboot \ud83e\uddea After reboot:</p> <p>Check again: systemd-analyze You should see your boot time drop from 2+ minutes \u2192 ~15 seconds or less.</p> <p>\ud83d\udd01 To Undo the Effects of Disabling and Masking the Wait-Online Service If you previously ran:</p> <ul> <li> <p>sudo systemctl disable systemd-networkd-wait-online.service</p> </li> <li> <p>sudo systemctl mask systemd-networkd-wait-online.service</p> </li> </ul> <p>You can restore the service like this:</p> <p>\u2705 Step 1: Unmask the service</p> <ul> <li>sudo systemctl unmask systemd-networkd-wait-online.service</li> </ul> <p>\u2705 Step 2: Re-enable the service at boot</p> <ul> <li>sudo systemctl enable systemd-networkd-wait-online.service \u2705 Step 3 (Optional): Start it immediately (not needed for reboot use)</li> </ul> <p>sudo systemctl start systemd-networkd-wait-online.service</p> <p>\ud83d\udd01 Same steps apply if you disabled NetworkManager-wait-online.service: Just replace the service name in each command. Example:</p> <p>sudo systemctl unmask NetworkManager-wait-online.service</p> <p>sudo systemctl enable NetworkManager-wait-online.service</p> <p>\ud83e\uddea Confirm it's active again:</p> <p>systemctl status systemd-networkd-wait-online.service or systemctl is-enabled systemd-networkd-wait-online.service</p> <p>You should see:</p> <p><code>enabled</code></p> <p>\u2705 Summary: Rollback Commands</p>"},{"location":"Mac/#unmask-and-re-enable-systemd-networkd-wait-online","title":"Unmask and re-enable systemd-networkd-wait-online","text":"<p>sudo systemctl unmask systemd-networkd-wait-online.service</p> <p>sudo systemctl enable systemd-networkd-wait-online.service</p>"},{"location":"Mac/#optional-also-for-networkmanager-wait-online-if-previously-masked","title":"Optional: Also for NetworkManager-wait-online (if previously masked)","text":"<p>sudo systemctl unmask NetworkManager-wait-online.service</p> <p>sudo systemctl enable NetworkManager-wait-online.service</p>"},{"location":"Matlab/","title":"Matlab","text":"<p>Install this Java Runtime before installing Matlab</p> <p>Java Runtime Required</p> <p>Native Apple silicon MATLAB requires a Java runtime be installed on your Mac. A supported Java 11 JRE is available free of charge with Amazon Corretto 11</p> <p>https://ww2.mathworks.cn/en/support/requirements/apple-silicon.html</p> <p>Had to install Xcode from App store</p>"},{"location":"Matlab_Data_Sources/","title":"Matlab Data Sources","text":""},{"location":"Matlab_Data_Sources/#matlab-data-sources-sql-vs-postgresql","title":"Matlab Data Sources - SQL vs PostgreSQL","text":""},{"location":"Matlab_Data_Sources/#postgresql-jdbc-for-macos","title":"PostgreSQL JDBC for macOS:","text":"<p>https://ww2.mathworks.cn/help/database/ug/postgresql-jdbc-for-macos.html</p>"},{"location":"Matlab_Data_Sources/#postgresql-jdbc-for-windows","title":"PostgreSQL JDBC for Windows:","text":"<p>https://ww2.mathworks.cn/help/database/ug/postgresql-jdbc-windows.html</p> <p>I have configured a JDBC data source on matlab using mysql-connector-j-9.3.0 as the driver , and connecting to a dockerized version of a mysql database, how can I change this to postgresql database instead with minimal changes?</p> <ol> <li>Replace JDBC Driver MySQL and PostgreSQL require different JDBC drivers: Current (MySQL): mysql-connector-j-9.3.0.jar Required (Postgres): postgresql-.jar (latest is 42.x) \ud83d\udc49 Download the .jar and place it in the same folder where MATLAB can access JDBC drivers (often under matlabroot/toolbox/local or a custom path you\u2019ve set). <p>Link to download JDBC Drivers</p> <p>https://ww2.mathworks.cn/en/products/database/driver-installation.html</p> <ul> <li>choose PostgreSQL JDBC driver</li> </ul> <p>\u2705 Which JDBC Driver to Use F rom the PostgreSQL JDBC site:</p> <p>You are on Postgres 16 \u2192 supported by all recent drivers.</p> <p>You are likely running Java 8+ (MATLAB since R2020a bundles Java 8, and newer releases may bundle Java 11).</p> <p>\ud83d\udc49 So you should use the JDBC 4.2 driver (version 42.7.8, latest stable). That means: <code>postgresql-42.7.8.jar</code> is the right driver for you.</p> <ul> <li> <p>check version of Java on Matlab</p> </li> <li> <p>check version of PostgreSQL used as JDBC data source</p> </li> </ul> <p>\ud83d\udd0d Things to Check Before Connecting</p> <p>Java Version in MATLAB</p> <p>In MATLAB, check which JVM it\u2019s using:</p> <p>version -java</p> <p>If it shows <code>Java 8 or higher</code>, you\u2019re good with <code>postgresql-42.7.8.jar</code>. If it shows Java 7 \u2192 use postgresql-42.2.29.jar. If Java 6 (unlikely unless MATLAB is very old) \u2192 use postgresql-42.2.27.jar.</p>"},{"location":"Matlab_Data_Sources/#content-of-docker-composeyml-file-to-use-postgresql-database-as-docker-container","title":"Content of docker-compose.yml file to use postgresql database as docker container","text":"<p>This version does not persist volume and data will vanish if you remove the container.</p> <pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: ccp-multi-ied-multi-plc\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: powerplant\n    ports:\n      - \"5432:5432\"\n    volumes:\n    - ./postgres-sql:/docker-entrypoint-initdb.d\n\n</code></pre> <p>Use of  -  volumes:       - ./sql:/docker-entrypoint-initdb.d</p> <pre><code>- This mounts your host directory ./sql into the container\u2019s /docker-entrypoint-initdb.d directory.\n\n- docker-entrypoint-initdb.d is a special folder: on first startup of the container, MySQL runs all .sql or .sh scripts there to initialize the DB.\n\n- After initialization, new runs don\u2019t automatically re-run scripts unless you remove the container and volume.\n\n\u2757 This is not where live database data is stored. Live data is in /var/lib/mysql. Since you\u2019re not mounting that, the data lives in a Docker-managed anonymous volume.\n\nSo: Changing this bind mount will not remove your MySQL data. It only affects which init scripts would run on container creation.\n</code></pre>"},{"location":"Matlab_Data_Sources/#content-of-docker-composeyml-file-to-use-postgresql-database-as-docker-container-with-persistent-volume","title":"Content of docker-compose.yml file to use postgresql database as docker container (with persistent volume)","text":"<pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: ccp-multi-ied-multi-plc-pg\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: powerplant\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data   # &lt;-- persistent volume for DB data\n      - ./postgres-sql:/docker-entrypoint-initdb.d  # &lt;-- init scripts\n\nvolumes:\n  postgres_data:   # &lt;-- named volume definition\n</code></pre>"},{"location":"Matlab_Data_Sources/#steps-to-use-postgresql-database-with-matlab","title":"Steps to use PostgreSQL database with MATLAB:","text":"<ol> <li>Download JDBC driver from PostgreSQL JDBC site (check version of Java on MATLAB &amp; Postgres version)</li> <li></li> </ol>"},{"location":"Matlab_Docker/","title":"Matlab Docker","text":"<p>Summarise steps to run the matlab docker container, requirements to run the matlab docker container and how I can check that my license fulfils the requirements. After running the docker container, how can i access the matlab docker container to add simulation files, configure data source, run simulation? Note that I will be running on a remote ubuntu vm that i have ssh-ed into using VPN tunnel and i can only view the browser on my local machine so what options do i have to access and use the matlab docker container and which is best ?</p> <p>Here\u2019s a complete, structured summary of what you need to know and do to run and access a MATLAB Docker container \u2014 tailored to your setup (remote Ubuntu VM accessed via SSH and VPN, with browser access only on your local machine).</p>"},{"location":"Matlab_Docker/#1-requirements-to-run-matlab-docker-container","title":"\ud83e\udde9 1. Requirements to Run MATLAB Docker Container","text":""},{"location":"Matlab_Docker/#software-requirements","title":"\u2705 Software Requirements","text":"<ul> <li> <p>Docker installed on your remote Ubuntu VM.</p> </li> <li> <p>Check: <code>docker --version</code></p> </li> <li> <p>If missing:</p> <p>``` sudo apt update</p> <p>sudo apt install docker.io -y</p> <p>sudo systemctl enable docker</p> <p>sudo systemctl start docker ```</p> </li> <li> <p>MATLAB container image from Docker Hub:</p> </li> </ul> <p><code>docker pull mathworks/matlab:r2025a   # version I used on Macbook</code></p>"},{"location":"Matlab_Docker/#license-requirements","title":"\u2705 License Requirements","text":"<p>Your MATLAB license must:</p> <ol> <li>Be valid for all MathWorks products you want to use.</li> <li>Be linked to your MathWorks account.</li> <li>Be configured for cloud use.</li> </ol> <p>License types that already support cloud use:</p> <ul> <li>Individual license \u2705</li> <li>Campus-Wide license \u2705</li> </ul> <p>For Concurrent (Network) licenses:</p> <ul> <li>You need to specify the license server:</li> </ul> <p><code>-e MLM_LICENSE_FILE=27000@MyLicenseServer</code></p>"},{"location":"Matlab_Docker/#how-to-check-if-your-license-meets-the-requirements","title":"\ud83d\udd0d How to Check if Your License Meets the Requirements","text":"<ol> <li>Go to your MathWorks Account page.</li> <li> <p>Under Licenses \u2192 Manage Licenses, check:</p> </li> <li> <p>License type (Individual, Campus-Wide, or Concurrent).</p> </li> <li>If Concurrent, note down the license server details (port and DNS address).</li> <li>Confirm with your license administrator or MathWorks support if it is enabled for cloud use.</li> </ol>"},{"location":"Matlab_Docker/#2-steps-to-run-the-matlab-docker-container","title":"\u2699\ufe0f 2. Steps to Run the MATLAB Docker Container","text":""},{"location":"Matlab_Docker/#option-1-run-matlab-in-browser-mode-simpler-recommended","title":"\ud83e\udded Option 1: Run MATLAB in Browser Mode (Simpler, Recommended)","text":"<p>Run this on your remote Ubuntu VM:</p> <pre><code>docker run -it --rm -p 8888:8888 --shm-size=512M mathworks/matlab:r2025a -browser\n</code></pre> <ul> <li><code>-p 8888:8888</code> \u2192 exposes port for browser connection</li> <li><code>--shm-size=512M</code> \u2192 required for MATLAB GUI stability</li> <li><code>-browser</code> \u2192 enables web-based MATLAB interface</li> </ul> <p>After running, Docker will print a URL like:</p> <pre><code>http://localhost:8888/?token=abcd1234\n</code></pre> <p>Since your browser is on your local machine, you must forward this port through SSH.</p>"},{"location":"Matlab_Docker/#ssh-port-forwarding-from-local-remote","title":"\ud83d\udedc SSH Port Forwarding (from Local \u2192 Remote)","text":"<p>Run this on your local machine (not inside SSH):</p> <pre><code>ssh -L 8888:localhost:8888 username@remote_vm_ip\n</code></pre> <p>Then open in your local browser:</p> <pre><code>http://localhost:8888\n</code></pre> <p>Login with your MathWorks credentials when prompted.</p>"},{"location":"Matlab_Docker/#option-2-run-matlab-in-vnc-mode-for-full-desktop-access","title":"\ud83e\udded Option 2: Run MATLAB in VNC Mode (for full desktop access)","text":"<p>Run this on your remote Ubuntu VM:</p> <pre><code>docker run --init -it --rm -p 5901:5901 -p 6080:6080 --shm-size=512M mathworks/matlab:r2024b -vnc\n</code></pre> <p>Then:</p> <ul> <li>Via VNC client: connect to <code>remote_vm_ip:5901</code> (password: <code>matlab</code>)</li> <li>OR via browser (no VNC client): open</li> </ul> <p><code>http://localhost:6080</code></p> <p>\u2192 use SSH port forwarding:</p> <p><code>ssh -L 6080:localhost:6080 username@remote_vm_ip</code></p>"},{"location":"Matlab_Docker/#3-accessing-and-using-matlab-container","title":"\ud83e\uddf0 3. Accessing and Using MATLAB Container","text":"<p>Once the container is running:</p>"},{"location":"Matlab_Docker/#add-simulation-files","title":"\ud83d\uddc2 Add Simulation Files","text":"<p>Option 1: Mount a directory from your remote VM into the container:</p> <pre><code>docker run -it --rm \\\n  -p 8888:8888 \\\n  -v /path/to/your/files:/home/matlab/projects \\\n  --shm-size=512M mathworks/matlab:r2024b -browser\n</code></pre> <p>Inside MATLAB, access your files under <code>/home/matlab/projects</code>.</p> <p>Option 2: Copy files into container (temporary)</p> <pre><code>docker cp localfile.m container_id:/home/matlab/\n</code></pre>"},{"location":"Matlab_Docker/#configure-data-source","title":"\u2699\ufe0f Configure Data Source","text":"<p>If your simulation depends on external data:</p> <ul> <li>Mount the data directory the same way using <code>-v</code>.</li> <li>If using a database or sensor stream, ensure the container has network access (use <code>--network host</code> if necessary).</li> </ul>"},{"location":"Matlab_Docker/#run-simulation","title":"\u25b6\ufe0f Run Simulation","text":"<p>Once inside MATLAB (browser or VNC):</p> <ul> <li>Open <code>.slx</code> or <code>.m</code> files from your mounted directory.</li> <li>Run simulations normally using the GUI or MATLAB command line.</li> </ul>"},{"location":"Matlab_Docker/#4-access-options-from-your-local-machine","title":"\ud83c\udf10 4. Access Options from Your Local Machine","text":"Access Method Description Pros Cons Recommended When Browser Mode (<code>-browser</code>) Use MATLAB Web interface through port 8888 Easiest, lightweight, no VNC setup Slightly limited desktop features \u2705 Best for general use VNC Mode (<code>-vnc</code>) Full MATLAB desktop via port 6080 or 5901 Supports toolboxes/add-ons GUI Slightly slower over VPN When full GUI features needed Batch Mode (no GUI) Run scripts or simulations directly Fastest, low resource use No graphical interface Automated simulations"},{"location":"Matlab_Docker/#best-setup-for-your-case-remote-ubuntu-vm-over-vpn","title":"\u2705 Best Setup for Your Case (Remote Ubuntu VM over VPN)","text":"<p>Given that:</p> <ul> <li>You SSH into the VM.</li> <li>You can only use your local browser (not remote desktop).</li> </ul> <p>\ud83d\udc49 Recommended Setup:</p> <ol> <li>Use Browser Mode (<code>-browser</code>).</li> <li>Use SSH port forwarding to map <code>8888</code> from remote to local.</li> <li>Mount your project directory via <code>-v</code>.</li> <li>Access MATLAB via <code>http://localhost:8888</code> in your local browser.</li> </ol> <p>Example final command:</p> <pre><code>ssh -L 8888:localhost:8888 user@remote_vm_ip\n</code></pre> <p>Then on the VM:</p> <pre><code>docker run -it --rm \\\n  -p 8888:8888 \\\n  -v ~/matlab_projects:/home/matlab/projects \\\n  --shm-size=512M mathworks/matlab:r2024b -browser\n</code></pre> <p>Access locally via: \ud83d\udc49 <code>http://localhost:8888</code></p> <p>Would you like me to include steps for saving MATLAB session data and simulations persistently (so it\u2019s not lost when the container stops)?</p>"},{"location":"Matlab_Docker/#question-i-need-to-install-the-database-toolbox-so-will-it-be-better-to-use-the-browser-or-vnc-option-to-run-and-access-the-matlab-docker-container","title":"Question: I need to install the database toolbox, so will it be better to use the browser or vnc option to run and access the matlab docker container","text":"<p>Excellent question \u2014 and you\u2019re absolutely right to pause and think about this before running the container.</p> <p>When you need to install toolboxes like the Database Toolbox, the VNC mode is the better and more reliable option.</p> <p>Let\u2019s break down why and how \ud83d\udc47</p>"},{"location":"Matlab_Docker/#1-key-difference-between-browser-and-vnc-modes","title":"\ud83e\udde9 1. Key Difference Between Browser and VNC Modes","text":"Mode Interface Can Install Toolboxes? When to Use Browser Mode (<code>-browser</code>) MATLAB Web Desktop (lightweight interface) \u274c No \u2013 the installer UI and Add-On Explorer are not supported When you just want to run existing MATLAB scripts or simulations VNC Mode (<code>-vnc</code>) Full MATLAB Desktop via virtual display \u2705 Yes \u2013 supports Toolbox installation, updates, Add-On Explorer When you need to install or manage MATLAB toolboxes"},{"location":"Matlab_Docker/#2-how-to-run-matlab-in-vnc-mode-recommended-for-installing-toolboxes","title":"\ud83e\uddf0 2. How to Run MATLAB in VNC Mode (Recommended for Installing Toolboxes)","text":"<p>Run this on your remote Ubuntu VM:</p> <pre><code>docker run --init -it --name matlab_container \\\n  -p 5901:5901 -p 6080:6080 \\\n  --shm-size=512M \\\n  mathworks/matlab:r2024b -vnc\n</code></pre>"},{"location":"Matlab_Docker/#explanation","title":"Explanation:","text":"<ul> <li><code>--init</code> \u2192 handles clean process shutdown</li> <li><code>-p 5901:5901</code> \u2192 for VNC client connection</li> <li><code>-p 6080:6080</code> \u2192 for browser-based VNC (no separate client needed)</li> <li><code>--shm-size=512M</code> \u2192 required shared memory</li> <li><code>-vnc</code> \u2192 launches MATLAB with a full GUI desktop</li> </ul> <p>Default VNC password: <code>matlab</code></p>"},{"location":"Matlab_Docker/#3-how-to-access-matlab-gui-from-your-local-machine","title":"\ud83c\udf10 3. How to Access MATLAB GUI from Your Local Machine","text":"<p>Since you\u2019re on a remote Ubuntu VM (accessed via SSH + VPN) and can only view browsers locally:</p>"},{"location":"Matlab_Docker/#option-1-use-browser-to-access-via-port-6080-simplest","title":"\u2705 Option 1: Use Browser to Access via Port 6080 (Simplest)","text":"<ol> <li>On your local machine, forward the VNC web port:</li> </ol> <p><code>ssh -L 6080:localhost:6080 username@remote_vm_ip</code></p> <ol> <li>Open in your local browser:</li> </ol> <p><code>http://localhost:6080</code></p> <ol> <li>Login with password <code>matlab</code>.    You\u2019ll see a full Ubuntu desktop running MATLAB.</li> </ol> <p>\u2192 From there, open MATLAB \u2192 Home \u2192 Add-Ons \u2192 Get Add-Ons, search for Database Toolbox, and install.</p>"},{"location":"Matlab_Docker/#option-2-use-a-vnc-client-tigervnc-realvnc-etc","title":"\ud83e\uddd1\u200d\ud83d\udcbb Option 2: Use a VNC Client (TigerVNC, RealVNC, etc.)","text":"<ol> <li>Forward the VNC port:</li> </ol> <p><code>ssh -L 5901:localhost:5901 username@remote_vm_ip</code> 2. Connect using a VNC viewer to:</p> <p><code>localhost:5901</code> 3. Password: <code>matlab</code></p> <p>This gives a smoother experience (less lag, clipboard support).</p>"},{"location":"Matlab_Docker/#4-keep-the-container-persistent","title":"\ud83d\udcbe 4. Keep the Container Persistent","text":"<p>When installing toolboxes, you\u2019ll definitely want to keep the container persistent so your installation isn\u2019t lost:</p> <pre><code>docker run --init -it --name matlab_container -p 5901:5901 -p 6080:6080 -v ~/matlab_projects:/home/matlab/projects --shm-size=512M mathworks/matlab:r2025a -vnc\n</code></pre> <p>Then after installation:</p> <pre><code>docker stop matlab_container      # stop container\ndocker start -ai matlab_container # restart with toolboxes intact\n</code></pre>"},{"location":"Matlab_Docker/#summary","title":"\ud83e\udde0 Summary","text":"Task Best Mode Why Running scripts/simulations Browser Lighter, easier to access Installing toolboxes / Add-Ons VNC Full desktop needed Updating MATLAB / managing licenses VNC GUI access required Automated jobs / headless execution Batch mode No GUI needed <p>\u2705 Recommendation for you Since you need to install the Database Toolbox, start MATLAB in VNC mode (<code>-vnc</code>) and access it through your local browser (port 6080) or a VNC viewer. Once installed, you can later switch back to browser mode (<code>-browser</code>) for daily use.</p> <p>Would you like me to show the exact steps to install the Database Toolbox inside the VNC MATLAB container, including how to make sure the toolbox persists across container restarts?</p>"},{"location":"Matlab_Docker/#question","title":"Question","text":"<p>explain each part of the command below and what they do</p> <p>docker run --init -it --name matlab_container -p 5901:5901 -p 6080:6080 -v ~/matlab_projects:/home/matlab/projects --shm-size=512M mathworks/matlab:r2025a -vnc</p> <p>Perfect \u2014 let\u2019s go through your MATLAB Docker command line by line, so you know exactly what each flag, option, and parameter does.</p>"},{"location":"Matlab_Docker/#full-command","title":"\ud83e\udde9 Full Command","text":"<pre><code>docker run --init -it --name matlab_container \\\n  -p 5901:5901 -p 6080:6080 \\\n  -v ~/matlab_projects:/home/matlab/projects \\\n  --shm-size=512M \\\n  mathworks/matlab:r2025a -vnc\n</code></pre>"},{"location":"Matlab_Docker/#step-by-step-breakdown","title":"\ud83e\udde0 Step-by-Step Breakdown","text":""},{"location":"Matlab_Docker/#docker-run","title":"\ud83d\udd39 <code>docker run</code>","text":"<p>This is the base Docker command that creates and starts a new container from an image.</p> <p>Think of it like \u201cbooting up a virtual MATLAB machine\u201d inside your remote Ubuntu VM.</p>"},{"location":"Matlab_Docker/#-init","title":"\ud83d\udd39 <code>--init</code>","text":"<ul> <li>Runs a lightweight init process (tini) inside the container.</li> <li> <p>This small helper process:</p> </li> <li> <p>Properly handles zombie processes.</p> </li> <li>Ensures clean shutdowns when you stop the container.</li> <li>Especially useful for long-running containers or ones that launch GUI processes (like MATLAB).</li> </ul> <p>\u2705 Good practice \u2014 prevents lingering or orphaned processes.</p>"},{"location":"Matlab_Docker/#-it","title":"\ud83d\udd39 <code>-it</code>","text":"<p>Combines two flags:</p> Flag Meaning <code>-i</code> Interactive mode \u2013 keeps STDIN open so you can interact with MATLAB in the terminal. <code>-t</code> Allocates a pseudo-TTY \u2013 gives you a readable terminal interface (text-based console). <p>Together, <code>-it</code> lets you see MATLAB logs or interact with the command-line interface if needed.</p>"},{"location":"Matlab_Docker/#-name-matlab_container","title":"\ud83d\udd39 <code>--name matlab_container</code>","text":"<ul> <li>Assigns a custom name (<code>matlab_container</code>) to the container.</li> <li> <p>Makes management easier:</p> </li> <li> <p>Stop container: <code>docker stop matlab_container</code></p> </li> <li>Restart container: <code>docker start -ai matlab_container</code></li> <li>Inspect container: <code>docker inspect matlab_container</code></li> </ul> <p>If you omit this, Docker assigns a random name like <code>sleepy_fermi</code>.</p>"},{"location":"Matlab_Docker/#-p-59015901","title":"\ud83d\udd39 <code>-p 5901:5901</code>","text":"<ul> <li>Maps port 5901 from your host (Ubuntu VM) to the same port inside the container.</li> <li>This is the standard port for VNC (Virtual Network Computing).</li> <li>Lets you connect to MATLAB\u2019s virtual desktop via a VNC client (e.g., RealVNC, TigerVNC).</li> </ul> <p>\ud83d\udda5\ufe0f Example: Connect from your local machine via SSH tunnel:</p> <pre><code>ssh -L 5901:localhost:5901 user@remote_vm_ip\n</code></pre> <p>Then open your VNC viewer \u2192 connect to <code>localhost:5901</code>.</p>"},{"location":"Matlab_Docker/#-p-60806080","title":"\ud83d\udd39 <code>-p 6080:6080</code>","text":"<ul> <li>Exposes port 6080, which is used for browser-based VNC access (no client needed).</li> <li>Lets you access the MATLAB desktop using a web browser.</li> </ul> <p>\ud83c\udf10 Example: After SSH port forwarding:</p> <pre><code>ssh -L 6080:localhost:6080 user@remote_vm_ip\n</code></pre> <p>Open in your browser:</p> <pre><code>http://localhost:6080\n</code></pre> <p>Default password: <code>matlab</code></p>"},{"location":"Matlab_Docker/#-v-matlab_projectshomematlabprojects","title":"\ud83d\udd39 <code>-v ~/matlab_projects:/home/matlab/projects</code>","text":"<ul> <li>Mounts a host directory (<code>~/matlab_projects</code> on your Ubuntu VM) into the container directory <code>/home/matlab/projects</code>.</li> </ul> Side Meaning <code>~/matlab_projects</code> Folder on your Ubuntu host (where your files live). <code>/home/matlab/projects</code> Folder inside the container, visible to MATLAB. <p>This allows:</p> <ul> <li>MATLAB inside the container to read/write files that persist outside the container.</li> <li>You to edit <code>.m</code> or <code>.slx</code> files from the host and see them instantly inside MATLAB.</li> </ul> <p>\u2705 Critical for persistence \u2014 your files survive even if you delete or recreate the container.</p>"},{"location":"Matlab_Docker/#-shm-size512m","title":"\ud83d\udd39 <code>--shm-size=512M</code>","text":"<ul> <li>Allocates 512 MB of shared memory to the container.</li> <li>MATLAB\u2019s graphical interface (and Simulink) require a certain amount of shared memory to run reliably.</li> <li>Without this, you may get errors like \u201cOut of memory\u201d or crashes when running GUI or simulations.</li> </ul> <p>You can increase it if needed:</p> <pre><code>--shm-size=1G\n</code></pre>"},{"location":"Matlab_Docker/#mathworksmatlabr2025a","title":"\ud83d\udd39 <code>mathworks/matlab:r2025a</code>","text":"<ul> <li>Specifies the Docker image to use.</li> <li><code>mathworks/matlab</code> \u2192 the official MATLAB image from Docker Hub.</li> <li><code>:r2025a</code> \u2192 the specific MATLAB release tag (e.g., R2025a).</li> </ul> <p>If not present locally, Docker will pull (download) it automatically from Docker Hub.</p>"},{"location":"Matlab_Docker/#-vnc","title":"\ud83d\udd39 <code>-vnc</code>","text":"<ul> <li>Argument passed to MATLAB inside the container (not a Docker flag).</li> <li>Tells MATLAB to start in VNC desktop mode, not browser mode or batch mode.</li> <li>This launches a full graphical desktop environment (Ubuntu + MATLAB GUI).</li> </ul> <p>So, inside the container, the <code>matlab</code> command effectively runs:</p> <pre><code>matlab -vnc\n</code></pre>"},{"location":"Matlab_Docker/#summary-table","title":"\ud83e\uddfe Summary Table","text":"Part Function <code>docker run</code> Start a new container <code>--init</code> Adds a small init process for stability <code>-it</code> Run interactively with a terminal <code>--name matlab_container</code> Give the container a custom name <code>-p 5901:5901</code> Expose VNC port for desktop access <code>-p 6080:6080</code> Expose web-VNC port for browser access <code>-v ~/matlab_projects:/home/matlab/projects</code> Mount host directory into container <code>--shm-size=512M</code> Allocate shared memory for MATLAB GUI <code>mathworks/matlab:r2025a</code> MATLAB Docker image and version <code>-vnc</code> Start MATLAB in VNC (desktop) mode <p>Would you like me to show the complementary commands to:</p> <ul> <li>stop/start this container safely, and</li> <li>connect to it from your local browser step-by-step via SSH port forwarding?</li> </ul>"},{"location":"Matlab_Docker/#does-the-folder-have-to-be-created-before-running-the-command-above","title":"Does the folder have to be created before running the command above:","text":"<p>\ud83e\udde9 The part in question -v ~/matlab_projects:/home/matlab/projects</p> <p>is a bind mount, meaning:</p> <p>The left side (~/matlab_projects) is a folder on your host (your remote Ubuntu VM).</p> <p>The right side (/home/matlab/projects) is the location inside the container where that folder will appear.</p>"},{"location":"Matlab_Docker/#database-toolbox-persist-database-toolbox-across-container-restarts","title":"Database Toolbox - Persist Database Toolbox across Container Restarts","text":"<p>Excellent \u2014 this is exactly the right question to ask \ud83d\udc4d because by default, MATLAB containers are ephemeral, meaning any toolbox installed inside them disappears when the container stops unless you persist it properly.</p> <p>Let\u2019s go through all the ways to make the Database Toolbox installation persistent, and then I\u2019ll recommend the best approach for your remote Ubuntu VM setup.</p>"},{"location":"Matlab_Docker/#why-toolboxes-disappear","title":"\ud83e\udde9 Why toolboxes disappear","text":"<p>When you install the Database Toolbox from the MATLAB GUI inside a container:</p> <ul> <li>It gets installed to a path inside the container\u2019s filesystem (typically <code>/usr/local/MATLAB/R2025a/toolbox</code>).</li> <li>That directory is part of the container image layer, not your mounted volume.</li> <li>So when you stop or remove the container, those changes are lost.</li> </ul> <p>To make toolboxes persistent, you have two main options:</p>"},{"location":"Matlab_Docker/#option-1-create-a-new-custom-docker-image-with-database-toolbox-preinstalled","title":"\u2705 Option 1: Create a new custom Docker image with Database Toolbox preinstalled","text":"<p>This is the recommended and professional method.</p> <p>It ensures:</p> <ul> <li>The Database Toolbox is installed permanently inside the image.</li> <li>You can restart or recreate containers anytime \u2014 the toolbox remains.</li> <li>The setup is reproducible for others or future VMs.</li> </ul>"},{"location":"Matlab_Docker/#steps","title":"Steps","text":""},{"location":"Matlab_Docker/#1-create-a-dockerfile-like-this","title":"1\ufe0f\u20e3 Create a <code>Dockerfile</code> like this:","text":"<pre><code>FROM mathworks/matlab:r2025a\n\n# Install Database Toolbox inside MATLAB\nRUN matlab -batch \"matlab.addons.installToolbox('/home/matlab/Database_Toolbox.mltbx'); exit\"\n\n# Optional: Set default working directory\nWORKDIR /home/matlab/projects\n</code></pre>"},{"location":"Matlab_Docker/#2-place-the-mltbx-file-inside-the-same-directory-as-the-dockerfile","title":"2\ufe0f\u20e3 Place the <code>.mltbx</code> file inside the same directory as the Dockerfile.","text":"<p>You can download the <code>Database_Toolbox.mltbx</code> file from MathWorks (via Add-Ons or File Exchange). If you already installed it once, MATLAB\u2019s Add-On Manager might have cached it in:</p> <pre><code>~/.matlab/Add-Ons/Downloads/\n</code></pre>"},{"location":"Matlab_Docker/#3-build-your-custom-image","title":"3\ufe0f\u20e3 Build your custom image","text":"<p>Run on your remote Ubuntu VM:</p> <pre><code>docker build -t matlab-with-dbtoolbox .\n</code></pre> <p>This will:</p> <ul> <li>Pull the official MATLAB image.</li> <li>Install the Database Toolbox inside it.</li> <li>Create a new image named <code>matlab-with-dbtoolbox</code>.</li> </ul>"},{"location":"Matlab_Docker/#4-run-your-container-from-the-new-image","title":"4\ufe0f\u20e3 Run your container from the new image","text":"<pre><code>docker run --init -it --name matlab_container \\\n  -p 5901:5901 -p 6080:6080 \\\n  -v ~/matlab_projects:/home/matlab/projects \\\n  --shm-size=512M \\\n  matlab-with-dbtoolbox -vnc\n</code></pre> <p>\u2705 Now every time you start or restart the container from this image, Database Toolbox will already be installed.</p>"},{"location":"Matlab_Docker/#option-2-use-a-persistent-volume-for-the-matlab-installation-directory","title":"\u2705 Option 2: Use a persistent volume for the MATLAB installation directory","text":"<p>If you don\u2019t want to build a new image, you can mount the MATLAB installation directory to a Docker volume or bind mount so that installed toolboxes remain there between runs.</p>"},{"location":"Matlab_Docker/#steps_1","title":"Steps","text":"<p>1\ufe0f\u20e3 First, find where MATLAB installs toolboxes inside the container: It\u2019s usually under:</p> <pre><code>/usr/local/MATLAB/R2025a/toolbox\n</code></pre> <p>2\ufe0f\u20e3 Create a directory on your host for MATLAB installation persistence:</p> <pre><code>mkdir -p ~/matlab_installation\n</code></pre> <p>3\ufe0f\u20e3 Run the container with that directory mounted:</p> <pre><code>docker run --init -it --name matlab_container \\\n  -p 5901:5901 -p 6080:6080 \\\n  -v ~/matlab_projects:/home/matlab/projects \\\n  -v ~/matlab_installation:/usr/local/MATLAB/R2025a/toolbox \\\n  --shm-size=512M \\\n  mathworks/matlab:r2025a -vnc\n</code></pre> <p>4\ufe0f\u20e3 Now when you install the Database Toolbox (via GUI or script), it\u2019s saved in your host directory <code>~/matlab_installation</code>, and it will persist between restarts.</p> <p>\u26a0\ufe0f However: MATLAB itself might not have permission to modify <code>/usr/local</code> as a non-root user, so you might need to run MATLAB as root the first time for installation:</p> <pre><code>sudo matlab &amp;\n</code></pre>"},{"location":"Matlab_Docker/#option-3-commit-your-running-container-to-a-new-image","title":"\u2705 Option 3: Commit your running container to a new image","text":"<p>If you already have a container where you successfully installed the Database Toolbox once, you can turn that running container into a new image that keeps everything as-is.</p>"},{"location":"Matlab_Docker/#steps_2","title":"Steps","text":"<p>1\ufe0f\u20e3 After successfully installing the Database Toolbox, check your running container name or ID:</p> <pre><code>docker ps\n</code></pre> <p>2\ufe0f\u20e3 Commit the container as a new image:</p> <pre><code>docker commit matlab_container matlab-with-dbtoolbox\n</code></pre> <p>3\ufe0f\u20e3 Verify:</p> <pre><code>docker images\n</code></pre> <p>You should now see:</p> <pre><code>matlab-with-dbtoolbox   latest   &lt;image_id&gt;   ...\n</code></pre> <p>4\ufe0f\u20e3 Next time, run containers from that image:</p> <pre><code>docker run --init -it --name matlab_container \\\n  -p 5901:5901 -p 6080:6080 \\\n  -v ~/matlab_projects:/home/matlab/projects \\\n  --shm-size=512M \\\n  matlab-with-dbtoolbox -vnc\n</code></pre> <p>\u2705 Now Database Toolbox persists because it\u2019s baked into the new image layer.</p>"},{"location":"Matlab_Docker/#recommended-approach-for-you","title":"\ud83d\udca1 Recommended Approach for You","text":"<p>Since you\u2019re using a remote Ubuntu VM and want a clean, reusable setup:</p> <p>\ud83d\udc49 Best Choice: Option 3 \u2014 Commit your configured container to a new image.</p> <p>It\u2019s fast, doesn\u2019t require a new Dockerfile, and keeps everything you just installed.</p>"},{"location":"Matlab_Docker/#summary-table_1","title":"\u2705 Summary Table","text":"Option Description Toolbox Persists? Complexity Recommended For 1. Custom Image (Dockerfile) Install via script during image build \u2705 Permanent Medium Reusable environments 2. Persistent Volume Mount <code>/usr/local/MATLAB/R2025a/toolbox</code> \u2705 Between restarts Medium Manual installs 3. Commit Container Save your current container as a new image \u2705 Permanent \ud83d\udd39Easy One-time setup (recommended for you) <p>Would you like me to give you the exact commands for Option 3 (commit method) so you can make your current container persist the Database Toolbox installation immediately after you reinstall it?</p>"},{"location":"Matlab_Docker/#installing-database-toolbox-on-matlab-docker-container","title":"Installing Database Toolbox on MATLAB Docker container","text":"<p>Excellent \u2014 that tells us the Database Toolbox files you copied over are not fully functional by themselves (they lack the compiled components and metadata MATLAB uses to register licensed toolboxes).</p> <p>Let\u2019s clarify the situation, then go over two reliable ways to install the Database Toolbox in your MATLAB Docker container so it actually works and persists.</p>"},{"location":"Matlab_Docker/#why-your-current-copy-doesnt-work","title":"\ud83d\udea8 Why your current copy doesn\u2019t work","text":"<p>The folder <code>/Applications/MATLAB_R2025a.app/toolbox/database/</code> on macOS contains only the high-level MATLAB scripts and doc files, but the actual toolbox relies on:</p> <ul> <li>Internal compiled libraries (<code>.p</code>, <code>.mex*</code>, <code>.jar</code>, <code>.xml</code>)</li> <li>License registration info stored in MATLAB\u2019s internal database</li> <li>Installation metadata from the Add-On Manager</li> </ul> <p>That\u2019s why copying just the folder doesn\u2019t make MATLAB recognize it \u2014 it\u2019s not a standalone package.</p>"},{"location":"Matlab_Docker/#you-have-two-correct-ways-to-install-it","title":"\u2705 You have two correct ways to install it","text":""},{"location":"Matlab_Docker/#option-1-use-the-matlab-add-on-installer-inside-the-container-recommended-if-license-allows-it","title":"Option 1 \u2014 Use the MATLAB Add-On Installer inside the container (recommended if license allows it)","text":"<p>This is the official way and ensures proper registration and license validation.</p>"},{"location":"Matlab_Docker/#steps_3","title":"Steps:","text":"<ol> <li>Run MATLAB Docker container with VNC:</li> </ol> <p><code>bash    docker start -ai matlab_container</code></p> <p>or if not created:</p> <p><code>bash    docker run --init -it --name matlab_container -p 5901:5901 -p 6080:6080 \\    -v ~/matlab_projects:/home/matlab/projects \\    --shm-size=512M mathworks/matlab:r2025a -vnc</code></p> <ol> <li>Access GUI:    Go to your browser and open:</li> </ol> <p><code>http://&lt;your_vm_ip&gt;:6080</code></p> <p>(Default password: <code>matlab</code>)</p> <ol> <li> <p>Install from MATLAB GUI:</p> </li> <li> <p>In MATLAB, go to Home \u2192 Add-Ons \u2192 Get Add-Ons</p> </li> <li>Search for Database Toolbox</li> <li> <p>Click Install</p> </li> <li> <p>MATLAB may restart automatically after installation \u2014 that\u2019s normal.    If it doesn\u2019t reopen, restart manually:</p> </li> </ol> <p><code>bash    matlab &amp;</code></p> <ol> <li>After restart, verify:</li> </ol> <p><code>matlab    ver    which database</code></p> <p>\u2705 If successful, MATLAB will now have Database Toolbox registered properly, and it will persist inside the container (since installation writes to <code>/usr/local/MATLAB</code>).</p>"},{"location":"Matlab_Docker/#option-2-download-the-mltbx-file-on-your-mac-and-transfer-it","title":"Option 2 \u2014 Download the <code>.mltbx</code> file on your Mac and transfer it","text":"<p>If the Add-On GUI fails (e.g., no internet access inside container), you can manually download and install the toolbox.</p>"},{"location":"Matlab_Docker/#steps_4","title":"Steps:","text":"<ol> <li>On your Mac, open MATLAB \u2192 Home \u2192 Add-Ons \u2192 Manage Add-Ons.    Find Database Toolbox, click View Details, and note the installation source path.</li> </ol> <p>Or go to:</p> <p><code>https://www.mathworks.com/matlabcentral/fileexchange/</code></p> <p>(or the Add-On Explorer web link for Database Toolbox if you have MathWorks access)</p> <ol> <li> <p>Download the Database Toolbox .mltbx installer file.</p> </li> <li> <p>Copy it to your Ubuntu VM:</p> </li> </ol> <p><code>bash    scp /path/to/Database_Toolbox.mltbx ubuntu@&lt;vm_ip&gt;:/home/ubuntu/</code></p> <ol> <li>Copy it into the container:</li> </ol> <p><code>bash    docker cp /home/ubuntu/Database_Toolbox.mltbx matlab_container:/home/matlab/</code></p> <ol> <li>Access the container:</li> </ol> <p><code>bash    docker exec -it matlab_container bash</code></p> <ol> <li>Run MATLAB inside:</li> </ol> <p><code>bash    matlab -batch \"matlab.addons.install('/home/matlab/Database_Toolbox.mltbx');\"</code></p> <ol> <li>Then verify:</li> </ol> <p><code>bash    matlab -batch \"ver\"</code></p> <p>\u2705 This method installs the toolbox properly, registers it with MATLAB\u2019s license manager, and persists across container restarts.</p>"},{"location":"Matlab_Docker/#summary_1","title":"\ud83e\udde0 Summary","text":"Method Description Requires Internet in Container Persists Add-On GUI Install Database Toolbox from MATLAB Add-Ons \u2705 Yes \u2705 Yes .mltbx Manual Install Install from local <code>.mltbx</code> file \u274c No \u2705 Yes Copying toolbox folder What you tried (won\u2019t register toolbox) N/A \u274c No <p>\ud83d\udc49 Since you already have the container running with GUI access, I recommend retrying Option 1 (Add-On GUI). If it fails again (e.g., after restart the toolbox doesn\u2019t show up), proceed with Option 2 \u2014 I can guide you to get the <code>.mltbx</code> file from your Mac MATLAB installation directory if needed.</p> <p>Would you like me to show you exactly where to find the <code>.mltbx</code> file on your Mac?</p>"},{"location":"NAT_Adapter_Iptables_Forwarding/","title":"NAT Adapter Iptables Fowarding","text":"<p><code>docker run -d --name eg_plc --net plc-macvlan --ip 192.168.40.202 --privileged openplc:v3</code></p> <p>ubuntu@pgt:~$ ip addr show enp0s3 2: enp0s3:  mtu 1500 qdisc fq_codel state UP group default qlen 1000     link/ether 08:00:27:62:7e:2a brd ff:ff:ff:ff:ff:ff     inet 192.168.30.60/24 brd 192.168.30.255 scope global enp0s3        valid_lft forever preferred_lft forever     inet6 fe80::a00:27ff:fe62:7e2a/64 scope link        valid_lft forever preferred_lft forever <p>Perfect \ud83d\udc4d \u2014 your VM\u2019s bridged IP is 192.168.30.60, so that\u2019s exactly the address to use in your forwarding rules. Here\u2019s the exact set of commands you can run inside your Ubuntu VM to make your 4 PLC Docker containers accessible from:</p> <ul> <li>the Remote Desktop host where the VM runs, and</li> <li>your own machine (the one you use to SSH into the VM).</li> </ul> <p>\ud83e\udde9 Step 1 \u2014 Run these commands inside the Ubuntu VM</p> <pre><code>sudo iptables -t nat -A PREROUTING -p tcp -d 192.168.30.60 --dport 8081 -j DNAT --to-destination 192.168.40.201:8080\nsudo iptables -t nat -A PREROUTING -p tcp -d 192.168.30.60 --dport 8082 -j DNAT --to-destination 192.168.40.202:8080\nsudo iptables -t nat -A PREROUTING -p tcp -d 192.168.30.60 --dport 8083 -j DNAT --to-destination 192.168.40.203:8080\nsudo iptables -t nat -A PREROUTING -p tcp -d 192.168.30.60 --dport 8084 -j DNAT --to-destination 192.168.40.204:8080\nsudo iptables -t nat -A POSTROUTING -j MASQUERADE\n</code></pre> <p>These rules tell Ubuntu: \"When traffic comes to 192.168.30.60:8081, forward it internally to 192.168.40.201:8080 (and so on for the others).\" \ud83e\udde9 Step 2 \u2014 Verify it\u2019s working From your Remote Desktop host, open: http://192.168.30.60:8081/login \u2192 PLC1 http://192.168.30.60:8082/login \u2192 PLC2 http://192.168.30.60:8083/login \u2192 PLC3 http://192.168.30.60:8084/login \u2192 PLC4 From your own computer (the one you use to SSH in), you can use the same URLs \u2014 as long as you can ping 192.168.30.60. You do not need to use localhost or SSH port forwarding anymore. \ud83e\udde9 Step 3 \u2014 (Optional but recommended) Make the rules persistent To make sure the forwarding stays active after reboot: sudo apt install iptables-persistent -y sudo netfilter-persistent save sudo netfilter-persistent enable  Now the rules you just added will automatically reapply every time the VM restarts. \u2705 Quick access summary PLC Container IP    Browser Access URL  Target (inside VM) PLC1    192.168.40.201  http://192.168.30.60:8081/login 192.168.40.201:8080 PLC2    192.168.40.202  http://192.168.30.60:8082/login 192.168.40.202:8080 PLC3    192.168.40.203  http://192.168.30.60:8083/login 192.168.40.203:8080 PLC4    192.168.40.204  http://192.168.30.60:8084/login 192.168.40.204:8080 Would you like me to modify your existing /usr/local/sbin/setup-macvlan.sh so it automatically sets up these forwarding rules too, so you only manage one script?</p> <p>TO DO: need to continue for other 3 plc containers, persist the rules</p> <p>make sure to docker start [plc_name]</p> <p>st_plc -&gt; http://192.168.30.60:8081/login </p> <p>Can be accessed on remote desktop (base machine of cyberX) or on your own host machine where you login to the VM</p> <p>After setting the DNAT rule:</p> <pre><code>sudo iptables -t nat -A PREROUTING -p tcp -d 192.168.30.60 --dport 8081 -j DNAT --to-destination 192.168.40.201:8080\nsudo iptables -t nat -A POSTROUTING -j MASQUERADE\n</code></pre> <p>Able to access http://192.168.30.60:8081/dashboard from the host machine (host machine that has ssh connection to ubuntu VM on remote host &amp; docker container is on this ubuntu vm) but NOT able to ping 192.168.30.60 </p> <p>\u2699\ufe0f Why http works but ping doesn\u2019t</p> <p><code>-p tcp --dport 8081</code></p> <p>So it matches only TCP packets going to port 8081. ping uses ICMP, not TCP. Therefore, your DNAT rule does not apply to ICMP at all. The host (192.168.30.60) has no direct route to 192.168.40.0/24 The internal Docker network (192.168.40.0/24) is isolated by default. Containers can reach out to the internet via NAT, but the host or external machines cannot directly reach them unless: You create a bridge network and expose it, Or add specific iptables / routing rules to forward ICMP (rarely done). HTTP works because of port forwarding (DNAT) When your browser connects to port 8081 on 192.168.30.60, iptables rewrites the packet\u2019s destination to 192.168.40.201:8080. Docker or the VM\u2019s networking stack then handles the return packets (via MASQUERADE). So HTTP works perfectly through this NAT.</p>"},{"location":"PostgreSQL/","title":"PostgreSQL","text":""},{"location":"PostgreSQL/#using-postgresql-db-from-docker-composeyml","title":"Using Postgresql DB from docker-compose.yml","text":"<pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: ccp-multi-ied-multi-plc-pg\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: powerplant\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data # persistent volume for DB data\n      - ./postgres-sql:/docker-entrypoint-initdb.d # init scripts\n\nvolumes:\n  postgres_data: # named volume definition\n</code></pre> <p>Got it \u2705 \u2014 you\u2019ve already defined your PostgreSQL service with user root, password root, and database powerplant.</p> <p>Once you get a bash shell inside the container, you can access PostgreSQL using the psql client. Here\u2019s how:</p> <ol> <li>Get into the container\u2019s shell <code>docker exec -it ccp-multi-ied-multi-plc-pg bash</code></li> </ol> <p>To enter PostgreSQL:</p> <p><code>psql -U root -d powerplant</code></p> <p>will prompt for a password -&gt; type <code>root</code> based on docker-compose.yml used</p> <p>Once insde psql, you will see something like:</p> <p><code>powerplant=#</code></p> <p>Now you can run SQL commands:</p> <pre><code>\\l           -- list databases\n\\c powerplant -- connect to powerplant DB\n\\dt          -- list tables\nSELECT * FROM ref;\nSELECT * FROM ccp_values;\n</code></pre> <p>\ud83d\udca1 Optional tips and customizations 1\ufe0f\u20e3 Disable the pager temporarily</p> <p>If you just want to see everything inline for one session:</p> <p>\\pset pager off</p> <p></p> <ol> <li>run postgresql db as a docker container : inside ccp_multi_IED_multi_plc_database_pg folder : docker compose up -d</li> <li>go into postgresql db : docker exec -it ccp-multi-ied-multi-plc-pg bash psql -U root -d powerplant password: root to show tables : \\dt SELECT * FROM ccp_values;</li> <li>figure out how to connect to matlab using JDBC data source -&gt; configure JDBC data source ...connector for postgresql (driver) </li> </ol> <p>data source name : Postgres_CCP database name : powerplant server: localhost port number : 5432 </p> <p>currently on postgres: 16 and java 8+ </p> <p>\ud83d\udc49 So you should use the JDBC 4.2 driver (version 42.7.8, latest stable). That means: postgresql-42.7.8.jar is the right driver for you. check what the xsteam is for ... and whether it needs to be changed try running with the postgresql db -&gt; what is the next part that needs to be connected check whether IED scripts need to change part at establish_connection() ... conn = mysqlconnector.connect( ....</p> <p>XSteam.m , XSteamUS.m , license.txt , X Steam for Matlab.pdf </p> <p>previously for a JDBC data source on matlab that used mysql database , on the HOME tab, I had done these: I selected 'Set Path' , 'Add Folder' and then selected xsteam folder  which contained these 4 files , will it make sense to add this same path to the same folder after I changed database to postgresql but im using it for the same functionality ? what is the xsteam folder for and does any of the files in it need to be modified ?</p> <p>XSteam folder and files belong to an independent MATLAB library called XSteam, used to calculate thermodynamic properties of water and steam (e.g. enthalpy, entropy, temperature, pressure-volume relationships, steam quality etc. )</p> <ol> <li>in stored_procedure_pg_GT.sql -&gt; timestamp was changed to ts as timestamp is a reserved keyword in postgresql</li> </ol> <p>need to check if it affects anything in the IEDs or PLCs for GT</p> <p>check the changes needed for the IED files</p> <p>Since you\u2019ve switched from MySQL to PostgreSQL, you can\u2019t use mysql.connector anymore. You need to use a PostgreSQL driver, like psycopg2 or psycopg. Here\u2019s how you can modify your function:</p> <pre><code>import psycopg2  # PostgreSQL driver\n\ndef establish_connection():\n    conn = psycopg2.connect(\n        host=\"10.34.11.15\",        # your PostgreSQL host\n        port=5432,                  # default PostgreSQL port\n        user=\"root\",                # your PostgreSQL username\n        password=\"root\",            # your PostgreSQL password\n        database=\"powerplant\"       # your PostgreSQL database name\n    )\n    return conn\n</code></pre> <p>Key Changes:</p> <ol> <li>Driver: Use psycopg2 instead of mysql.connector.</li> <li>Port: PostgreSQL default is 5432 (MySQL default is 3306).</li> <li>SQL syntax: Some queries may need slight changes if they use MySQL-specific syntax (e.g., backticks <code>table</code> vs. double quotes \"table\" in PostgreSQL), but most simple SELECT statements are fine.</li> <li>Stored procedures/functions:</li> </ol> <p>In PostgreSQL, your stored procedures are functions, so you\u2019ll call them differently:</p> <pre><code>cur = conn.cursor()\ncur.execute(\"SELECT * FROM get_gt_ied_data();\")  # note the parentheses\nrows = cur.fetchall()\n</code></pre> <p>Excellent \u2014 yes, you\u2019re thinking of <code>psql</code>'s display and output behavior settings, and there is a way to make it show query results, then immediately return to the prompt without odd pauses or output paging.</p> <p>Let\u2019s go through this cleanly \ud83d\udc47</p>"},{"location":"PostgreSQL/#problem","title":"\ud83c\udfaf Problem","text":"<p>When you run a command in <code>psql</code>, sometimes:</p> <ul> <li>It pauses after showing output (especially for long results).</li> <li>Or it pipes output through a pager like <code>less</code>, requiring you to press <code>q</code> to get back.</li> <li>Or you want it to just print everything directly and go back to the prompt.</li> </ul> <p>Excellent \u2014 yes, you\u2019re thinking of psql's display and output behavior settings, and there is a way to make it show query results, then immediately return to the prompt without odd pauses or output paging.</p> <p>Let\u2019s go through this cleanly \ud83d\udc47</p>"},{"location":"PostgreSQL/#solution-disable-the-pager-and-control-display-settings","title":"\u2705 Solution \u2014 Disable the pager and control display settings","text":"<p>You can use these <code>psql</code> internal commands:</p>"},{"location":"PostgreSQL/#1-disable-the-pager-so-output-prints-immediately","title":"1. Disable the pager (so output prints immediately)","text":"<pre><code>\\pset pager off\n</code></pre> <p>This ensures query results are shown directly in the terminal, and you return immediately to the <code>powerplant=#</code> prompt after the output.</p> <p>If you want to re-enable it later:</p> <pre><code>\\pset pager on\n</code></pre> <p>You can check its status:</p> <pre><code>\\pset\n</code></pre>"},{"location":"PostgreSQL/#2-disable-formatting-for-simpler-plain-output","title":"2. Disable formatting for simpler plain output","text":"<p>If you want more compact output (no table borders or alignment):</p> <pre><code>\\pset format unaligned\n\\pset border 0\n</code></pre> <p>Then results will appear like plain text (useful for scripts or logs).</p> <p>Restore normal table output with:</p> <pre><code>\\pset format aligned\n\\pset border 1\n</code></pre>"},{"location":"PostgreSQL/#3-disable-headers-or-footers-optional","title":"3. Disable headers or footers (optional)","text":"<p>If you don\u2019t want to see the <code>(3 rows)</code> line:</p> <pre><code>\\pset footer off\n</code></pre> <p>To bring it back:</p> <pre><code>\\pset footer on\n</code></pre>"},{"location":"PostgreSQL/#4-make-settings-permanent","title":"4. Make settings permanent","text":"<p>To persist these preferences, add them to your <code>~/.psqlrc</code> file:</p> <pre><code>echo '\\pset pager off' &gt;&gt; ~/.psqlrc\necho '\\pset format aligned' &gt;&gt; ~/.psqlrc\necho '\\pset border 1' &gt;&gt; ~/.psqlrc\n</code></pre> <p>Then they\u2019ll apply automatically every time you start <code>psql</code>.</p>"},{"location":"PostgreSQL/#example-session","title":"\ud83e\udde0 Example session","text":"<pre><code>psql -U root -d powerplant\n</code></pre> <p>Inside <code>psql</code>:</p> <pre><code>\\pset pager off\n\\pset format aligned\nselect * from ccp_values limit 5;\n</code></pre> <p>\u27a1 Output prints immediately \u27a1 Cursor returns cleanly to <code>powerplant=#</code></p> <p>Would you like me to suggest an optimal <code>.psqlrc</code> configuration for smooth day-to-day work (no pauses, clean formatting, and instant results)?</p>"},{"location":"Recovering_VM/","title":"Recovering VM","text":""},{"location":"Recovering_VM/#at-grub-shell","title":"At GRUB Shell:","text":"<p>\ud83d\udd27 Steps to boot into text console At the grub&gt; prompt, run these exact commands:</p> <pre><code>set root=(hd0,gpt2)\n\nlinux /vmlinuz root=/dev/ubuntu-vg/ubuntu-lv ro systemd.unit=multi-user.target\n\ninitrd /initrd.img\n\nboot\n</code></pre> <p>\ud83d\udccc Explanation</p> <ul> <li> <p>set root=(hd0,gpt2) \u2192 tells GRUB where /boot is.</p> </li> <li> <p>linux /vmlinuz root=/dev/ubuntu-vg/ubuntu-lv ro systemd.unit=multi-user.target</p> </li> <li> <p>/vmlinuz \u2192 your kernel (symlink to latest: vmlinuz-6.8.0-71-generic).</p> </li> <li> <p>root=/dev/ubuntu-vg/ubuntu-lv \u2192 because your root filesystem is inside LVM (lvm/ubuntu--vg-ubuntu--lv showed up in your ls).</p> </li> <li> <p>systemd.unit=multi-user.target \u2192 boots into text console instead of GUI.</p> </li> <li>initrd /initrd.img \u2192 loads your initramfs.</li> <li>boot \u2192 starts the kernel.</li> </ul> <p>\ud83d\udda5\ufe0f After booting You\u2019ll land in a text login prompt. Login with your username/password. Then fix the problematic services:</p> <pre><code>sudo systemctl disable NetworkManager-wait-online.service\nsudo systemctl mask NetworkManager-wait-online.service\nsudo apt update\nsudo apt install --reinstall gdm3 virtualbox-guest-utils virtualbox-guest-x11 virtualbox-guest-dkms\nsudo systemctl set-default multi-user.target\n</code></pre> <p>Reboot:</p> <pre><code>sudo reboot\n</code></pre> <p>You should now get a stable text login every time. Once everything is fixed, you can switch back to GUI with:</p> <pre><code>sudo systemctl set-default graphical.target\nsudo systemctl isolate graphical.target\n</code></pre>"},{"location":"Recovering_VM/#checking-space-left-on-vm","title":"Checking space left on VM:","text":"<pre><code>\u2191 Share se into data and signature falled \nUbuntu 24.04.2 Clone [HSRG_PLC PPTwin] [Running] \nW: Failed to fetch http://ports.ubuntu.com/ubuntu-ports/dists/noble-updates/InRelease \nError writing to file - write (28: No space left on device) [IP: 185.12 190.36 80] \nW: Failed to fetch http://ports.ubuntu.com/ubuntu-ports/dists/noble-backports/InRelease Error writing to file write (28: No space left on device) [IP: 185. 5.190.36 80] \nW: Failed to fetch http://ports.ubuntu.com/ubuntu-ports/dists/noble-security/InRelease Error writing to file - write (28: No space left on device) [IP: 185.1 .190.36 80] \nW: Some index files failed to download. They have been ignored, or old ones used instead. \nsreedevi@sreedevi:~$ dpkg -1 'linux-image*' | grep ^ii ii \nlinux-image-6.8.0-71-generic ii  6.8.0-71.71 arm64 Signed kernel image generic\nlinux-image-generic 6.8.0-71.71 arm64 Generic Linux kernel image\nsreedevi@sreedevi :~$ sudo apt clean      \nsreedevi@sreedevi:~$ sudo journalctl -vacuum-time=2d \njournalctl: invalid option -- 'v' \nsreedevi@sreedevi :~$ sudo journalctl --vacuum-time=2d \nVacuuming done, freed B of archived journals from /var/log/journal/ef 121b3f41774ea893499e6942bb2ab8. \nVacuuming done, freed B of archived journals from /var/log/journal. \nVacuuming done, freed B of archived journals from /run/log/journal. \nsreedevi@sreedevi:~$ sudo rm -rf /var/lib/apt/lists/* \nsreedevi@sreedevi: \"$ df -h \nFilesystem Size Used Avail Use% Mounted on \ntmpfs 391M 1.3M 389M 1% /run \nefivarfs 256K 54K 203K 21% /sys/firmware/efi/efivars \n/dev/ubuntu-vg/ubuntu-lv 16G 15G \u00d8 100% / \ntmpfs 2.0G \u04e8 2.0G 0% /dev/shm\ntmpfs 5.0M 8.0K 5.0M 1% /run/lock \n/dev/sda2 2.0G 106M 1.7G 6% /boot \n/dev/sda1 1.1G 6.4M 1.1G 1% /boot/efi \nPOWER PLANT_TWIN 927G 421G 506G 46% /media/sf_POWER_PLANT_TWIN \nubuntu24_shared_folder 927G 421G 506G 46% /media/sf_ubuntu24_shared_folder \ntmpfs 391M 88K 390M 1% /run/user/1000 \nsreedevi@sreedevi:~$\n</code></pre> <ul> <li>Your root filesystem is: /dev/ubuntu-vg/ubuntu-lv   16G   15G   0   100%   / So your root volume is 16 GB and completely full. That\u2019s why apt fails and the VM hangs. Your /boot and /boot/efi partitions have space, but they don\u2019t help since all system packages/logs live under /.</li> </ul>"},{"location":"Recovering_VM/#free-some-emergency-space","title":"Free some emergency space","text":"<p>Just enough so apt and the system can breathe:</p> <pre><code>\n# Clear apt cache (already done, but run again just in case)\nsudo apt clean\n\n# Remove old logs\nsudo rm -rf /var/log/journal/*\n\n# Truncate big logs if they exist\nsudo truncate -s 0 /var/log/*.log\nsudo truncate -s 0 /var/log/*/*.log\n</code></pre>"},{"location":"Recovering_VM/#check-if-there-is-free-space-in-the-volume-group","title":"Check if there is free space in the volume group","text":"<pre><code>sreedevi@sreedevi:~$ sudo vgdisplay ubuntu-vg \n\nVolume group \n\nVG Name ubuntu-vg \nSystem ID \nFormat 1vm2 \nMetadata Areas 1 \nMetadata Sequence No 2 \nVG Access read/write \nVG Status resizable \nMAX LV \u04e8 \nCur LV 1 \nOpen LV 1 \nMax PV \u04e8 \nCur PV 1 \nAct PV 1 \nVG Size &lt;31.95 GiB \nPE Size 4.00 MiB \nTotal PE 8178 \nAlloc PE Size 4089 / 15.97 GiB \nFree PE Size 4089/15.97 GiB \nVG UUID TkZXdq-3dSV-izeB-Ewns-UJJn-1VaN-FLMNwc \nsreedevi@sreedevi:~$\n</code></pre> <p>This means the volume group <code>ubuntu-vg</code> is ~32 GB total, but only 16 GB is allocated to <code>/</code> (<code>/dev/ubuntu-vg/ubuntu-lv</code>). So this means you already have ~16GB free inside the VG ready to use - you don't even need to resize the VirtualBox disk.</p>"},{"location":"Recovering_VM/#expand-your-root-filesystem","title":"Expand your root filesystem","text":"<p>Run these 2 commmands:</p> <pre><code># Extend root LV to use all free space\nsudo lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv\n\n# Resize filesystem to fill LV\nsudo resize2fs /dev/ubuntu-vg/ubuntu-lv\n</code></pre> <pre><code>sreedevi@sreedevi:~$ sudo lvextend -1 +100%FREE /dev/ubuntu-vg/ubuntu-lv \n[sudo] password for sreedevi: \nSize of logical volume ubuntu-vg/ubuntu-lv changed from 15.97 GiB (4089 extents) to &lt;31.95 GiB (8178 extents). \nLogical volume ubuntu-vg/ubuntu-lv successfully resized. \nsreedevi@sreedevi :~$ sudo resize2fs /dev/ubuntu-vg/ubuntu-lv resize2fs 1.47.0 (5-Feb-2023) \nFilesystem at /dev/ubuntu-vg/ubuntu-ly is mounted on ; on-line resizing required old_desc_blocks = 2, new_desc_blocks = 4 \nThe filesystem on /dev/ubuntu-vg/ubuntu-lv is now 8374272 (4k) blocks long. \nsreedevi@sreedevi: df -h \nFilesystem Size Used Avail Use% Mounted on \ntmpfs 391M 1.3M 389M 1% /run \nefivarfs 256K 54K 203K 21% /sys/firmware/efi/efivars \n/dev/ubuntu-vg/ubuntu-lv 32G 15G 16G 50% / \ntmpfs 2.0G \u04e8 2.0G 0% /dev/shm \ntmpfs 5.0\u043c 8.\u043e\u043a 5.0\u043c 1% /run/lock \n/dev/sda2 2.0G 106M 1.7G 6% /boot \n/dev/sda1 1.1G 6.4M 1.1G 1% /boot/efi \nPOWER PLANT_TWIN 927G 422G 506G 46% /media/sf_POWER_PLANT_TWIN \nubuntu24_shared_folder 927G 422G 506G 46% /media/sf_ubuntu24_shared_folder \ntmpfs 391M 88K 390M 1% /run/user/1000 \nsreedevi@sreedevi:~$\n</code></pre> <p>issue faced when trying steps </p> <pre><code>sreedevi@sreedevi :~$ sudo apt install --reinstall gdm3 virtualbox-guest-utils virtualbox-guest-x11 virtualbox-guest-dkms. \nReading package lists... \nDone Building dependency tree... \nDone Reading state information... \nDone E: Unable to locate package virtualbox-guest-utils \nE: Unable to locate package virtualbox-guest-x11 \nE: Unable to locate package virtualbox-guest-dkms \nsreedevi@sreedevi: \"$ \n</code></pre> <p>Steps to fix boot delay from \"waiting for network online\" and reconfigure GDM for next boot: </p> <p>sudo systemctl disable NetworkManager-wait-online.service  sudo systemctl mask NetworkManager-wait-online.service  sudo apt update  sudo apt install spice-vdagent virtualbox-guest-additions-iso sudo systemctl set-default multi-user.target </p> <p>sudo systemctl set-default graphical.target # switching to GUI</p> <p>Prompt from me: After Select Advanced options for Ubuntu \u2192 recovery mode. Choose Root \u2013 Drop to root shell prompt. the prompt says enter for maintenance and press control D to continue after I pressed control d I see <code>root@sreedevi</code> how should I proceed to do these : sudo systemctl disable NetworkManager-wait-online.service sudo systemctl mask NetworkManager-wait-online.service sudo apt update sudo apt install virtualbox-guest-utils virtualbox-guest-x11 virtualbox-guest-dkms sudo dpkg-reconfigure gdm3</p> <p>If root filesystem is mounted as read-only : <code>mount -o remount,rw /</code></p> <p>After Select Advanced options for Ubuntu -&gt; recovery mode </p>"},{"location":"Remote_PortForwarding_SSH/","title":"Remote Port Fowarding SSH","text":"<p>The ubuntu vm is on a Remote Desktop, docker container is running on the ubuntu vm which is on the Remote Desktop, and I want to be able to access the docker container from my own host machine, not on the Remote Desktop, are there additional steps to be done if I want to access the plc browser of the docker container</p> <p>\ud83e\udde9 Your Current Setup (summarized)</p> <p>Let\u2019s restate the network chain: Your Laptop (Host)    \u2193 Remote Desktop Server (Hypervisor or another host)    \u2193 Ubuntu VM (IP: 192.168.30.60)    \u2193 Docker Container (OpenPLC, IP: 192.168.30.201 on macvlan)</p> <p>You already did: The macvlan network setup \u2705 The container is running and reachable from inside the Ubuntu VM \u2705 (ping -I macvlan0 192.168.30.201)</p> <p>The OpenPLC web UI runs on port 8080 inside the container \u2705 (default)</p> <p>Now the problem: You cannot reach http://192.168.30.201:8080 from your own laptop, only from within the Ubuntu VM. \ud83e\udde0 Why This Happens</p> <p>A macvlan network gives containers their own MAC addresses and connects them directly to the physical network \u2014 but only within the same Layer 2 domain (LAN). If your laptop and the Ubuntu VM\u2019s network (192.168.30.x) are not on the same LAN, or if the Remote Desktop host doesn\u2019t bridge your VM\u2019s NIC properly, your laptop won\u2019t be able to reach that container\u2019s IP directly.</p> <p>So let\u2019s go through possible solutions.</p> <p>\ud83d\udee0\ufe0f Option 1: Access via Port Forwarding (simplest &amp; most reliable) You can forward the container\u2019s port (8080) to the Ubuntu VM\u2019s IP, which you already can reach through Remote Desktop.</p> <p>\ud83e\ude9c Steps: 1\ufe0f\u20e3 Stop and remove the container that\u2019s using macvlan: docker stop st_plc docker rm st_plc</p> <p>2\ufe0f\u20e3 Run it again using bridge network (not macvlan), and expose port 8080: docker run -d --name st_plc -p 8080:8080 --privileged openplc:v3</p> <p>3\ufe0f\u20e3 Check it\u2019s running: docker ps</p> <p>4\ufe0f\u20e3 From your laptop, open: http://:8080 <p>Example: http://192.168.30.60:8080</p> <p>\u2705 You\u2019ll see the OpenPLC web interface.</p> <p>\ud83d\udca1 Why this works</p> <p>The port mapping (-p 8080:8080) tells Docker:</p> <p>\u201cForward traffic from the VM\u2019s port 8080 \u2192 to the container\u2019s port 8080.\u201d</p> <p>Your laptop can reach the Ubuntu VM\u2019s IP.</p> <p>So you can reach the container UI directly.</p> <p>\ud83e\uddf1 Option 2: Keep macvlan and bridge traffic from outside (advanced)</p> <p>If you must keep macvlan (for PLC simulation realism or LAN integration), then your VM\u2019s NIC must be in the same broadcast domain as your laptop.</p> <p>That means:</p> <p>The 192.168.30.x network of your container and VM must be accessible from your laptop\u2019s network.</p> <p>If your Remote Desktop VM is NATed, your laptop can\u2019t reach it directly \u2014 you\u2019ll need to set up a route or bridge on the hypervisor level.</p> <p>For example, on the Remote Desktop host (if you control it):</p> <p>sudo ip route add 192.168.30.0/24 via  <p>And ensure the hypervisor\u2019s virtual switch is in bridged mode (not NAT).</p> <p>But this setup requires admin access to the Remote Desktop host \u2014 not always possible.</p> <p>\u2699\ufe0f Option 3: SSH Tunnel (secure workaround)</p> <p>If you cannot change network bridging but can SSH into the Ubuntu VM, you can tunnel the OpenPLC port to your local machine.</p> <p>\ud83e\ude9c Steps from your laptop:</p> <p><code>ssh -L 8080:192.168.30.201:8080 ubuntu@&lt;Ubuntu_VM_IP&gt;</code></p> <p>Then, on your laptop, visit:</p> <p>http://localhost:8080</p> <p>\u2705 SSH securely forwards port 8080 from your local laptop to the container inside the VM, even through NAT or Remote Desktop.</p> <p>\ud83e\udde9 Summary Table Goal    Method  Pros    Cons</p> <p>\u2705 Access from your laptop easily    Port forwarding (-p 8080:8080)  Easiest, works anywhere Container no longer uses macvlan</p> <p>\u2699\ufe0f Keep macvlan realism Bridge networking on host   Same subnet realism Needs admin control on Remote Desktop host</p> <p>\ud83d\udd12 Secure remote access  SSH tunnel  No reconfiguration, encrypted   Needs SSH access every time</p> <p>\ud83d\ude80 Recommended for You</p> <p>Since your VM is inside a Remote Desktop environment, the simplest and most stable method is: \u27a4 Use Option 1 (Port Forwarding):</p> <p><code>docker run -d --name st_plc -p 8080:8080 --privileged openplc:v3</code></p> <p>Then access from your host browser:</p> <p><code>http://&lt;Ubuntu_VM_IP&gt;:8080</code></p> <p>That\u2019ll reliably work regardless of NAT or RDP network complexity.</p> <p>Would you like me to show how you can keep both macvlan and port forwarding simultaneously \u2014 so you can test PLC communications and still access the UI from your host machine?</p>"},{"location":"Remote_PortForwarding_SSH/#question","title":"Question","text":"<p>I have multiple plc containers running on different ip addresses on the docker macvlan network  If I use localhost to access it will not be possible to access different containers on the browser separately so I need to use the ip address to be unique and use that to access the plc runtime </p> <p>ssh -L 8080:192.168.30.201:8080 ubuntu@192.168.30.60</p> <p>http://localhost:8080</p>"},{"location":"Remote_PortForwarding_SSH/#answer","title":"Answer","text":"<p>Excellent observation \u2014 you\u2019re absolutely right \u2705</p> <p>If you have multiple OpenPLC containers, each running on a different IP (for example 192.168.30.201, 192.168.30.202, 192.168.30.203, etc.) in your Docker macvlan network, then mapping them all to localhost:8080 would overlap \u2014 only one tunnel could use that local port at a time. Let\u2019s go over how to handle this cleanly and flexibly.</p> <p>\ud83e\udde0 Why localhost won\u2019t scale</p> <p>If you tunnel all of them like:</p> <p><code>ssh -L 8080:192.168.30.201:8080 ubuntu@192.168.30.60</code></p> <p>then you\u2019d always have to reuse localhost:8080 \u2014 so only one can be accessed at once.</p> <p>You need different local ports for each tunnel. \u2699\ufe0f The Correct Way (multiple tunnels, unique local ports) Use different local ports per container:</p>"},{"location":"Remote_PortForwarding_SSH/#for-first-plc","title":"For first PLC","text":"<p>ssh -L 8081:192.168.30.201:8080 ubuntu@192.168.30.60</p>"},{"location":"Remote_PortForwarding_SSH/#for-second-plc","title":"For second PLC","text":"<p>ssh -L 8082:192.168.30.202:8080 ubuntu@192.168.30.60</p>"},{"location":"Remote_PortForwarding_SSH/#for-third-plc","title":"For third PLC","text":"<p>ssh -L 8083:192.168.30.203:8080 ubuntu@192.168.30.60</p> <p>Then on your host browser, you can open: PLC Browser URL PLC #1  http://localhost:8081 PLC #2  http://localhost:8082 PLC #3  http://localhost:8083 Each one will map to a different container, through the SSH tunnel.</p> <p>\ud83e\udde0 What\u2019s happening behind the scenes</p> <p>SSH on your host forwards:</p> <p>localhost:8081 \u2192 (SSH tunnel) \u2192 192.168.30.201:8080 localhost:8082 \u2192 (SSH tunnel) \u2192 192.168.30.202:8080 localhost:8083 \u2192 (SSH tunnel) \u2192 192.168.30.203:8080</p> <p>over the single SSH connection to your Ubuntu VM.</p> <p>\ud83d\ude80 Optional: Single SSH session, multiple tunnels Instead of opening multiple SSH windows, you can define multiple -L options in one command: ssh \\ -L 8081:192.168.30.201:8080 \\ -L 8082:192.168.30.202:8080 \\ -L 8083:192.168.30.203:8080 \\ ubuntu@192.168.30.60</p> <p>Then open all three PLC interfaces from your host browser:</p> <p>http://localhost:8081 http://localhost:8082 http://localhost:8083</p> <p>\ud83e\udde9 Summary Goal    Command Browser URL Access PLC 1    ssh -L 8081:192.168.30.201:8080 ubuntu@192.168.30.60    http://localhost:8081 Access PLC 2    ssh -L 8082:192.168.30.202:8080 ubuntu@192.168.30.60    http://localhost:8082 Access PLC 3    ssh -L 8083:192.168.30.203:8080 ubuntu@192.168.30.60    http://localhost:8083 Would you like me to show you how to automate this SSH tunneling (e.g. using an .ssh/config file or startup script) so you don\u2019t need to type all these each time?</p>"},{"location":"Routes/","title":"Question: how do I check that this does not conflict with any of the other existing routes sudo route -n add 192.168.40.0/24 192.168.30.60","text":"<p>To check whether adding the route will conflict with any existing routes on your macOS machine, follow these steps before you run:</p> <pre><code>sudo route -n add 192.168.40.0/24 192.168.30.60\n</code></pre>"},{"location":"Routes/#1-check-your-current-routing-table","title":"\u2705 1. Check your current routing table","text":"<p>On your MacBook, run:</p> <pre><code>netstat -rn\n</code></pre> <p>or:</p> <pre><code>route -n get 192.168.40.0\n</code></pre> <p>You are looking for any existing route entries that already reference 192.168.40.0/24.</p>"},{"location":"Routes/#example-of-what-to-look-for","title":"Example of what to look for:","text":"<ul> <li>If you see something like:</li> </ul> <pre><code>192.168.40/24      link#xx\n</code></pre> <p>or:</p> <pre><code>192.168.40.0/24    192.168.1.1\n</code></pre> <p>That means a route already exists, and adding a new one will create conflict or override it.</p> <p>\u2705 If no entry exists for <code>192.168.40.*</code>, then it is safe.</p>"},{"location":"Routes/#2-check-for-overlapping-networks","title":"\u2705 2. Check for overlapping networks","text":"<p>Run:</p> <pre><code>ifconfig\n</code></pre> <p>or:</p> <pre><code>ipconfig getifaddr en0\n</code></pre> <p>You want to ensure none of your Mac interfaces are already using:</p> <ul> <li>192.168.40.x</li> <li>A VPN that uses 192.168.40.x</li> <li>A hotspot that uses 192.168.40.x</li> </ul> <p>If any interface is already in <code>192.168.40.0/24</code>, then the macvlan network will conflict.</p>"},{"location":"Routes/#3-verify-gateway-reachability-1921683060","title":"\u2705 3. Verify gateway reachability (192.168.30.60)","text":"<p>The route you plan to add uses:</p> <p>192.168.30.60 \u2192 Ubuntu VM interface (enp0s3)</p> <p>Check that it is reachable from your Mac:</p> <pre><code>ping 192.168.30.60\n</code></pre> <p>If unreachable, the static route will not work.</p>"},{"location":"Routes/#4-verify-that-macvlan-network-is-isolated","title":"\u2705 4. Verify that macvlan network is isolated","text":"<p>macvlan uses:</p> <ul> <li>PLCs: <code>192.168.40.x</code></li> <li>Ubuntu host macvlan interface: <code>192.168.40.250</code></li> <li>But macOS cannot reach them unless routed.</li> </ul> <p>So adding the route:</p> <pre><code>192.168.40.0/24 via 192.168.30.60\n</code></pre> <p>is correct as long as no other network on your Mac already uses 192.168.40.0/24.</p>"},{"location":"Routes/#5-simulate-the-routing-decision-dry-run","title":"\u2705 5. Simulate the routing decision (Dry-run)","text":"<p>Before adding the route, check how macOS currently resolves that subnet:</p> <pre><code>route -n get 192.168.40.100\n</code></pre> <p>Expected output before adding route:</p> <pre><code>route: no route to host\n</code></pre> <p>or:</p> <pre><code>gateway: 192.168.30.1 (your home router)\n</code></pre> <p>If the gateway shown is your router (192.168.30.1), then it's fine\u2014you want macOS to stop sending 192.168.40.* traffic to your router and instead route it to the VM.</p>"},{"location":"Routes/#final-steps","title":"\u2705 Final steps","text":""},{"location":"Routes/#if-no-conflict-add-the-route","title":"\u2705 If no conflict \u2192 Add the route:","text":"<pre><code>sudo route -n add 192.168.40.0/24 192.168.30.60\n</code></pre>"},{"location":"Routes/#confirm-the-route-was-added","title":"\u2705 Confirm the route was added:","text":"<pre><code>netstat -rn | grep 192.168.40\n</code></pre> <p>Expected:</p> <pre><code>192.168.40/24    192.168.30.60    UGSc\n</code></pre>"},{"location":"Routes/#after-adding-route-test-connectivity-to-plc-container","title":"\u2705 After adding route, test connectivity to PLC container:","text":"<pre><code>curl http://192.168.40.202:8080\n</code></pre> <p>or:</p> <pre><code>ping 192.168.40.202\n</code></pre>"},{"location":"Routes/#if-you-want-me-to-check-your-routing-table-now","title":"\u2705 If you want me to check your routing table now","text":"<p>Paste the output:</p> <pre><code>netstat -rn\n</code></pre> <p>and I will tell you exactly whether it conflicts and whether it is safe to add this new static route.</p>"},{"location":"Routes/#dnat-rules","title":"DNAT RULES","text":"<p>Here is how you can reuse the same iptables DNAT (port-forwarding) setup even when you must stay connected through a VPN and cannot rely on macOS en0 bridging.</p>"},{"location":"Routes/#goal-recap","title":"\u2705 Goal Recap","text":"<p>You want to access 4 different remote servers (192.168.40.201\u2013204 port 8080) through your remote Ubuntu VM using DNAT:</p> Public Port on VM Internal Target 8081 192.168.40.201:8080 8082 192.168.40.202:8080 8083 192.168.40.203:8080 8084 192.168.40.204:8080 <p>And you access them from your MacBook using:</p> <pre><code>http://192.168.30.60:8081/login\nhttp://192.168.30.60:8082/login\nhttp://192.168.30.60:8083/login\nhttp://192.168.30.60:8084/login\n</code></pre> <p>This means: \u2705 You want all port forwarding to stay inside the remote Ubuntu VM \u2705 You want to keep using the VM as a reverse proxy \u2705 You want this to work whether VPN is up or not \u2705 macOS interface (en0) should not matter</p> <p>\u2705 This is possible. You just need correct IP forwarding + NAT rules inside the Ubuntu VM.</p>"},{"location":"Routes/#1-enable-ip-forwarding-on-the-remote-ubuntu-vm","title":"\u2705 1. Enable IP Forwarding on the Remote Ubuntu VM","text":"<p>Run:</p> <pre><code>sudo sysctl -w net.ipv4.ip_forward=1\n</code></pre> <p>Make it permanent:</p> <pre><code>echo \"net.ipv4.ip_forward=1\" | sudo tee -a /etc/sysctl.conf\nsudo sysctl -p\n</code></pre>"},{"location":"Routes/#2-add-your-dnat-port-forwarding-rules","title":"\u2705 2. Add Your DNAT Port-Forwarding Rules","text":""},{"location":"Routes/#dnat-destination-nat","title":"DNAT (destination NAT):","text":"<pre><code>sudo iptables -t nat -A PREROUTING -p tcp --dport 8081 -j DNAT --to-destination 192.168.40.201:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8082 -j DNAT --to-destination 192.168.40.202:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8083 -j DNAT --to-destination 192.168.40.203:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8084 -j DNAT --to-destination 192.168.40.204:8080\n</code></pre>"},{"location":"Routes/#snat-so-return-packets-go-back-correctly","title":"SNAT (so return packets go back correctly):","text":"<p>Assuming the VM internal interface is eth0 and its internal IP is 192.168.30.60:</p>"},{"location":"Routes/#this-is-not-correct-refer-to-version-below-instead","title":"THIS IS NOT CORRECT - REFER TO VERSION BELOW INSTEAD !","text":"<pre><code>sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE \n</code></pre> <p>If the VPN creates a tun0 interface and traffic comes in through it, add:</p> <pre><code>sudo iptables -t nat -A POSTROUTING -o tun0 -j MASQUERADE\n</code></pre>"},{"location":"Routes/#3-allow-traffic-through-firewall-if-ufw-is-active","title":"\u2705 3. Allow traffic through firewall (if ufw is active)","text":"<p>Check:</p> <pre><code>sudo ufw status\n</code></pre> <p>If active:</p> <pre><code>sudo ufw allow 8081/tcp\nsudo ufw allow 8082/tcp\nsudo ufw allow 8083/tcp\nsudo ufw allow 8084/tcp\n</code></pre>"},{"location":"Routes/#4-verify-port-forwarding-is-working","title":"\u2705 4. Verify Port-Forwarding Is Working","text":""},{"location":"Routes/#check-nat-rules","title":"Check NAT rules:","text":"<pre><code>sudo iptables -t nat -L -n -v\n</code></pre> <p>You should see entries like:</p> <pre><code>DNAT tcp -- anywhere 192.168.30.60 tcp dpt:8081 to:192.168.40.201:8080\n</code></pre>"},{"location":"Routes/#5-test-from-the-ubuntu-vm-first","title":"\u2705 5. Test from the Ubuntu VM first","text":"<pre><code>curl http://192.168.30.60:8081/login\ncurl http://192.168.30.60:8082/login\n</code></pre> <p>If these work \u2192 MacBook will also work.</p>"},{"location":"Routes/#6-nothing-needed-on-your-macbook","title":"\u2705 6. Nothing Needed on Your MacBook","text":"<p>Since DNAT happens inside the Ubuntu VM, macOS en0 or VPN interfaces do not affect your success.</p> <p>You only need:</p> <p>\u2705 You can reach 192.168.30.60 \u2705 The Ubuntu VM has working DNAT rules \u2705 VPN routes traffic to 192.168.40.x</p>"},{"location":"Routes/#summary-this-solves-your-problem","title":"\u2705 Summary: This solves your problem","text":"<p>\u2705 Keep using VPN \u2705 Keep using the Ubuntu VM \u2705 Reuse iptables DNAT \u2705 No need to modify Mac networking \u2705 Access continues to work through:</p> <pre><code>192.168.30.60:8081 \u2192 192.168.40.201:8080\n192.168.30.60:8082 \u2192 192.168.40.202:8080\n192.168.30.60:8083 \u2192 192.168.40.203:8080\n192.168.30.60:8084 \u2192 192.168.40.204:8080\n</code></pre> <p>If you'd like, I can also:</p> <p>\u2705 generate a persistent iptables script \u2705 convert this to nftables (recommended on Ubuntu 20.04+) \u2705 troubleshoot broken forwarding rules</p> <p>Just tell me!</p>"},{"location":"Routes/#question-which-of-these-am-i-supposed-to-do","title":"QUESTION: which of these am I supposed to do","text":"<p>SNAT (so return packets go back correctly):</p> <p>Assuming the VM internal interface is eth0 and its internal IP is 192.168.30.60:</p> <p>sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE</p> <p>If the VPN creates a tun0 interface and traffic comes in through it, add:</p> <p>sudo iptables -t nat -A POSTROUTING -o tun0 -j MASQUERADE</p> <p>Here is the simple rule for you:</p> <p>\u2705 You only need to MASQUERADE on the interface that traffic is leaving the VM through. You do not apply both unless your traffic leaves through both.</p> <p>Let\u2019s break it down clearly.</p>"},{"location":"Routes/#step-1-identify-which-interface-is-used-for-routing-to-19216840x","title":"\u2705 Step 1 \u2014 Identify which interface is used for routing to 192.168.40.x","text":"<p>Run this on the Ubuntu VM:</p> <pre><code>ip route get 192.168.40.201\n</code></pre> <p>This will output something like:</p> <pre><code>192.168.40.201 via 192.168.30.1 dev eth0\n</code></pre> <p>or maybe:</p> <pre><code>192.168.40.201 via 10.8.0.1 dev tun0\n</code></pre> <p>\u2705 Whatever dev  appears is the interface you must use in POSTROUTING."},{"location":"Routes/#step-2-use-the-correct-masquerade-rule-based-on-the-result","title":"\u2705 Step 2 \u2014 Use the correct MASQUERADE rule based on the result","text":""},{"location":"Routes/#case-a-if-it-says-dev-eth0","title":"\u2705 Case A: If it says <code>dev eth0</code>","text":"<p>Use:</p> <pre><code>sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\n</code></pre>"},{"location":"Routes/#case-b-if-it-says-dev-tun0","title":"\u2705 Case B: If it says <code>dev tun0</code>","text":"<p>Use:</p> <pre><code>sudo iptables -t nat -A POSTROUTING -o tun0 -j MASQUERADE\n</code></pre>"},{"location":"Routes/#case-c-if-some-servers-go-through-eth0-and-some-through-tun0","title":"\u2705 Case C: If some servers go through eth0 and some through tun0","text":"<p>Then \u2014 and only then \u2014 use both.</p>"},{"location":"Routes/#why-this-matters","title":"\u2705 Why this matters","text":"<p>DNAT forwards traffic into your VM, but when the internal server replies:</p> <ul> <li>It must send packets back the same interface the VM used to reach it.</li> <li>MASQUERADE ensures the internal server returns traffic to the VM, not directly to your Mac.</li> </ul> <p>Without the correct MASQUERADE \u2192 the return packets never reach you.</p>"},{"location":"Routes/#quick-example-so-youre-confident","title":"\u2705 Quick example so you're confident","text":""},{"location":"Routes/#if-you-run","title":"If you run:","text":"<pre><code>ip route get 192.168.40.201\n</code></pre> <p>Result is:</p> <pre><code>192.168.40.201 via 192.168.30.1 dev eth0\n</code></pre> <p>\u2705 You only need the eth0 rule.</p>"},{"location":"Routes/#want-me-to-check-your-routing-output","title":"\u2705 Want me to check your routing output?","text":"<p>Paste the output of:</p> <pre><code>ip route\nip addr\nip route get 192.168.40.201\n</code></pre> <p>I will tell you exactly which MASQUERADE rule is correct.</p>"},{"location":"Routes/#output","title":"Output :","text":"<pre><code>ubuntu@pgt:~/Setting up IED (py files to share)/IED_DOCKERS$ ip route get 192.168.40.201 192.168.40.201 dev macvlan0 src 192.168.40.254 uid 1000 cache ubuntu@pgt:~/Setting up IED (py files to share)/IED_DOCKERS$\n</code></pre> <p>Great \u2014 this makes it very clear what you need.</p> <p>Your Ubuntu VM reaches 192.168.40.x through:</p> <p>\u2705 Interface: <code>macvlan0</code> \u2705 Source IP: <code>192.168.40.254</code></p> <p>That means:</p> <ul> <li>Your VM already has an IP inside the 192.168.40.0/24 network (via macvlan)</li> <li>Packets to 192.168.40.x do NOT leave through eth0 or tun0</li> <li>DNAT forwarding will only work if you MASQUERADE out of macvlan0</li> </ul> <p>So the correct SNAT rule is:</p>"},{"location":"Routes/#snat-masquerade-rule","title":"\u2705 SNAT / MASQUERADE rule","text":"<pre><code>sudo iptables -t nat -A POSTROUTING -o macvlan0 -j MASQUERADE\n</code></pre> <p>This ensures that replies from:</p> <ul> <li>192.168.40.201</li> <li>192.168.40.202</li> <li>192.168.40.203</li> <li>192.168.40.204</li> </ul> <p>\u2026go back to the VM instead of being misrouted.</p>"},{"location":"Routes/#final-dnat-configuration-for-reference","title":"\u2705 Final DNAT configuration (for reference)","text":"<pre><code>sudo iptables -t nat -A PREROUTING -p tcp --dport 8081 -j DNAT --to-destination 192.168.40.201:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8082 -j DNAT --to-destination 192.168.40.202:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8083 -j DNAT --to-destination 192.168.40.203:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8084 -j DNAT --to-destination 192.168.40.204:8080\n</code></pre> <p>And SNAT:</p> <pre><code>sudo iptables -t nat -A POSTROUTING -o macvlan0 -j MASQUERADE\n</code></pre>"},{"location":"Routes/#how-to-test-now","title":"\u2705 How to test now","text":"<p>From your Mac:</p> <pre><code>curl http://192.168.30.60:8082/login\n</code></pre> <p>OR:</p> <pre><code>curl -v http://192.168.30.60:8082/login\n</code></pre> <p>If still no response, I can walk you through enabling <code>iptables</code> logging and confirm packets are being NATed.</p> <p>If you want, show me:</p> <pre><code>ip addr\nbrctl show\ndocker network ls\ndocker network inspect &lt;your network&gt;\n</code></pre> <p>I can confirm exactly how your macvlan is wired.</p>"},{"location":"Routes/#question-will-these-dnat-configurations-persist-when-vm-is-rebooted-if-not-how-can-i-make-it-persist-across-reboots-of-the-vm","title":"Question: will these DNAT configurations persist when VM is rebooted? if not, how can I make it persist across reboots of the VM ?","text":"<pre><code>\u2705 Final DNAT configuration (for reference)\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8081 -j DNAT --to-destination 192.168.40.201:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8082 -j DNAT --to-destination 192.168.40.202:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8083 -j DNAT --to-destination 192.168.40.203:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8084 -j DNAT --to-destination 192.168.40.204:8080\nAnd SNAT:\nsudo iptables -t nat -A POSTROUTING -o macvlan0 -j MASQUERADE\n</code></pre> <p>Short answer: No, they will NOT persist after a reboot unless you explicitly save and restore them.</p> <p>Below is the safest and simplest way to make your DNAT + SNAT rules persistent on Ubuntu.</p>"},{"location":"Routes/#best-method-use-iptables-persistent","title":"\u2705 Best Method: Use <code>iptables-persistent</code>","text":""},{"location":"Routes/#1-install-the-persistence-package","title":"1. Install the persistence package","text":"<pre><code>sudo apt update\nsudo apt install iptables-persistent\n</code></pre> <p>During install, it will ask:</p> <p>\u2705 Save current IPv4 rules? \u2192 Select Yes \u2705 Save current IPv6 rules? \u2192 You can select No if you don\u2019t use IPv6.</p>"},{"location":"Routes/#2-add-your-dnat-snat-rules-again","title":"\u2705 2. Add your DNAT + SNAT rules again","text":"<p>(re-enter them so they get saved)</p> <pre><code>sudo iptables -t nat -A PREROUTING -p tcp --dport 8081 -j DNAT --to-destination 192.168.40.201:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8082 -j DNAT --to-destination 192.168.40.202:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8083 -j DNAT --to-destination 192.168.40.203:8080\nsudo iptables -t nat -A PREROUTING -p tcp --dport 8084 -j DNAT --to-destination 192.168.40.204:8080\n\nsudo iptables -t nat -A POSTROUTING -o macvlan0 -j MASQUERADE\n</code></pre>"},{"location":"Routes/#3-save-the-rules-permanently","title":"\u2705 3. Save the rules permanently","text":"<pre><code>sudo netfilter-persistent save\n</code></pre> <p>This writes the rules into:</p> <pre><code>/etc/iptables/rules.v4\n</code></pre>"},{"location":"Routes/#4-verify-that-they-are-saved","title":"\u2705 4. Verify that they are saved","text":"<pre><code>sudo cat /etc/iptables/rules.v4\n</code></pre> <p>You should see your DNAT + MASQUERADE rules in this file.</p>"},{"location":"Routes/#5-test-persistence-by-rebooting","title":"\u2705 5. Test persistence by rebooting","text":"<pre><code>sudo reboot\n</code></pre> <p>After reboot:</p> <pre><code>sudo iptables -t nat -L -n -v\n</code></pre> <p>You should still see:</p> <ul> <li>PREROUTING DNAT rules for ports 8081\u20138084</li> <li>POSTROUTING MASQUERADE for <code>macvlan0</code></li> </ul>"},{"location":"Routes/#alternative-not-recommended-unless-needed","title":"\u2705 Alternative (not recommended unless needed)","text":"<p>You can put rules inside a script like:</p> <p><code>/etc/network/if-up.d/iptables-rules</code></p> <p>But <code>iptables-persistent</code> is cleaner, reliable, and survives all reboots + network restarts.</p>"},{"location":"Routes/#summary","title":"\u2705 Summary","text":"Question Answer Do DNAT/SNAT rules persist after reboot? \u274c No How to persist them? \u2705 Install <code>iptables-persistent</code> and save Do they auto-load on boot? \u2705 Yes <p>If you want, I can generate the exact <code>/etc/iptables/rules.v4</code> file for your setup so you don\u2019t have to manually edit anything.</p>"},{"location":"SQL/","title":"SQL","text":"<p>To check if a column exists in table ccp_values;</p> <p><code>SHOW COLUMNS FROM ccp_values</code></p> <p>After confirming a column exists, to see all values from the column eg. st_steam_mass_flow_rate from the table:</p> <p><code>SELECT st_steam_mass_flow_rate FROM ccp_values;</code> </p> <p>To see recent values eg. last 10 rows only:</p> <p><code>SELECT timestamp, st_steam_mass_flow_rate FROM ccp_values ORDER BY timestamp, DESC LIMIT 10;</code></p>"},{"location":"SSH/","title":"SSH","text":"<p>keypair was created using command like below on windows :</p> <p>ssh-keygen-t ed25519 ...</p> <p>had to convert the private key that was generated on windows using dos2unix as the line endings of the text file generated on windows will be based on carriage return  can check using cat -v ~/.ssh/id_ed25519_private If you see ^M at the end of lines \u2192 that\u2019s a Windows CR (\\r).</p> <p>command to ssh to ncl node 10.13 this does not work (as need to create a config file ): ssh -i ~/.ssh/id_ed25519_private -J ADSC_SmartGrid_5@gateway.ncl.sg -i ~/.ssh/id_ed25519_private pgt@10.10.10.13</p> <p>Create config file first</p> <pre><code># Gateway host\nHost gateway\n    HostName gateway.ncl.sg\n    User ADSC_SmartGrid_5\n    IdentityFile ~/.ssh/id_ed25519_private\n\n# Intermediate host\nHost intermediate\n    HostName 172.18.178.17\n    User pgt\n    Port 4696\n    IdentityFile ~/.ssh/id_ed25519_private\n    ProxyJump gateway\n\n# Final internal host\nHost pgt10.13\n    HostName 10.10.10.13\n    User pgt\n    IdentityFile ~/.ssh/id_ed25519_private\n    ProxyJump intermediate\n\n</code></pre> <p><code>chmod 600 ~/.ssh/config</code> <code>ssh -v pgt10.13</code></p>"},{"location":"ScadaLTS/","title":"ScadaLTS","text":""},{"location":"ScadaLTS/#content-of-docker-composeyml","title":"Content of docker-compose.yml","text":"<p>For Powerplant Twin</p> <pre><code>version: '3.8'\nservices:\n  database:\n    container_name: mysql\n    image: mysql/mysql-server:5.7\n    environment:\n      - MYSQL_ROOT_PASSWORD=root\n      - MYSQL_USER=root\n      - MYSQL_PASSWORD=root\n      - MYSQL_DATABASE=scadalts\n    ports:\n      - \"3307:3306\"\n    volumes:\n      - mysql-data:/var/lib/mysql\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\", \"-uroot\", \"-proot\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n      start_period: 20s\n\n  scadalts:\n    container_name: scadalts\n    image: scadalts/scadalts:latest\n    depends_on:\n      database:\n        condition: service_healthy\n    ports:\n      - \"8080:8080\"\n    expose:\n      - \"8000\"\n\nvolumes:\n  mysql-data:\n</code></pre>"},{"location":"Systemctl_Systemd/","title":"Systemctl Systemd","text":"<p>Excellent question \u2014 this is a network/DNS resolution issue, not a Docker configuration problem. Let\u2019s break down what\u2019s happening and how to fix it properly.</p>"},{"location":"Systemctl_Systemd/#what-the-error-means","title":"\ud83e\udde9 What the error means","text":"<pre><code>Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 127.0.0.53:53: read udp 127.0.0.1:46176-&gt;127.0.0.53:53: i/o timeout\n</code></pre> <p>Explanation:</p> <ul> <li>Docker is trying to pull the image from Docker Hub (<code>registry-1.docker.io</code>).</li> <li>It\u2019s using the system DNS resolver (<code>127.0.0.53</code>, which is <code>systemd-resolved</code>).</li> <li>The lookup request timed out, meaning the VM can\u2019t resolve DNS names (even though it may have internet connectivity, e.g., <code>ping 8.8.8.8</code> works).</li> </ul>"},{"location":"Systemctl_Systemd/#step-1-test-dns-resolution-on-your-ubuntu-vm","title":"\u2705 Step 1: Test DNS resolution on your Ubuntu VM","text":"<p>Run:</p> <pre><code>ping -c 3 google.com\n</code></pre> <p>If it fails with \u201cTemporary failure in name resolution,\u201d DNS is the issue.</p>"},{"location":"Systemctl_Systemd/#step-2-fix-your-etcresolvconf","title":"\u2705 Step 2: Fix your <code>/etc/resolv.conf</code>","text":"<p>Run this command to check the current content:</p> <pre><code>cat /etc/resolv.conf\n</code></pre> <p>You\u2019ll likely see something like:</p> <pre><code>nameserver 127.0.0.53\noptions edns0 trust-ad\nsearch ...\n</code></pre> <p>That\u2019s not helpful for Docker containers when DNS forwarding breaks.</p> <p>Replace it with public DNS servers:</p> <pre><code>sudo bash -c 'cat &gt; /etc/resolv.conf &lt;&lt;EOF\nnameserver 8.8.8.8\nnameserver 1.1.1.1\nEOF'\n</code></pre>"},{"location":"Systemctl_Systemd/#step-3-restart-networking-and-docker","title":"\u2705 Step 3: Restart networking and Docker","text":"<pre><code>sudo systemctl restart systemd-resolved\nsudo systemctl restart docker\n</code></pre> <p>Then try again:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"Systemctl_Systemd/#optional-persist-dns-changes-if-using-cloud-init-or-netplan","title":"\u2699\ufe0f Optional: Persist DNS changes (if using cloud-init or Netplan)","text":"<p>Since your <code>/etc/netplan/50-cloud-init.yaml</code> file had:</p> <pre><code>nameservers:\n  addresses:\n    - 127.0.0.53\n    - 8.8.8.8\n    - 8.8.4.4\n</code></pre> <p>Update it to remove 127.0.0.53, like this:</p> <pre><code>nameservers:\n  addresses:\n    - 8.8.8.8\n    - 8.8.4.4\n</code></pre> <p>Then apply the changes:</p> <pre><code>sudo netplan apply\n</code></pre>"},{"location":"Systemctl_Systemd/#why-this-fixes-it","title":"\ud83e\udde0 Why this fixes it","text":"<p>Docker relies on the host\u2019s DNS resolution to fetch images. When Ubuntu uses <code>systemd-resolved</code> (127.0.0.53) and that resolver breaks (common on VMs), Docker can\u2019t resolve <code>registry-1.docker.io</code>. Setting <code>/etc/resolv.conf</code> to external DNS servers like Google or Cloudflare bypasses that problem.</p> <p>Would you like me to give you a version of this fix that persists across reboots (since cloud-init can overwrite <code>/etc/resolv.conf</code> each time)?</p> <p>Perfect \u2014 that\u2019s a very common situation on Ubuntu 20.04+ and 22.04 systems. You\u2019re correct not to directly edit <code>/etc/resolv.conf</code> because it\u2019s a symlink managed by <code>systemd-resolved</code>, not a static file.</p> <p>Here\u2019s exactly what you should do \ud83d\udc47</p>"},{"location":"Systemctl_Systemd/#step-by-step-fix-for-dns-resolution-recommended-way","title":"\ud83e\udded Step-by-step fix for DNS resolution (recommended way)","text":""},{"location":"Systemctl_Systemd/#step-1-confirm-the-symlink","title":"\ud83e\udde9 Step 1: Confirm the symlink","text":"<p>Run:</p> <pre><code>ls -l /etc/resolv.conf\n</code></pre> <p>You\u2019ll likely see something like:</p> <pre><code>/etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf\n</code></pre> <p>This confirms <code>systemd-resolved</code> is managing it.</p>"},{"location":"Systemctl_Systemd/#step-2-tell-systemd-resolved-to-use-public-dns-servers","title":"\ud83e\udde9 Step 2: Tell systemd-resolved to use public DNS servers","text":"<p>Create or edit this file:</p> <pre><code>sudo nano /etc/systemd/resolved.conf\n</code></pre> <p>Find or add these lines under <code>[Resolve]</code>:</p> <pre><code>[Resolve]\nDNS=8.8.8.8 8.8.4.4\nFallbackDNS=1.1.1.1\nDNSStubListener=no\n</code></pre> <p>\ud83d\udca1 The <code>DNSStubListener=no</code> line disables the local 127.0.0.53 stub that\u2019s currently breaking Docker lookups.</p> <p>Save and exit (<code>Ctrl+O</code>, <code>Enter</code>, then <code>Ctrl+X</code>).</p>"},{"location":"Systemctl_Systemd/#step-3-point-etcresolvconf-to-the-real-resolv-file","title":"\ud83e\udde9 Step 3: Point <code>/etc/resolv.conf</code> to the real resolv file","text":"<p>Replace the current symlink:</p> <pre><code>sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf\n</code></pre> <p>This file (<code>/run/systemd/resolve/resolv.conf</code>) contains the actual upstream DNS servers rather than the local stub.</p>"},{"location":"Systemctl_Systemd/#step-4-restart-systemd-resolved","title":"\ud83e\udde9 Step 4: Restart systemd-resolved","text":"<pre><code>sudo systemctl restart systemd-resolved\n</code></pre> <p>Then verify:</p> <pre><code>resolvectl status\n</code></pre> <p>You should now see:</p> <pre><code>DNS Servers: 8.8.8.8 8.8.4.4 1.1.1.1\n</code></pre>"},{"location":"Systemctl_Systemd/#step-5-test-dns-resolution","title":"\ud83e\udde9 Step 5: Test DNS resolution","text":"<pre><code>ping -c 3 google.com\n</code></pre> <p>If that works, try:</p> <pre><code>docker compose up -d\n</code></pre> <p>It should now be able to pull from Docker Hub successfully.</p>"},{"location":"Systemctl_Systemd/#why-this-method-is-best","title":"\ud83e\udde0 Why this method is best","text":"<p>This approach:</p> <ul> <li>Keeps <code>systemd-resolved</code> in control (so no conflicts with Ubuntu updates).</li> <li>Permanently replaces the stub DNS (127.0.0.53) with direct DNS servers.</li> <li>Persists across reboots and cloud-init updates.</li> </ul> <p>Would you like me to show you the persistent fix that also survives a full VM reboot or cloud-init regeneration (so it\u2019s never reset back to 127.0.0.53)?</p>"},{"location":"Systemctl_Systemd/#persistent-fix","title":"Persistent Fix","text":"<p>Excellent \u2014 let\u2019s make your DNS fix fully persistent, even across reboots and <code>cloud-init</code> regenerations.</p> <p>Here\u2019s the robust setup that ensures your VM always keeps working DNS (and Docker can always pull images).</p>"},{"location":"Systemctl_Systemd/#goal","title":"\ud83e\udded Goal","text":"<p>Stop Ubuntu\u2019s <code>cloud-init</code> and <code>systemd-resolved</code> from resetting <code>/etc/resolv.conf</code> to <code>127.0.0.53</code> \u2705 Survives reboots \u2705 Survives <code>cloud-init</code> reconfiguration \u2705 Keeps Docker name resolution reliable</p>"},{"location":"Systemctl_Systemd/#step-1-disable-cloud-init-network-management-optional-but-recommended","title":"\ud83d\udd27 Step 1: Disable <code>cloud-init</code> network management (optional but recommended)","text":"<p>Cloud-init regenerates <code>/etc/netplan/50-cloud-init.yaml</code> and resets DNS. To prevent that:</p> <pre><code>sudo mkdir -p /etc/cloud/cloud.cfg.d/\necho \"network: {config: disabled}\" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n</code></pre> <p>This stops cloud-init from overwriting your network configuration or DNS settings.</p>"},{"location":"Systemctl_Systemd/#step-2-configure-permanent-dns-in-systemd-resolved","title":"\ud83d\udd27 Step 2: Configure permanent DNS in <code>systemd-resolved</code>","text":"<p>Edit the config file:</p> <pre><code>sudo nano /etc/systemd/resolved.conf\n</code></pre> <p>Update it to include your preferred DNS servers:</p> <pre><code>[Resolve]\nDNS=8.8.8.8 8.8.4.4\nFallbackDNS=1.1.1.1\nDNSStubListener=no\n</code></pre> <p>Save and exit (<code>Ctrl+O</code>, <code>Enter</code>, <code>Ctrl+X</code>).</p>"},{"location":"Systemctl_Systemd/#step-3-replace-etcresolvconf-with-the-real-resolver-file","title":"\ud83d\udd27 Step 3: Replace <code>/etc/resolv.conf</code> with the real resolver file","text":"<p>Right now <code>/etc/resolv.conf</code> likely points to:</p> <pre><code>/run/systemd/resolve/stub-resolv.conf\n</code></pre> <p>\u2026which uses the stub <code>127.0.0.53</code> listener.</p> <p>Replace it permanently:</p> <pre><code>sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf\n</code></pre> <p>That file (<code>/run/systemd/resolve/resolv.conf</code>) always reflects the real upstream DNS servers (like 8.8.8.8).</p>"},{"location":"Systemctl_Systemd/#step-4-restart-services","title":"\ud83d\udd27 Step 4: Restart services","text":"<pre><code>sudo systemctl restart systemd-resolved\n</code></pre> <p>Then check status:</p> <pre><code>resolvectl status\n</code></pre> <p>You should now see:</p> <pre><code>DNS Servers: 8.8.8.8 8.8.4.4 1.1.1.1\nDNS Stub Listener: no\n</code></pre>"},{"location":"Systemctl_Systemd/#step-5-verify-dns","title":"\ud83d\udd27 Step 5: Verify DNS","text":"<p>Run:</p> <pre><code>ping -c 3 google.com\n</code></pre> <p>and:</p> <pre><code>docker pull hello-world\n</code></pre> <p>Both should now succeed instantly.</p>"},{"location":"Systemctl_Systemd/#why-this-is-persistent-and-safe","title":"\ud83e\udde0 Why this is persistent and safe","text":"Component Problem What we did <code>cloud-init</code> Rewrites network config Disabled network reconfiguration <code>systemd-resolved</code> Uses stub resolver (127.0.0.53) Pointed it to public DNS directly <code>/etc/resolv.conf</code> Symlinked to stub file Re-linked to the real resolver Reboot resets Temporary files regenerated <code>/etc/systemd/resolved.conf</code> persists"},{"location":"Systemctl_Systemd/#optional-make-sure-etcnetplan-matches-your-setup","title":"\ud83e\udde9 Optional: make sure <code>/etc/netplan</code> matches your setup","text":"<p>If your <code>/etc/netplan/50-cloud-init.yaml</code> defines DNS like:</p> <pre><code>nameservers:\n  addresses:\n    - 127.0.0.53\n</code></pre> <p>You can safely change it to:</p> <pre><code>nameservers:\n  addresses:\n    - 8.8.8.8\n    - 8.8.4.4\n</code></pre> <p>Then apply it:</p> <pre><code>sudo netplan apply\n</code></pre> <p>\u2705 After doing all this: Your DNS resolution (and Docker access) will remain correct permanently \u2014 even after reboot, <code>apt upgrade</code>, or VM image regeneration.</p> <p>Would you like me to show how to verify after reboot that your changes persist (a quick 2-command check)?</p>"},{"location":"Systemctl_Systemd/#persistent-dns-network-configuration-fix-across-reboots-and-cloud-init-resets","title":"Persistent DNS + network configuration fix across reboots and cloud-init resets.","text":"<p>''' explain each of these commands :</p> <p>sudo mkdir -p /etc/cloud/cloud.cfg.d/ echo \"network: {config: disabled}\" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg</p> <p>this is the content of /etc/netplan/50-cloud-init.yaml , and there is a comment that says any changes will not persist across an instance reboot  , what should I do to persist changes on this file too? summarise and consolidate all changes that I need to do to resolve the error here , to persist and to verify it after reboot </p> <p>ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ sudo cat /etc/netplan/50-cloud-init.yaml  [sudo] password for ubuntu: </p>"},{"location":"Systemctl_Systemd/#this-file-is-generated-from-information-provided-by-the-datasource-changes","title":"This file is generated from information provided by the datasource.  Changes","text":""},{"location":"Systemctl_Systemd/#to-it-will-not-persist-across-an-instance-reboot-to-disable-cloud-inits","title":"to it will not persist across an instance reboot.  To disable cloud-init's","text":""},{"location":"Systemctl_Systemd/#network-configuration-capabilities-write-a-file","title":"network configuration capabilities, write a file","text":""},{"location":"Systemctl_Systemd/#etccloudcloudcfgd99-disable-network-configcfg-with-the-following","title":"/etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:","text":""},{"location":"Systemctl_Systemd/#network-config-disabled","title":"network: {config: disabled}","text":"<p>network:     ethernets:         enp0s3:             dhcp4: false             addresses:               - 192.168.30.60/24             nameservers:               addresses:                 - 127.0.0.53                 - 8.8.8.8                 - 8.8.4.4             routes:               - to: default                 via: 192.168.30.1     version: 2 ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ </p> <p>drwxr-xr-x 6 ubuntu ubuntu 4096 Oct 21 02:20 ccp_multi_IED_multi_plc_database_pg ubuntu@pgt:~$ cd ccp_multi_IED_multi_plc_database_pg/ ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ docker compose up -d [+] Running 1/1  \u2718 postgres Error Get \"https://registry-1.docker.io/v2/...                10.2s  Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 127.0.0.53:53: read udp 127.0.0.1:46176-&gt;127.0.0.53:53: i/o timeout ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ ping -c 3 google.com ping: google.com: Temporary failure in name resolution ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ ping -c 3 google.com ping: google.com: Temporary failure in name resolution ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ cat /etc/resolv.conf</p>"},{"location":"Systemctl_Systemd/#this-is-runsystemdresolvestub-resolvconf-managed-by-mansystemd-resolved8","title":"This is /run/systemd/resolve/stub-resolv.conf managed by man:systemd-resolved(8).","text":""},{"location":"Systemctl_Systemd/#do-not-edit","title":"Do not edit.","text":""},{"location":"Systemctl_Systemd/#_1","title":"Systemctl Systemd","text":""},{"location":"Systemctl_Systemd/#this-file-might-be-symlinked-as-etcresolvconf-if-youre-looking-at","title":"This file might be symlinked as /etc/resolv.conf. If you're looking at","text":""},{"location":"Systemctl_Systemd/#etcresolvconf-and-seeing-this-text-you-have-followed-the-symlink","title":"/etc/resolv.conf and seeing this text, you have followed the symlink.","text":""},{"location":"Systemctl_Systemd/#_2","title":"Systemctl Systemd","text":""},{"location":"Systemctl_Systemd/#this-is-a-dynamic-resolvconf-file-for-connecting-local-clients-to-the","title":"This is a dynamic resolv.conf file for connecting local clients to the","text":""},{"location":"Systemctl_Systemd/#internal-dns-stub-resolver-of-systemd-resolved-this-file-lists-all","title":"internal DNS stub resolver of systemd-resolved. This file lists all","text":""},{"location":"Systemctl_Systemd/#configured-search-domains","title":"configured search domains.","text":""},{"location":"Systemctl_Systemd/#_3","title":"Systemctl Systemd","text":""},{"location":"Systemctl_Systemd/#run-resolvectl-status-to-see-details-about-the-uplink-dns-servers","title":"Run \"resolvectl status\" to see details about the uplink DNS servers","text":""},{"location":"Systemctl_Systemd/#currently-in-use","title":"currently in use.","text":""},{"location":"Systemctl_Systemd/#_4","title":"Systemctl Systemd","text":""},{"location":"Systemctl_Systemd/#third-party-programs-should-typically-not-access-this-file-directly-but-only","title":"Third party programs should typically not access this file directly, but only","text":""},{"location":"Systemctl_Systemd/#through-the-symlink-at-etcresolvconf-to-manage-manresolvconf5-in-a","title":"through the symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a","text":""},{"location":"Systemctl_Systemd/#different-way-replace-this-symlink-by-a-static-file-or-a-different-symlink","title":"different way, replace this symlink by a static file or a different symlink.","text":""},{"location":"Systemctl_Systemd/#_5","title":"Systemctl Systemd","text":""},{"location":"Systemctl_Systemd/#see-mansystemd-resolvedservice8-for-details-about-the-supported-modes-of","title":"See man:systemd-resolved.service(8) for details about the supported modes of","text":""},{"location":"Systemctl_Systemd/#operation-for-etcresolvconf","title":"operation for /etc/resolv.conf.","text":"<p>nameserver 127.0.0.53 options edns0 trust-ad search . ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ ls -l /etc/resolv.conf lrwxrwxrwx 1 root root 39 Sep 11  2024 /etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ ls /etc/netplan/ 50-cloud-init.yaml content of file /etc/netplan/50-cloud-init.yaml:^C '''</p> <p>Perfect \u2014 you\u2019ve given me everything I need. Let\u2019s walk through what\u2019s happening, what those commands do, and then I\u2019ll give you a complete, persistent DNS + network configuration fix that survives reboots and cloud-init resets.</p>"},{"location":"Systemctl_Systemd/#whats-happening","title":"\ud83e\udde9 What\u2019s happening","text":"<ul> <li>Your VM\u2019s <code>/etc/resolv.conf</code> is symlinked to a dynamically managed file:</li> </ul> <p><code>/etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf</code></p> <p>This is controlled by systemd-resolved.   So even if you edit it manually, it gets overwritten after reboot or service restart.</p> <ul> <li>The file <code>/etc/netplan/50-cloud-init.yaml</code> is auto-generated by cloud-init.   The comment:</li> </ul> <p>\u201cChanges to it will not persist across an instance reboot\u201d means cloud-init rewrites it every time the VM boots.</p> <ul> <li>Your DNS (<code>127.0.0.53</code>) is a local DNS stub resolver, which is failing to resolve queries.</li> </ul>"},{"location":"Systemctl_Systemd/#step-1-explanation-of-the-commands-you-mentioned","title":"\ud83e\udde0 Step 1: Explanation of the commands you mentioned","text":"<pre><code>sudo mkdir -p /etc/cloud/cloud.cfg.d/\n</code></pre> <ul> <li>Creates the directory <code>/etc/cloud/cloud.cfg.d/</code> if it doesn\u2019t exist.</li> <li>This directory stores override configurations for cloud-init.</li> </ul> <pre><code>echo \"network: {config: disabled}\" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n</code></pre> <ul> <li>This disables cloud-init\u2019s automatic network configuration, preventing it from regenerating <code>/etc/netplan/50-cloud-init.yaml</code> at each reboot.</li> <li>Once disabled, you control networking manually via Netplan.</li> </ul>"},{"location":"Systemctl_Systemd/#step-2-apply-persistent-dns-network-fix","title":"\u2699\ufe0f Step 2: Apply persistent DNS + network fix","text":"<p>Follow these steps exactly in order:</p>"},{"location":"Systemctl_Systemd/#1-disable-cloud-inits-network-regeneration","title":"1\ufe0f\u20e3 Disable cloud-init\u2019s network regeneration","text":"<pre><code>sudo mkdir -p /etc/cloud/cloud.cfg.d/\necho \"network: {config: disabled}\" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n</code></pre>"},{"location":"Systemctl_Systemd/#2-create-your-own-persistent-netplan-file","title":"2\ufe0f\u20e3 Create your own persistent netplan file","text":"<p>Create <code>/etc/netplan/01-netcfg.yaml</code> (custom, not cloud-init generated):</p> <pre><code>sudo nano /etc/netplan/01-netcfg.yaml\n</code></pre> <p>Paste the following content:</p> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp0s3:\n      dhcp4: no\n      addresses:\n        - 192.168.30.60/24\n      gateway4: 192.168.30.1\n      nameservers:\n        addresses:\n          - 8.8.8.8\n          - 8.8.4.4\n</code></pre> <p>\u2705 Explanation:</p> <ul> <li><code>renderer: networkd</code> ensures systemd-networkd handles the interface.</li> <li>We\u2019ve removed 127.0.0.53 so DNS goes directly to Google\u2019s servers.</li> <li>This file is not managed by cloud-init, so it persists.</li> </ul>"},{"location":"Systemctl_Systemd/#3-apply-and-verify","title":"3\ufe0f\u20e3 Apply and verify","text":"<pre><code>sudo netplan apply\n</code></pre> <p>Then test:</p> <pre><code>ping -c 3 google.com\n</code></pre> <p>If ping works \u2014 DNS is fixed.</p>"},{"location":"Systemctl_Systemd/#step-3-make-etcresolvconf-static-and-persistent","title":"\ud83d\udd01 Step 3: Make <code>/etc/resolv.conf</code> static and persistent","text":"<p>To break the dynamic symlink and use a static resolv.conf:</p> <pre><code>sudo rm /etc/resolv.conf\nsudo bash -c 'cat &gt; /etc/resolv.conf &lt;&lt;EOF\nnameserver 8.8.8.8\nnameserver 8.8.4.4\nEOF'\n</code></pre> <p>Then make sure it isn\u2019t replaced by systemd-resolved:</p> <pre><code>sudo systemctl disable systemd-resolved.service\nsudo systemctl stop systemd-resolved.service\n</code></pre> <p>Now <code>/etc/resolv.conf</code> will remain static and persistent.</p>"},{"location":"Systemctl_Systemd/#step-4-verify-persistence-after-reboot","title":"\u2705 Step 4: Verify persistence after reboot","text":"<p>After reboot:</p> <pre><code>cat /etc/resolv.conf\nls -l /etc/netplan/\ncat /etc/netplan/01-netcfg.yaml\n</code></pre> <p>You should see:</p> <ul> <li><code>resolv.conf</code> no longer symlinked.</li> <li>DNS pointing to 8.8.8.8 and 8.8.4.4.</li> <li><code>netplan</code> using your custom YAML.</li> <li>Network still working (<code>ping google.com</code> succeeds).</li> </ul>"},{"location":"Systemctl_Systemd/#summary-of-all-changes-to-persist-dns-fix-error","title":"\ud83e\uddfe Summary of all changes (to persist DNS &amp; fix error)","text":"Step Action Purpose Persistent? 1 <code>sudo mkdir -p /etc/cloud/cloud.cfg.d/</code> Ensure override directory exists \u2705 2 <code>echo \"network: {config: disabled}\" &gt; /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg</code> Disable cloud-init network resets \u2705 3 Create <code>/etc/netplan/01-netcfg.yaml</code> Manual static IP + DNS setup \u2705 4 <code>sudo netplan apply</code> Apply new config \u2705 5 Remove symlink <code>/etc/resolv.conf</code> and create static file Fix DNS permanently \u2705 6 <code>sudo systemctl disable systemd-resolved</code> Prevent it from overwriting resolv.conf \u2705 <p>Would you like me to show a minimal \u201ccopy-paste-ready\u201d version of all commands you can run in sequence (no explanations, just working setup)?</p>"},{"location":"Systemctl_Systemd/#after-sudo-netplan-apply-in-the-above-steps-still-got-error-in-dns-name-resolution","title":"After sudo netplan apply in the above steps, still got error in DNS name resolution","text":"<p>After doing sudo netplan apply, below is the output I get , why do I still get failure in name resolution and how to resolve</p> <pre><code>ubuntu@pgt:~$ ping -c 3 8.8.8.8\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=115 time=2.22 ms\n64 bytes from 8.8.8.8: icmp_seq=2 ttl=115 time=2.30 ms\n64 bytes from 8.8.8.8: icmp_seq=3 ttl=115 time=2.43 ms\n\n--- 8.8.8.8 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2002ms\nrtt min/avg/max/mdev = 2.224/2.316/2.425/0.082 ms\nubuntu@pgt:~$ ping -c 3 google.com\nping: google.com: Temporary failure in name resolution\nubuntu@pgt:~$ \n</code></pre> <p>Good observation \u2014 this tells us exactly what\u2019s working and what\u2019s not \ud83d\udc47</p> <ul> <li>\u2705 <code>ping 8.8.8.8</code> works \u2192 your network routing and internet connectivity are fine.</li> <li>\u274c <code>ping google.com</code> fails \u2192 your DNS resolution is broken.</li> </ul> <p>Let\u2019s go step-by-step to fix and persist it properly.</p>"},{"location":"Systemctl_Systemd/#step-1-confirm-what-etcresolvconf-points-to","title":"\ud83e\udde9 Step 1: Confirm what <code>/etc/resolv.conf</code> points to","text":"<p>Run:</p> <pre><code>ls -l /etc/resolv.conf\ncat /etc/resolv.conf\n</code></pre> <p>You will almost certainly see this:</p> <pre><code>/etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf\nnameserver 127.0.0.53\n</code></pre> <p>That means DNS queries are still going through the systemd-resolved stub (<code>127.0.0.53</code>), which is not using your Netplan <code>8.8.8.8</code> entries.</p>"},{"location":"Systemctl_Systemd/#step-2-point-resolvconf-to-the-real-dns-configuration","title":"\ud83e\uddf0 Step 2: Point resolv.conf to the real DNS configuration","text":"<p>You can safely replace that symlink with one that points to the correct file managed by <code>systemd-resolved</code>:</p> <pre><code>sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf\n</code></pre> <p>That file (<code>/run/systemd/resolve/resolv.conf</code>) contains the real upstream nameservers that systemd-resolved learned from Netplan.</p> <p>Now check again:</p> <pre><code>cat /etc/resolv.conf\n</code></pre> <p>\u2705 Expected:</p> <pre><code>nameserver 8.8.8.8\nnameserver 8.8.4.4\n</code></pre> <p>Then test:</p> <pre><code>ping -c3 google.com\n</code></pre> <p>If it resolves, DNS is now fixed.</p>"},{"location":"Systemctl_Systemd/#step-3-make-sure-this-survives-reboot","title":"\ud83e\udde9 Step 3: Make sure this survives reboot","text":"<p>You\u2019ve already disabled Cloud-Init network overwriting with:</p> <pre><code>echo \"network: {config: disabled}\" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n</code></pre> <p>and moved <code>50-cloud-init.yaml</code>, so this DNS configuration will persist.</p> <p>The <code>/etc/resolv.conf</code> symlink fix also persists automatically because it\u2019s a filesystem change, not a runtime file.</p>"},{"location":"Systemctl_Systemd/#step-4-optional-verification-after-reboot","title":"\ud83e\udde9 Step 4: Optional verification after reboot","text":"<p>After you reboot:</p> <pre><code>cat /etc/resolv.conf\nresolvectl status\nping -c3 google.com\n</code></pre> <p>All should show Google\u2019s DNS and successful resolution.</p> <p>\u2705 Summary of what was wrong and what fixed it</p> <ul> <li>Problem: <code>/etc/resolv.conf</code> still pointed to the stub at <code>127.0.0.53</code>, ignoring your Netplan DNS.</li> <li>Fix: Replace that symlink to point to <code>/run/systemd/resolve/resolv.conf</code>, which contains your correct DNS servers.</li> </ul> <p>After that, Docker and all other applications should be able to resolve hostnames normally.</p>"},{"location":"Ubuntu_Python_Packages/","title":"Ubuntu Python Packages","text":"<pre><code>sreedevi@sreedevi:~/Desktop/EPIC Attacks/Attacks/Attack 4 (Control Breaker)$ python3 -m venv venv\nThe virtual environment was not created successfully because ensurepip is not\navailable.  On Debian/Ubuntu systems, you need to install the python3-venv\npackage using the following command.\n\n    apt install python3.12-venv\n\nYou may need to use sudo with that command.  After installing the python3-venv\npackage, recreate your virtual environment.\n\nFailing command: /home/sreedevi/Desktop/EPIC Attacks/Attacks/Attack 4 (Control Breaker)/venv/bin/python3\n\nsreedevi@sreedevi:~/Desktop/EPIC Attacks/Attacks/Attack 4 (Control Breaker)$ \n</code></pre> <p>Nice \u2014 you hit a common bootstrap problem. The message means your system doesn\u2019t have the small support package that creates virtual environments (<code>ensurepip</code>), so <code>python3 -m venv</code> can\u2019t populate the venv with a working <code>pip</code>. Below are clear, safe steps to fix it and then install <code>pyiec61850</code> inside a venv.</p> <p>Follow these commands (copy-paste):</p> <ol> <li>Update apt and install venv support (and some helpful extras)</li> </ol> <pre><code>sudo apt update\n# install the venv helper; this will provide ensurepip inside venvs\nsudo apt install -y python3-venv\n\n# optional but recommended: pip, build tools and git\nsudo apt install -y python3-pip python3-full build-essential git\n</code></pre> <p>Notes:</p> <ul> <li>On newer Ubuntu releases with a specific Python minor (e.g. 3.12) the package name may be <code>python3.12-venv</code>. If <code>python3-venv</code> doesn't work, check your python version and install the matching package:</li> </ul> <pre><code>python3 --version\n# if it prints Python 3.12.x:\nsudo apt install -y python3.12-venv\n</code></pre> <ol> <li>Create the virtual environment again</li> </ol> <pre><code>cd ~/Desktop/\"EPIC Attacks\"/\"Attacks\"/\"Attack 4 (Control Breaker)\"\npython3 -m venv venv\n</code></pre> <ol> <li>Activate the venv and make sure pip is usable</li> </ol> <pre><code># activate (bash/zsh)\nsource venv/bin/activate\n\n# upgrade packaging tools inside the venv\npip install --upgrade pip setuptools wheel\n</code></pre> <ol> <li>Install the package inside the venv</li> </ol> <pre><code>pip install pyiec61850\n</code></pre> <p>If <code>pyiec61850</code> is not on PyPI, the previous <code>pip install</code> will say \u201cNo matching distribution.\u201d In that case install from GitHub (example):</p> <pre><code>git clone https://github.com/thiagohersan/pyiec61850.git\ncd pyiec61850\npip install .\n</code></pre> <ol> <li>Quick test</li> </ol> <pre><code>python -c \"import iec61850; print('OK:', iec61850)\"\n</code></pre> <p>If that prints without error, you\u2019re good.</p>"},{"location":"Ubuntu_Python_Packages/#troubleshooting-tips","title":"Troubleshooting tips","text":"<ul> <li> <p>If <code>python3 -m venv venv</code> still fails after installing <code>python3-venv</code>:</p> </li> <li> <p>Confirm <code>python3 --version</code> and install the matching <code>python3.X-venv</code> package (replace <code>X</code>).</p> </li> <li>Ensure you used <code>sudo apt update</code> before <code>apt install</code>.</li> <li> <p>If using WSL or an unusual distro, ensure your distro\u2019s Python packages are available (or consider using <code>pyenv</code> to install a self-contained Python).</p> </li> <li> <p>If you must install system-wide (not recommended), you can bypass PEP 668 with:</p> </li> </ul> <p><code>sudo pip3 install pyiec61850 --break-system-packages</code></p> <p>\u26a0\ufe0f This is discouraged because it can break OS-managed Python packages.</p> <ul> <li>If the library has C dependencies (common for IEC libraries), you may need <code>build-essential</code> and the <code>libiec61850</code> C library built/installed first. If <code>pip install</code> fails with a build error, paste the error and I\u2019ll help parse it.</li> </ul> <p>If you want, I can:</p> <ul> <li>Give the exact commands tailored to the Python version on your machine (tell me the output of <code>python3 --version</code>), or</li> <li>Walk through building <code>libiec61850</code> (the C library) and hooking it into Python if <code>pyiec61850</code> needs that.</li> </ul> <p>Which would you like next?</p>"},{"location":"VM-to-VM-Communication/","title":"VM-to-VM Communication","text":""},{"location":"VM-to-VM-Communication/#for-vm-to-vm-communication-on-virtualbox","title":"FOR VM TO VM COMMUNICATION ON VIRTUALBOX:","text":"<p>Had to use 2 adapters: NAT - enables internet connection for that VM Host only network - make sure to use the SAME host network for both </p>"},{"location":"Wine/","title":"Wine","text":"<p>How Linux WINE Works Under the Hood :</p> <p>https://www.youtube.com/watch?v=XSZ3UJbHqAk</p> <ul> <li>check about file sharing between wine -&gt; ubuntu -&gt; local pc (eg. MacBook)</li> <li>heck about file sharing between wine -&gt; local pc (eg. MacBook)</li> </ul> <p>https://unix.stackexchange.com/questions/344795/is-there-a-program-like-mobaxterm-for-linux-systems-sftp-gui-browser-termina</p>"},{"location":"Wine/#how-to-install-wine-on-ubuntu-2404-lts-linux-running-windows-programs-on-linux","title":"How to Install Wine on Ubuntu 24.04 LTS Linux | Running Windows Programs on Linux","text":"<p>https://youtu.be/J6EkEdQCNaQ?si=1QM9hzJSIiUMwCYW</p>"}]}