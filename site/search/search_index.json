{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Docker, Virtual Machines & Networks","text":"<p>Welcome! Use the sidebar to navigate to different sections of this project.</p>"},{"location":"#docker","title":"Docker","text":"<ul> <li>Docker Build</li> <li>Docker Networking</li> <li>Docker Macvlan Network</li> <li>Communication Between Inside &amp; Outside Docker</li> <li>Docker Container Interfaces</li> <li>Docker Engine on Ubuntu</li> <li>Using tarballs (.tgz)</li> <li>Docker Compose</li> <li>Docker Volumes</li> </ul>"},{"location":"#virtualization","title":"Virtualization","text":"<ul> <li>VM-to-VM Communication</li> <li>Fix Slow Ubuntu VM Boot</li> <li>Recovering VM</li> </ul>"},{"location":"#tools","title":"Tools","text":"<ul> <li>SSH Setup</li> <li>Matlab Setup</li> <li>Matlab_Docker</li> <li>Wine on Linux</li> <li>SQL</li> <li>[PostgreSQL] (PostgreSQL.md)</li> <li>Matlab Data Sources</li> <li>Remote Port Forwarding SSH</li> <li>NAT_Adapter_Iptables_Forwarding.md</li> <li>Bridged_Adapter</li> <li>File Transfer Host To Remote</li> <li>Systemctl Systemd</li> </ul>"},{"location":"#scada","title":"SCADA","text":"<ul> <li>ScadaLTS</li> </ul>"},{"location":"#os","title":"OS","text":"<ul> <li>Mac Notes</li> </ul>"},{"location":"Bridged_Adapter/","title":"Bridged Adapter","text":""},{"location":"Bridged_Adapter/#how-do-you-know-that-enp0s3-is-the-bridged-adapter","title":"How do you know that enp0s3 is the bridged adapter?","text":"<p>Excellent question \u2014 and one that a lot of people get tripped up on when mixing VirtualBox/VMware networking modes with Docker networking.</p> <p>Let\u2019s go step by step to make it crystal clear why we can conclude that <code>enp0s3</code> is the interface tied to the bridged adapter in your Ubuntu VM setup.</p>"},{"location":"Bridged_Adapter/#step-1-how-virtualbox-or-other-hypervisors-name-interfaces","title":"\ud83e\udde9 Step 1. How VirtualBox (or other hypervisors) name interfaces","text":"<p>When you create a VM in VirtualBox, each \u201cAdapter\u201d (NAT, Bridged, Host-only, etc.) is exposed inside the guest OS (Ubuntu) as a network interface \u2014 typically named:</p> <pre><code>enp0s3, enp0s8, enp0s9, etc.\n</code></pre> <p>The first adapter (Adapter 1) almost always appears as <code>enp0s3</code> inside the VM. So if Adapter 1 in VirtualBox is configured as Bridged Adapter, <code>enp0s3</code> is the bridged interface.</p>"},{"location":"Bridged_Adapter/#step-2-confirming-with-your-vms-ip","title":"\ud83e\udde9 Step 2. Confirming with your VM\u2019s IP","text":"<p>You ran:</p> <pre><code>ip addr show enp0s3\n</code></pre> <p>and got:</p> <pre><code>inet 192.168.30.60/24 brd 192.168.30.255 scope global enp0s3\n</code></pre> <p>Your Remote Desktop network and host network are also on the <code>192.168.30.0/24</code> subnet. \u27a1\ufe0f That means <code>enp0s3</code> is connected to the same LAN as your host and remote desktop \u2014 typical of a Bridged Adapter connection.</p> <p>If it were a NAT adapter, you\u2019d see a different subnet, like <code>10.0.2.x</code> (the default NAT range VirtualBox uses).</p> <p>\u2705 Therefore: The fact that <code>enp0s3</code> has <code>192.168.30.60</code> \u2014 a LAN IP \u2014 means it\u2019s bridged to your physical network card.</p>"},{"location":"Bridged_Adapter/#step-3-how-you-used-it-in-docker","title":"\ud83e\udde9 Step 3. How you used it in Docker","text":"<p>You created the Docker macvlan using:</p> <pre><code>docker network create -d macvlan \\\n  --subnet=192.168.40.0/24 \\\n  --gateway=192.168.40.1 \\\n  -o parent=enp0s3 \\\n  plc-macvlan-40\n</code></pre> <p>The <code>-o parent=enp0s3</code> tells Docker:</p> <p>\u201cAttach this macvlan network to the same Layer 2 segment as <code>enp0s3</code>.\u201d</p> <p>Since <code>enp0s3</code> is your bridged adapter to the real network, each container\u2019s IP (e.g., <code>192.168.40.201</code>) appears as a separate device on that LAN (from the VM\u2019s perspective).</p>"},{"location":"Bridged_Adapter/#step-4-how-you-verified-it-works","title":"\ud83e\udde9 Step 4. How you verified it works","text":"<p>You were able to access PLC containers from outside (your host):</p> <pre><code>http://192.168.30.60:8081/dashboard\n</code></pre> <p>That\u2019s only possible because:</p> <ul> <li><code>enp0s3</code> connects your Ubuntu VM directly to the same physical network as your host and remote desktop (bridged).</li> <li>iptables DNAT forwards traffic from <code>192.168.30.60</code> (on enp0s3) \u2192 PLC containers (on macvlan).</li> </ul> <p>If <code>enp0s3</code> were a NAT adapter, your host could not reach <code>192.168.30.60:8081</code> directly.</p>"},{"location":"Bridged_Adapter/#summary","title":"\u2705 Summary","text":"Interface Source Meaning <code>enp0s3</code> First network adapter in VirtualBox This is the Bridged Adapter connected to your LAN <code>192.168.30.60</code> IP address assigned to <code>enp0s3</code> Same subnet as your Remote Desktop and host \u2014 confirms it\u2019s bridged Docker parent <code>-o parent=enp0s3</code> Macvlan attaches to this physical NIC <code>192.168.40.0/24</code> macvlan subnet Separate subnet for Docker containers iptables rules Map 8081\u20138084 to 192.168.40.201\u2013204 Enable access from LAN to macvlan network via enp0s3"},{"location":"Bridged_Adapter/#tldr","title":"\ud83e\udde0 TL;DR","text":"<p>You know <code>enp0s3</code> is the bridged adapter because:</p> <ol> <li>It\u2019s the first adapter (VirtualBox convention).</li> <li>It has an IP in your LAN subnet (<code>192.168.30.x</code>).</li> <li>Traffic from your host can reach it directly.</li> <li>Your iptables DNAT setup works only because it\u2019s bridged.</li> </ol> <p>If you want, I can show you exactly how to verify this conclusively with a one-liner using:</p> <pre><code>ip route show default\n</code></pre> <p>Would you like me to include that check?</p>"},{"location":"Bridged_Adapter/#on-my-own-host-machine-where-i-access-the-plc-browsers-using-http19216830608081dashboard-i-do-not-see-enp0s3-or-any-interface-that-matches-the-subnet-of-the-vm-on-the-remote-desktop-explain-further-how-my-host-machine-is-able-to-access-the-ubuntu-vms-ip-and-the-docker-container-running-in-the-ubuntu-vm-when-there-is-no-corresponding-interface-or-correct-me-if-there-is-a-corresponding-interface-to-access-the-1921683060-on-enp0s3-plc-container","title":"On my own host machine where I access the plc browsers using http://192.168.30.60:8081/dashboard , I do not see enp0s3 or any interface that matches the subnet of the VM on the Remote Desktop , explain further how my host machine is able to access the ubuntu vm's ip and the docker container running in the Ubuntu vm when there is no corresponding interface, or correct me if there is a corresponding interface to access the 192.168.30.60 (on enp0s3) \u2192 PLC container","text":"<pre><code>Last login: Fri Oct 17 17:38:12 on ttys032\nsreedevirajavelu@iTrust-9000003511 ~ % ifconfig\nlo0: flags=8049&lt;UP,LOOPBACK,RUNNING,MULTICAST&gt; mtu 16384\n    options=1203&lt;RXCSUM,TXCSUM,TXSTATUS,SW_TIMESTAMP&gt;\n    inet 127.0.0.1 netmask 0xff000000\n    inet6 ::1 prefixlen 128 \n    inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\ngif0: flags=8010&lt;POINTOPOINT,MULTICAST&gt; mtu 1280\nstf0: flags=0&lt;&gt; mtu 1280\nanpi0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=400&lt;CHANNEL_IO&gt;\n    ether e6:59:2d:4a:00:b0\n    media: none\n    status: inactive\nanpi1: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=400&lt;CHANNEL_IO&gt;\n    ether e6:59:2d:4a:00:b1\n    media: none\n    status: inactive\nen3: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=400&lt;CHANNEL_IO&gt;\n    ether e6:59:2d:4a:00:90\n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: none\n    status: inactive\nen4: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=400&lt;CHANNEL_IO&gt;\n    ether e6:59:2d:4a:00:91\n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: none\n    status: inactive\nen1: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=460&lt;TSO4,TSO6,CHANNEL_IO&gt;\n    ether 36:e3:5e:f0:6d:80\n    media: autoselect &lt;full-duplex&gt;\n    status: inactive\nen2: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=460&lt;TSO4,TSO6,CHANNEL_IO&gt;\n    ether 36:e3:5e:f0:6d:84\n    media: autoselect &lt;full-duplex&gt;\n    status: inactive\nbridge0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=63&lt;RXCSUM,TXCSUM,TSO4,TSO6&gt;\n    ether 36:e3:5e:f0:6d:80\n    Configuration:\n        id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0\n        maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200\n        root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0\n        ipfilter disabled flags 0x0\n    member: en1 flags=3&lt;LEARNING,DISCOVER&gt;\n            ifmaxaddr 0 port 8 priority 0 path cost 0\n    member: en2 flags=3&lt;LEARNING,DISCOVER&gt;\n            ifmaxaddr 0 port 9 priority 0 path cost 0\n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: &lt;unknown type&gt;\n    status: inactive\nap1: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=6460&lt;TSO4,TSO6,CHANNEL_IO,PARTIAL_CSUM,ZEROINVERT_CSUM&gt;\n    ether b6:9a:14:e0:e6:d7\n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: autoselect (none)\n    status: inactive\nen0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=6460&lt;TSO4,TSO6,CHANNEL_IO,PARTIAL_CSUM,ZEROINVERT_CSUM&gt;\n    ether 4a:e2:04:eb:3b:e2\n    inet6 fe80::871:f226:16fb:5018%en0 prefixlen 64 secured scopeid 0xb \n    inet 10.34.11.15 netmask 0xffffe000 broadcast 10.34.31.255\n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: autoselect\n    status: active\nawdl0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=6460&lt;TSO4,TSO6,CHANNEL_IO,PARTIAL_CSUM,ZEROINVERT_CSUM&gt;\n    ether 7e:e4:55:0f:aa:48\n    inet6 fe80::7ce4:55ff:fe0f:aa48%awdl0 prefixlen 64 scopeid 0xd \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: autoselect\n    status: active\nllw0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\n    options=400&lt;CHANNEL_IO&gt;\n    ether 7e:e4:55:0f:aa:48\n    inet6 fe80::7ce4:55ff:fe0f:aa48%llw0 prefixlen 64 scopeid 0xe \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\n    media: autoselect (none)\nutun0: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1500\n    inet6 fe80::e4aa:ba08:b55a:fee%utun0 prefixlen 64 scopeid 0xf \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\nutun1: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1380\n    inet6 fe80::6ff9:4e20:6ac1:7bb7%utun1 prefixlen 64 scopeid 0x10 \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\nutun2: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 2000\n    inet6 fe80::c335:87af:5360:d8cd%utun2 prefixlen 64 scopeid 0x11 \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\nutun3: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1000\n    inet6 fe80::ce81:b1c:bd2c:69e%utun3 prefixlen 64 scopeid 0x12 \n    nd6 options=201&lt;PERFORMNUD,DAD&gt;\nutun4: flags=8051&lt;UP,POINTOPOINT,RUNNING,MULTICAST&gt; mtu 1500\n    inet 200.10.30.154 --&gt; 200.10.30.153 netmask 0xfffffffc\nsreedevirajavelu@iTrust-9000003511 ~ % \n</code></pre> <p>Excellent, detailed observation \u2014 and this is one of those subtle but really interesting networking questions where you\u2019re absolutely right to notice:</p> <p>\u201cWait, I don\u2019t see any 192.168.30.x interface on my host \u2014 so how am I even reaching 192.168.30.60?\u201d</p> <p>Let\u2019s unpack exactly why you can access the Ubuntu VM (192.168.30.60) and its PLC containers even though your host has no visible interface on that subnet.</p>"},{"location":"Bridged_Adapter/#the-key-is-the-remote-desktop-rdp-network-path","title":"\ud83e\udde9 The key is the Remote Desktop (RDP) network path","text":"<p>Here\u2019s the hierarchy you described:</p> <pre><code>[Your MacBook or Host Machine]  \u2190 (RDP connection) \u2192  [Remote Desktop: 192.168.30.5]\n       \u2502                                               \u2502\n       \u2502                                               \u2514\u2500\u2500 (Ubuntu VM: 192.168.30.60)\n       \u2502                                                        \u251c\u2500\u2500 (Docker macvlan: 192.168.40.0/24)\n       \u2502                                                        \u2514\u2500\u2500 iptables DNAT forwarding 8081\u20138084\n</code></pre>"},{"location":"Bridged_Adapter/#whats-actually-happening","title":"\ud83e\udde0 What\u2019s actually happening","text":"<p>You\u2019re not accessing 192.168.30.60 directly from your Mac\u2019s local interface. Instead, your RDP (Remote Desktop Protocol) session to <code>192.168.30.5</code> acts like a portal into that internal network (192.168.30.0/24).</p> <p>Your Mac likely has:</p> <ul> <li>An active VPN or corporate tunnel interface (e.g., <code>utun4</code> showing <code>200.10.30.154</code>)</li> <li>That VPN provides routing to internal subnets like <code>192.168.30.0/24</code>.</li> </ul> <p>So when you go to:</p> <pre><code>http://192.168.30.60:8081/dashboard\n</code></pre> <p>\u2192 the traffic goes through your VPN or RDP tunnel, into the Remote Desktop network, and reaches the Ubuntu VM.</p>"},{"location":"Bridged_Adapter/#why-you-dont-see-19216830x-in-ifconfig","title":"\ud83e\uddf1 Why you don\u2019t see 192.168.30.x in <code>ifconfig</code>","text":"<p>Your Mac only lists its local interfaces \u2014 those physically present or logically connected.</p> <p>But your connection to <code>192.168.30.60</code> doesn\u2019t require a local interface on that subnet because:</p> <ul> <li>There\u2019s a route (via VPN, RDP tunnel, or NAT) that sends traffic to that subnet indirectly.</li> <li>macOS knows: \u201cif I need to reach 192.168.30.60, send packets via my VPN interface (utun4).\u201d</li> </ul> <p>You can confirm this routing path using:</p> <pre><code>netstat -rn | grep 192.168.30\n</code></pre> <p>or</p> <pre><code>route get 192.168.30.60\n</code></pre> <p>\u2705 You\u2019ll likely see something like:</p> <pre><code>route to: 192.168.30.60\ndestination: 192.168.30.0\ngateway: 200.10.30.153\ninterface: utun4\n</code></pre> <p>That shows your packets go via <code>utun4</code> \u2014 your VPN/tunnel interface \u2014 not a local interface like <code>en0</code>.</p>"},{"location":"Bridged_Adapter/#how-the-access-chain-actually-works","title":"\ud83d\udd04 How the access chain actually works","text":"<p>Let\u2019s trace your connection when you visit <code>http://192.168.30.60:8081/dashboard</code>:</p> <ol> <li>Your browser on the host \u2192 sends TCP packets to 192.168.30.60:8081</li> <li>macOS routing \u2192 sees that 192.168.30.0/24 is reachable via <code>utun4</code> (VPN or remote tunnel)</li> <li>Traffic enters the remote LAN (the same network where your Remote Desktop 192.168.30.5 lives)</li> <li>VM\u2019s bridged adapter enp0s3 (192.168.30.60) receives the traffic</li> <li>iptables DNAT in the Ubuntu VM forwards:</li> </ol> <p><code>192.168.30.60:8081 \u2192 192.168.40.201:8080</code></p> <p>and so on for the other PLCs. 6. Docker macvlan interface passes it to the correct PLC container 7. Response travels back \u2192 through the same path \u2192 appears in your browser.</p> <p>So even though your host doesn\u2019t have a <code>192.168.30.x</code> interface, it can still reach it via a routed or tunneled network.</p>"},{"location":"Bridged_Adapter/#analogy","title":"\ud83e\udded Analogy","text":"<p>Think of it like this:</p> <p>You can visit a friend in another city without living there \u2014 because the highway (your VPN or RDP network) connects your location to theirs.</p> <p>Your host\u2019s local \u201caddress\u201d (10.x.x.x) doesn\u2019t matter \u2014 the route takes care of the connection.</p>"},{"location":"Bridged_Adapter/#summary_1","title":"\u2705 Summary","text":"Observation Explanation No 192.168.30.x interface on host Host doesn\u2019t need one \u2014 routing or VPN handles connectivity Can access <code>192.168.30.60:8081</code> Because traffic goes through VPN or RDP network that knows that subnet Ubuntu VM\u2019s <code>enp0s3</code> = 192.168.30.60 Bridged to the same LAN as the Remote Desktop Docker macvlan <code>192.168.40.x</code> Secondary subnet for PLC containers iptables DNAT Forwards ports 8081\u20138084 \u2192 specific PLC IPs Your host access works Due to routed connectivity through VPN or RDP\u2019s internal network <p>Would you like me to diagram this path visually \u2014 showing how the traffic travels from your Mac\u2019s interface (utun4) \u2192 Remote Desktop network (192.168.30.0/24) \u2192 Ubuntu VM \u2192 PLC container? It would help make this chain very intuitive.</p> <pre><code>Last login: Fri Oct 17 17:39:55 on ttys032\nsreedevirajavelu@iTrust-9000003511 ~ % netstat -rn | grep 192.168.30\n\n192.168.30         200.10.30.153      UGSc                utun4       \nsreedevirajavelu@iTrust-9000003511 ~ % \n</code></pre> <pre><code>sreedevirajavelu@iTrust-9000003511 ~ % route get 192.168.30.60\n\n   route to: 192.168.30.60\ndestination: 192.168.30.60\n    gateway: static-tu-200-10-30-153.superredes.net\n  interface: utun4\n      flags: &lt;UP,GATEWAY,HOST,DONE,WASCLONED,IFSCOPE,IFREF&gt;\n recvpipe  sendpipe  ssthresh  rtt,msec    rttvar  hopcount      mtu     expire\n       0         0         0         0         0         0      1500         0 \nsreedevirajavelu@iTrust-9000003511 ~ % \n</code></pre>"},{"location":"CommunicationBetweenInsideAndOutsideDocker/","title":"Communication Inside & Outside Docker","text":"<p>what does this mean and should I try connecting to this ip address if I have an sql database in a docker container on Macbook and I'm trying to connect to it from a python script inside an ubuntu vm in virtual box</p> <p>sreedevirajavelu@srees-MacBook-Air ~ % ipconfig getifaddr en0</p> <p>10.34.11.15 sreedevirajavelu@srees-MacBook-Air ~ % </p> <p>The IP address <code>10.34.11.15</code> is your MacBook\u2019s local network IP address on the <code>en0</code> interface (usually Wi-Fi).</p>"},{"location":"CommunicationBetweenInsideAndOutsideDocker/#what-this-means-for-your-setup","title":"What this means for your setup:","text":"<ul> <li>Your MacBook\u2019s Docker container MySQL is running on <code>localhost</code> inside your MacBook.</li> <li>Your Ubuntu VM in VirtualBox is a separate machine from the MacBook host.</li> <li>The VM needs to reach the Docker container through your MacBook\u2019s network IP (10.34.11.15) if networking between the VM and the Mac is set up properly.</li> </ul>"},{"location":"CommunicationBetweenInsideAndOutsideDocker/#can-you-connect-to-the-mysql-docker-container-on-ip-10341115-from-the-ubuntu-vm","title":"Can you connect to the MySQL Docker container on IP <code>10.34.11.15</code> from the Ubuntu VM?","text":"<p>It depends on the VirtualBox network mode:</p> <ol> <li> <p>NAT (default):</p> </li> <li> <p>The VM is behind a NAT and cannot directly access host IP like <code>10.34.11.15</code>.</p> </li> <li> <p>You would need to set up port forwarding in VirtualBox to forward the host port 3306 to the VM or use other tricks.</p> </li> <li> <p>Bridged Adapter:</p> </li> <li> <p>The VM gets an IP on the same LAN as your MacBook.</p> </li> <li> <p>VM can directly access <code>10.34.11.15</code> on port 3306 if firewall allows.</p> </li> <li> <p>Host-only Adapter:</p> </li> <li> <p>VM and host can communicate using a special virtual network IP (not the LAN IP).</p> </li> <li>You\u2019d check the IP of the host on that virtual network interface.</li> </ol>"},{"location":"CommunicationBetweenInsideAndOutsideDocker/#what-should-you-do","title":"What should you do?","text":"<ul> <li> <p>Check your VM\u2019s network mode in VirtualBox settings.</p> </li> <li> <p>If it\u2019s NAT, either:</p> </li> <li> <p>Switch to Bridged Adapter mode for the VM to be on the same LAN as the Mac, or</p> </li> <li> <p>Set up port forwarding in VirtualBox (host port 3306 \u2192 VM port 3306).</p> </li> <li> <p>Try to ping the MacBook\u2019s IP (<code>10.34.11.15</code>) from inside the Ubuntu VM:</p> </li> </ul> <pre><code>ping 10.34.11.15\n</code></pre> <p>If ping works and firewall allows, you can try to connect to MySQL at <code>10.34.11.15:3306</code> from your Python script running in the VM.</p>"},{"location":"CommunicationBetweenInsideAndOutsideDocker/#summary","title":"Summary","text":"Your Mac IP VM network type Access from VM to MySQL container? <code>10.34.11.15</code> NAT No direct access without port forwarding <code>10.34.11.15</code> Bridged Adapter Yes, direct access if firewall allows VirtualBox host-only IP Host-only Adapter Access via that network IP, not <code>10.34.11.15</code> <p>If you want, I can guide you how to set up VirtualBox network so the VM can access your Docker MySQL on the Mac!</p>"},{"location":"DockerBuild/","title":"Docker Build","text":""},{"location":"DockerBuild/#building-docker-images","title":"Building Docker images","text":"<p><code>docker build --network host --tag database .</code></p> <p>What each part does:</p> <ul> <li> <p><code>docker build</code>: command to build a Docker image</p> </li> <li> <p><code>--network host</code>: uses host's network stack during the build process (allows the build to access network resources as if it were running directly on the host machine)</p> </li> <li> <p><code>--tag database</code>: tags (names) the resulting image as \"database\"</p> </li> <li> <p><code>.</code>: uses current directory as the build context (where Docker looks for the Dockerfile and any files it needs to copy)</p> </li> <li> <p><code>--network host</code>: this flag is often used when the build process needs to download packages or access network resources that might be blocked by Docker's default networking.</p> </li> </ul>"},{"location":"DockerEngine_Ubuntu/","title":"Docker Engine on Ubuntu","text":"<p>Steps to install Docker Engine on ubuntu 24.04.2 :</p> <ul> <li>https://docs.docker.com/engine/install/ubuntu/</li> </ul> <p>Check ubuntu version using:</p> <ul> <li><code>lsb_release -a</code></li> <li><code>cat /etc/os-release</code></li> </ul> <p></p> <p>Check CPU architecture - <code>uname -m</code></p> <ul> <li>x86_64 \u2192 64-bit Intel/AMD (amd64) \u2705 supported</li> <li>aarch64 \u2192 64-bit ARM (arm64) \u2705 supported</li> <li>armv7l \u2192 32-bit ARM \u274c not supported by Docker Engine</li> <li>s390x, ppc64le \u2192 supported but only on special hardware</li> </ul> <p>On a VirtualBox VM on a MacBook, you\u2019ll almost certainly see x86_64 if your Mac is Intel-based, or aarch64 if your MacBook is Apple Silicon (M1/M2/M3).</p> <p></p> <p>Steps taken from https://docs.docker.com/engine/install/ubuntu/ </p> <p>Run the following command to uninstall all conflicting packages:</p> <pre><code>for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n</code></pre> <p>Install using the apt repository</p> <p>Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker apt repository. Afterward, you can install and update Docker from the repository.</p> <ol> <li>Set up Docker's apt repository.</li> </ol> <pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n</code></pre> <ol> <li>Install the Docker packages</li> </ol> <p>To install the latest version, run:</p> <pre><code>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>Note: The Docker service starts automatically after installation. To verify that Docker is running, use:</p> <p><code>sudo systemctl status docker</code></p> <p>Some systems may have this behavior disabled and will require a manual start:</p> <p><code>sudo systemctl start docker</code></p>"},{"location":"DockerEngine_Ubuntu/#issues-and-fixes","title":"Issues and Fixes","text":"<p><code>docker --version</code> works but </p> <p>for <code>docker ps</code> gives permission denied error meaning current user ubuntu does not have permission to access the Docker socket </p> <p>Solution: </p> <ol> <li>Check the socket permissions </li> </ol> <p><code>ls -l /var/run/docker.sock</code></p> <p>If you see something like: <code>srw-rw---- 1 root docker 0 Oct 10 13:20 /var/run/docker.sock</code></p> <p>That means only users in the docker group can use it.</p> <ol> <li>Add your user to the docker group</li> </ol> <p><code>sudo usermod -aG docker $USER</code></p> <p>Then log out and log back in - the new group membership only takes effect after re-login</p>"},{"location":"Docker_Compose/","title":"Docker Compose","text":""},{"location":"Docker_Compose/#difference-between-docker-compose-up-with-hyphen-and-docker-compose-up-d-no-hyphen-between-docker-and-compose","title":"Difference between docker-compose up (with hyphen) and docker compose up -d (no hyphen between docker and compose )","text":""},{"location":"Docker_Compose/#docker-compose-up-with-hyphen","title":"docker-compose up (with hyphen)","text":"<ul> <li>uses old Docker Compose (v1) which is a python based tool</li> <li>starts containers in the foreground, so you will see all logs from all containers in your terminal</li> <li>if you stop the terminal (Ctrl C) , the containers are also stopped unless you add the -d (detached)</li> <li>this command is still supported, but considered legacy - Docker now recommends the plugin-based syntax  </li> </ul>"},{"location":"Docker_Compose/#docker-compose-up-d-no-hyphen-between-docker-compose","title":"docker compose up -d (no hyphen between docker &amp; compose)","text":"<ul> <li>uses new Docker Compose (v2), which is now part of the docker CLI as plugin</li> <li>-d means detached mode:</li> <li>containers start in the background. </li> <li>you get your terminal back immediately</li> <li>logs are not streamed to terminal but you can view it with <code>docker compose logs -f</code></li> </ul>"},{"location":"Docker_Container_Interfaces/","title":"Docker Container Interfaces","text":""},{"location":"Docker_Container_Interfaces/#docker-container-interfaces","title":"Docker Container Interfaces","text":"<ul> <li><code>ip a</code> inside the docker container of an IED after <code>docker exec -it ds1cb1 bash</code>:</li> </ul> <p>Different use cases of the 2 network interfaces:</p>"},{"location":"Docker_Container_Interfaces/#1-eth0-1001624","title":"1. <code>eth0 : 10.0.1.6/24</code>","text":"<p>Default interface created when the container is attached to a Docker network (bridge, overlay, or custom)</p> <p>Use case:</p> <ul> <li>Used for container-to-container communication inside the same Docker network</li> <li>Other containers on the same network (subnet <code>10.0.1.0/24</code>) can reach this container via <code>10.0.1.6</code></li> <li>Not directly routable from the host machine unless you expose ports (via <code>-p host_port:container_port</code>)</li> </ul> <p>Example:</p> <ul> <li>If you have another container on the same Docker network, it can <code>ping 10.0.1.6</code> or connect directly to its services.</li> </ul>"},{"location":"Docker_Container_Interfaces/#2-eth1-17224101124","title":"2. <code>eth1 : 172.24.10.11/24</code>","text":"<p>A second interface \u2014 likely because you attached this container to another Docker network (such as with <code>--network</code> flag or via Docker Compose multiple networks)</p> <p>Use case:</p> <ul> <li>Provides connectivity to containers/services on the <code>172.24.10.0/24</code> subnet</li> <li>Might be used for external integration (such as simulating a \"backend network\" while <code>eth0</code> handles a \"frontend network\")</li> <li>Useful in multi-network setups, where a container acts as a bridge between 2 networks</li> </ul> <p>Example:</p> <ul> <li>If you are running a database container on the <code>172.24.10.0/24</code> network, this container can talk to it via <code>172.24.10.x</code></li> </ul>"},{"location":"Docker_Container_Interfaces/#which-ip-can-you-use-from-the-host-machine","title":"Which IP can you use from the host machine?","text":"<ul> <li>Normally, you don't use these internal container IPs from the host.</li> <li>Instead:</li> <li>Use localhost (127.0.0.1) on the host with port mappings (-p 8080:80)</li> <li>If using macvlan/host networks, the container may be directly reachable on one of these IPs (<code>10.0.1.6</code> or <code>172.24.10.11</code>) from the host or even other machines on the LAN.</li> </ul>"},{"location":"Docker_Container_Interfaces/#summary","title":"Summary:","text":"<ul> <li><code>eth0 (10.0.1.6)</code> : main Docker network, container-to-container communication.</li> <li><code>eth1 (172.24.10.0)</code>: secondary Docker network, used if container needs to talk to another isolated group of services.</li> <li>From the host, you usually connect via <code>localhost</code>+ published ports, unless you intentionally configured a special network (macvlan, host, overlay).</li> </ul> <p>To check if these IPs are reachable directly from host or internal-only :</p> <p>...to be continued...</p>"},{"location":"Docker_Macvlan_Network/","title":"Docker Macvlan Network","text":""},{"location":"Docker_Macvlan_Network/#docker-macvlan-network","title":"Docker Macvlan Network","text":"<p>A macvlan nerwork allows each Docker container to appear on the same Layer 2 network as your host (Ubuntu VM).</p> <ul> <li> <p>Each container gets its own unique IP address on your physical LAN (e.g. 192.168.30.x)</p> </li> <li> <p>The containers can be reached directly by other devices on that LAN - without NAT.</p> </li> </ul> <p>Because of that, the macvlan network must be bound to your host's physical network interface (the one actually connected to your LAN or VM bridge).</p>"},{"location":"Docker_Macvlan_Network/#2-choosing-the-correct-interface-to-create-mavvlan-network","title":"\ud83e\udde9 2\ufe0f\u20e3 Choosing the correct interface to create Mavvlan network","text":"<p>ip a output</p> <pre><code>ubuntu@pgt:~$ ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 08:00:27:62:7e:2a brd ff:ff:ff:ff:ff:ff\n    inet 192.168.30.60/24 brd 192.168.30.255 scope global enp0s3\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fe62:7e2a/64 scope link \n       valid_lft forever preferred_lft forever\n3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 52:18:4f:1d:0b:aa brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5018:4fff:fe1d:baa/64 scope link \n       valid_lft forever preferred_lft forever\nubuntu@pgt:~$ \n</code></pre> <p>From your ip a output:</p> <p>2: enp0s3:  <pre><code>inet 192.168.30.60/24\n</code></pre> <p>\u2705 enp0s3 is UP, has a valid IPv4 (192.168.30.60/24), and is your main interface connected to the external network.</p> <p>That makes enp0s3 the correct interface to use for a macvlan network.</p> <p>The others:</p> <p>lo \u2014 loopback (ignore)</p> <p>docker0 \u2014 Docker\u2019s internal bridge (not physical, so not usable for macvlan)</p> <p>So yes, you should use enp0s3 for your macvlan setup.</p> <p></p> <p>There are three private IPv4 address ranges defined by RFC 1918. These are reserved for internal networks only (they are not routable on the public Internet).</p> Private Range CIDR Typical Use <code>10.0.0.0</code> \u2013 <code>10.255.255.255</code> <code>/8</code> Large enterprise LANs <code>172.16.0.0</code> \u2013 <code>172.31.255.255</code> <code>/12</code> Corporate or internal subnets <code>192.168.0.0</code> \u2013 <code>192.168.255.255</code> <code>/16</code> Home, office, and VM networks <p>ubuntu@pgt:~$ ip route</p> <p>default via 192.168.30.1 dev enp0s3 proto static </p> <p>172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown </p> <p>192.168.30.0/24 dev enp0s3 proto kernel scope link src 192.168.30.60 </p> <p>ubuntu@pgt:~$ </p> <p>Create Docker macvlan network </p> <p><code>docker network create -d macvlan --subnet=192.168.30.0/24 --gateway=192.168.30.1 -o parent=enp0s3 plc-macvlan</code></p> <p>Persist Docker macvlan network:</p>"},{"location":"Docker_Macvlan_Network/#accessing-docker-macvlan-network-from-host-machine","title":"Accessing docker macvlan network from host machine:","text":"<ol> <li>How Docker macvlan works</li> </ol> <p>When you create a Docker network using the macvlan driver, Docker gives each container its own virtual MAC address and attaches it directly to the physical network via the VM\u2019s NIC (e.g., enp0s3).</p> <p>Each container looks like a separate device on the LAN \u2014 it can communicate directly with:</p> <p>Other containers on the same macvlan network</p> <p>Other physical machines on the same LAN</p> <p>But here\u2019s the catch \ud83d\udc47</p> <p>The host interface (the Ubuntu VM itself) is not automatically connected to this macvlan network.</p> <p>This is by design \u2014 Linux macvlan interfaces cannot talk to their parent interface (and vice versa).</p> <p>So the VM\u2019s main NIC (enp0s3) cannot directly send or receive packets to/from containers on the macvlan network.</p>"},{"location":"Docker_Macvlan_Network/#question","title":"Question:","text":"<p>How do I create and persist a linux virtual network interface on the ubuntu VM so that I can access the docker containers which I create on the docker macvlan network inside the ubuntu vm on the host, these are the details of how I created the docker macvlan network and the docker container on the ubuntu vm create docker macvlan network on ubuntu vm (which I have sshed into):</p> <ul> <li> <p><code>docker network create -d macvlan --subnet=192.168.30.0/24 --gateway=192.168.30.1 -o parent=enp0s3 plc-macvlan</code></p> </li> <li> <p>create and run the docker container for plc in ubuntu vm : </p> </li> </ul> <p><code>docker run -d --name st_plc--net plc-macvlan --ip 192.168.30.201 --privileged openplc:v3</code></p> <p>I want to access this website on my host machine : http://192.168.30.201:8080/login</p> <p>Perfect \u2014 you\u2019ve explained this clearly, and your goal is exactly right \ud83d\udc4f</p> <p>You already have:</p> <p>The Ubuntu VM running with IP 192.168.30.60 on interface enp0s3</p> <p>A Docker macvlan network (plc-macvlan) attached to that interface</p> <p>A PLC container with IP 192.168.30.201 on that macvlan</p> <p>Now you want to access that container\u2019s web UI (OpenPLC) from your host machine (e.g., your physical computer running VirtualBox, VMware, or similar).</p> <p>Let\u2019s go step-by-step and make the setup both functional and persistent.</p> <p>\ud83e\udde0 Why This Is Needed As you already know:</p> <p>A macvlan network gives containers their own MAC addresses and puts them directly on the LAN.</p> <p>But the Ubuntu VM itself (the host of the containers) can\u2019t communicate with them, because Linux doesn\u2019t allow the parent interface (enp0s3) to talk to its macvlan children.</p> <p>Step 1: Create the macvlan interface (temporary test)</p> <p>Run these inside your Ubuntu VM (SSH session):</p> <pre><code># Create the macvlan0 interface attached to enp0s3\nsudo ip link add macvlan0 link enp0s3 type macvlan mode bridge\n\n# Assign a free IP in the same subnet (different from 192.168.30.60 and 192.168.30.1)\nsudo ip addr add 192.168.30.254/24 dev macvlan0\n\n# Bring it up\nsudo ip link set macvlan0 up\n</code></pre> <p>Context: You are running a Docker container inside an Ubuntu VM, and that container is attached to a macvlan network (plc-macvlan). By design, a macvlan network isolates container traffic - the host (your VM) cannot communicate with containers on that macvlan unless you explicitly add a macvlan on the host side too.  </p> <p>So the 3 commands above make the VM itself a part of the same network as the container.</p> <ol> <li> <p><code>sudo ip link add macvlan0 link enp0s3 type macvlan mode bridge</code></p> </li> <li> <p>creates a new virtual network interface called <code>macvlan0</code></p> </li> <li> <p>this interface is attached to your physical network interface <code>enp0s3</code> (the one the VM uses to access your LAN)</p> </li> <li> <p>the type macvlan creates a separate interface with its own MAC address </p> </li> <li> <p><code>mode bridge</code> means that traffic on <code>enp0s3</code> will be bridged with <code>macvlan0</code> - both can send/receive packets in the same Layer 2 domain.</p> </li> </ol> <p>Why is it needed:</p> <ul> <li> <p>Docker's <code>macvlan</code> driver puts containers in the same L2 domain but isolates them from the host.</p> </li> <li> <p>By creating <code>macvlan0</code>, your VM becomes reachable to and from containers in that macvlan network.</p> </li> <li> <p><code>sudo ip addr add 192.168.30.254/24 dev macvlan0</code></p> </li> <li> <p>Assigns an IP address (192.168.30.254) to the new <code>macvlan0</code> interface.</p> </li> <li> <p>The <code>/24</code> subnet mask means it is part of the same 192.168.30.x network as your VM (<code>192.168.30.254</code>) and containers (like <code>192.168.30.201</code>).</p> </li> <li> <p>it acts as the gateway for your VM to communicate with containers</p> </li> <li> <p>without an IP, the VM can't send packets directly through macvlan0</p> </li> <li> <p><code>sudo ip link set macvlan0 up</code></p> </li> <li> <p>brings the macvlan0 interface online (equivalent to 'turning it on')</p> </li> <li> <p>by default, new interfaces are down</p> </li> <li> <p>this command activates it so it can actually pass network traffic</p> </li> </ul> <p>After these steps: </p> <p>VM can now <code>ping</code> and access containers on the plc-macvlan network e.g. 192.168.30.201</p> <p><code>ping -I macvlan0 192.168.30.201</code></p> <p>To persist the macvlan interface created above:</p> <p>By default, <code>ip link add</code> changes are temporary and vanish after reboot.</p> <p>To persist it across reboots, choose one of these methods:</p> <p>Option 1: create a startup script + systemd service </p> <p>Step 1:</p> <ol> <li>create the script:</li> </ol> <p><code>sudo nano /usr/local/sbin/setup-macvlan.sh</code></p> <p>Paste:</p> <pre><code>#!/bin/bash\nip link add macvlan0 link enp0s3 type macvlan mode bridge\nip addr add 192.168.30.254/24 dev macvlan0\nip link set macvlan0 up\n</code></pre> <ol> <li>Make it executable:</li> </ol> <p><code>sudo chmod +x /usr/local/sbin/setup-macvlan.sh</code></p> <p>Step 2:</p> <ol> <li>Create the systemd service file</li> </ol> <p><code>sudo nano /etc/systemd/system/setup-macvlan.service</code></p> <ol> <li>Paste this:</li> </ol> <pre><code>[Unit]\nDescription=Setup macvlan0 network interface\nAfter=network.target\n\n[Service]\nType=oneshot\nExecStart=/usr/local/sbin/setup-macvlan.sh\nRemainAfterExit=yes\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Step 3: Reload and enable the service</p> <p><code>sudo systemctl daemon-reload</code></p> <p>Enable it to run automatically at reboot</p> <p><code>sudo systemctl enable setup-macvlan.service</code></p> <p>Test it immediately without rebooting:</p> <p><code>sudo systemctl start setup-macvlan.service</code></p> <p>Step 4: verify it worked </p> <p>Check if the interface is up: <code>ip addr show macvlan0</code></p> <p>And verify service status: <code>sudo systemctl status setup-macvlan.service</code></p>"},{"location":"Docker_Macvlan_Network/#to-make-docker-container-reachable-from-vms-default-interface","title":"To make docker container reachable from VM's default interface :","text":"<pre><code>sudo ip link add macvlan-host link enp0s3 type macvlan mode bridge\nsudo ip addr add 192.168.30.250/24 dev macvlan-host\nsudo ip link set macvlan-host up\n</code></pre> <p>1\ufe0f\u20e3 Container is running and reachable from the VM From your last command: ping -I macvlan0 192.168.30.201 You get: 64 bytes from 192.168.30.201 ... \u2705 This means inside the VM, the container is reachable via the macvlan0 interface. The container is up and responding to pings from the VM itself. 2\ufe0f\u20e3 Container is not reachable from the VM\u2019s default interface When you just run: ping 192.168.30.201 You get: Destination Host Unreachable This happens because Docker macvlan creates an isolated network, and the host VM\u2019s default interface (enp0s3) cannot talk to the macvlan interface directly. This is a known limitation of macvlan: A host cannot communicate with a container on a macvlan network by default. 3\ufe0f\u20e3 How to reach the container from the host machine (your physical machine or host VM) Since your VM is attached as Bridged Adapter, the host machine is on the same subnet (192.168.30.0/24) as your container. To reach the container: Option A: Add a macvlan sub-interface on the host machine You need to create a macvlan interface on your host machine, attached to the same physical network as the VM. Give it an IP in the same subnet (but not conflicting with gateway, VM, or container IPs). Example on host: sudo ip link add macvlan-host link  type macvlan mode bridge sudo ip addr add 192.168.30.250/24 dev macvlan-host sudo ip link set macvlan-host up Then ping: ping 192.168.30.201 <p>TROUBLESHOOTING :</p> <pre><code>ping -I macvlan0 192.168.30.201   # works\nping 192.168.30.201               # fails\n</code></pre> <p>1\ufe0f\u20e3 What\u2019s wrong currently From docker inspect plc-macvlan: \"Subnet\": \"192.168.30.0/24\", \"Gateway\": \"192.168.30.1\", \"Options\": { \"parent\": \"enp0s3\" } Your macvlan network for containers is on the same subnet as your VM\u2019s default interface (192.168.30.0/24). Your VM\u2019s IP is 192.168.30.60. Your containers have IPs like 192.168.30.201. Linux cannot send packets from the parent interface (enp0s3) to macvlan child interfaces on the same subnet. This is a known limitation of macvlan: Host \u2194 Macvlan containers on the same L2 subnet is blocked unless you use a separate \u201cmacvlan-host\u201d interface with a different subnet or routing.</p> <p>2\ufe0f\u20e3 What you need to change Solution: put your macvlan containers on a different subnet, e.g., 192.168.40.0/24. Then: Use a \u201chost macvlan interface\u201d (macvlan-host) on the VM in the same subnet, e.g., 192.168.40.254. Enable IP forwarding + iptables NAT to allow the VM and host to reach the container network.</p> <p>I will have 4 different docker containers for 4 plcs on the ubuntu vm hence using the same url and the same ssh method of accessing every plc will not make sense, how can I use the unique ip address of each plc container to access the browser</p> <p>http://localhost:8080/dashboard</p> <p>ssh -L 8080:192.168.40.201:8080 ubuntu@192.168.30.60</p> <p>Perfect \u2014 that\u2019s exactly the right way to think about this. Since you\u2019ll be running 4 PLC containers, each with a unique IP (e.g., 192.168.40.201, 192.168.40.202, etc.) on the macvlan network, you want to be able to access each PLC\u2019s web interface directly by its IP instead of tunneling everything to localhost:8080. Let\u2019s go step by step \ud83d\udc47 \ud83e\udde0 Why the problem happens The reason you can\u2019t currently access 192.168.40.x IPs from your Remote Desktop host (or from your own PC) is that macvlan networks isolate traffic from the parent host interface. That means: The Ubuntu VM can talk to the containers \u2705 But the Remote Desktop host (the VM\u2019s hypervisor host) cannot \u274c Nor can your own machine (accessing via Remote Desktop or SSH) \u274c So, even though the containers have unique IPs, those IPs exist only inside the VM\u2019s internal macvlan network.</p> <p>\ud83e\udde9 To fix this \u2014 you have two design options: Option 1: Add a second adapter (NAT or Host-only) + iptables forwarding This approach lets you access each PLC by its unique IP directly \u2014 without using SSH tunneling or localhost. Step 1: Add another network adapter to your Ubuntu VM In your VM settings (in VirtualBox, VMware, etc.): Keep the Bridged Adapter (this is your current enp0s3). Add a second adapter: Type: NAT (or Host-only) Name: automatically created, e.g. enp0s8 in Ubuntu. This will give your VM a second IP (say, 10.0.2.15 for NAT or 192.168.56.101 for host-only). Step 2: Enable IP forwarding Inside the Ubuntu VM: sudo sysctl -w net.ipv4.ip_forward=1</p> <p>To persist it: echo \"net.ipv4.ip_forward=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Step 3: Forward each PLC port range Let\u2019s say you have these PLC containers: PLC Container IP    Web Port    External Port PLC1    192.168.40.201  8080    8081 PLC2    192.168.40.202  8080    8082 PLC3    192.168.40.203  8080    8083 PLC4    192.168.40.204  8080    8084</p> <p>Add forwarding rules: sudo iptables -t nat -A PREROUTING -p tcp -d  --dport 8081 -j DNAT --to-destination 192.168.40.201:8080 sudo iptables -t nat -A PREROUTING -p tcp -d  --dport 8082 -j DNAT --to-destination 192.168.40.202:8080 sudo iptables -t nat -A PREROUTING -p tcp -d  --dport 8083 -j DNAT --to-destination 192.168.40.203:8080 sudo iptables -t nat -A PREROUTING -p tcp -d  --dport 8084 -j DNAT --to-destination 192.168.40.204:8080 <p>sudo iptables -t nat -A POSTROUTING -j MASQUERADE Replace  with the Ubuntu VM\u2019s default bridged IP (the one reachable from your Remote Desktop). Step 4: Access from Remote Desktop or your own PC Now you can open, from Remote Desktop or your own machine: http://:8081/login \u2192 PLC1 http://:8082/login \u2192 PLC2 http://:8083/login \u2192 PLC3 http://:8084/login \u2192 PLC4 \u2705 Each one maps uniquely to the right container. To persist the iptables rules: sudo apt install iptables-persistent sudo netfilter-persistent save Option 2: Bridge the macvlan directly to the Remote Desktop network This is more complex but allows direct IP access (e.g., you could ping 192.168.40.201 directly from your Remote Desktop host). However: You must ensure the Remote Desktop\u2019s LAN supports the 192.168.40.0/24 subnet. The Remote Desktop\u2019s physical adapter must allow macvlan bridge mode. If multiple VMs use the same network, you may have IP conflicts. If your VM is on a corporate or managed network, this method is not recommended \u2014 Option 1 (NAT + iptables) is safer and works everywhere. Summary Goal    Recommended Solution Access PLC browser from Remote Desktop  Use iptables port forwarding from VM IP to container IP Access PLC browser from your own host PC    Use same forwarding \u2014 just access VM\u2019s bridged IP Keep separate PLC dashboards    Assign unique external ports (8081\u20138084) Keep setup persistent   Use iptables-persistent or a startup script Would you like me to give you the exact command sequence and a ready-to-use startup script that automatically sets up this forwarding for all 4 PLCs whenever the VM boots?"},{"location":"Docker_Networks/","title":"Docker Networking","text":""},{"location":"Docker_Networks/#docker-networking-tutorial-bridge-none-host-ipvlan-macvlan-overlay","title":"Docker Networking Tutorial (Bridge - None - Host - IPvlan - Macvlan - Overlay)","text":"<p>Reference: https://youtu.be/fBRgw5dyBd4?si=LodjJDx4AdnjYOS6</p>"},{"location":"Docker_Networks/#docker-bridge","title":"Docker Bridge","text":"<ul> <li>2 segments that the bridge connects is:</li> <li>local host e.g. your laptop, or linux server where you run your Docker containers</li> <li>virtual network created by Docker</li> <li></li> </ul>"},{"location":"Docker_Networks/#when-you-install-docker-for-the-first-time-it-will-create-a-default-bridge-network-on-the-host-slightly-different-on-macos-as-it-will-run-al-containers-inside-a-linux-vm-virtualization-framework","title":"When you install Docker for the first time, it will create a default bridge network on the host (slightly different on MacOS as it will run al containers inside a Linux VM, virtualization framework)","text":"<ul> <li>all containers you create on that host will get an IP address on that range</li> </ul> <p>On a Linux host, Docker Engine directly creates a docker0 bridge network interface in the host\u2019s network stack. That\u2019s the familiar default bridge you see with ifconfig or ip a (usually 172.17.0.1/16). </p> <p>On a Mac (macOS) or Windows, things are different:</p> <p>Docker Desktop does not create a native docker0 bridge interface on your MacBook\u2019s network stack.</p> <p>Instead, Docker Desktop runs all containers inside a lightweight Linux VM (on macOS, this is managed using Apple\u2019s HyperKit or now a Virtualization Framework).</p> <p>Inside that VM, Docker does create a docker0 bridge (just like on Linux). But this bridge is inside the VM, not on macOS itself.</p> <p>From your MacBook\u2019s perspective, you won\u2019t see docker0 if you run ifconfig or ip a. Instead, Docker Desktop sets up a special networking layer that forwards traffic between your MacBook and the VM.</p> <p>So to answer directly:</p> <p>\ud83d\udc49 No, Docker Desktop does not create a default bridge network on your MacBook\u2019s host network stack. \ud83d\udc49 Yes, it does create a default bridge network inside the Linux VM where your containers run.</p> <p>If you run: <code>docker network ls</code> you\u2019ll still see bridge, host, and none listed, because those networks exist in the Docker VM environment, even though your Mac itself doesn\u2019t expose them as interfaces.</p> <p>Do you want me to also explain how you can check the actual IP range of the bridge network inside Docker Desktop on your Mac?</p>"},{"location":"Docker_Networks/#containers-created-in-the-default-bridge-network","title":"Containers created in the default bridge network","text":"<ul> <li>can communicate with each other</li> <li>default bridge network has restrictions and is not recommended for production, better to use user-defined bridge network.</li> <li>Default bridge network: Cannot use container DNS from host </li> <li>In default bridge network, DNS is not supported. Also cannot use DNS to send requests between containers.</li> </ul>"},{"location":"Docker_Networks/#user-defined-bridge-network","title":"User defined bridge network**:","text":"<ul> <li>Can use DNS to send requests to containers. </li> <li>This only applies to communications inside the bridge network between containers. You still will NOT be able to use the container DNS from the host.</li> </ul>"},{"location":"Docker_Networks/#host-mode-networking-option","title":"Host mode - networking option","text":"<ul> <li>container will not get its own IP and instead share the same networking namespace as the host where you run the container</li> <li>will appear as if you were running a regular application on that host</li> <li>thus any application running on a different server will be able to access the container using the host's IP address</li> </ul>"},{"location":"Docker_Networks/#ipvlan-network","title":"IPvlan network:","text":"<ul> <li>Traditionally, to expose a container to the outside world we used bridge network. But it adds additional complexity and performance penalty. Packet needs to go through additional hop, need to map ports from the container to the host to expose it to other applications.  </li> <li>Does not use a bridge for isolation and is associated directly with the Linux network interface. No need for port mappings in these scenarios.</li> </ul>"},{"location":"Docker_Networks/#macvlan-network","title":"MacVLAN network:","text":"<ul> <li>Some legacy applications and those that monitor network traffic expect to be directly connected to the physical network.</li> <li>In this case, can use the MacVLAN network driver to assign a MAC address to each container's virtual network interface.</li> <li>It will appear as a physical network interface directly connected to the physical network.</li> <li>to create it, must specify the subnet that the host uses, the gateway, and the parent network interface</li> <li>compare the parent MAC address (parent interface) -&gt; e.g. ip addr show ens33</li> <li>MAC address of the container -&gt; ssh into it and do <code>ip addr</code></li> </ul>"},{"location":"Docker_Networks/#difference-between-ipvlan-and-macvlan","title":"Difference between IPVLAN and MACVLAN:","text":"<ul> <li>if use IPVLAN: container will get the same MAC address as your host</li> <li>if use MACVLAN: container will have a different MAC address from host</li> </ul>"},{"location":"Docker_Networks/#overlay-network","title":"Overlay network","text":"<ul> <li>When you deploy your applications to production, you will need more than one physical or virtual server, each with a Docker daemon installed.</li> <li>The overlay network driver creates a distributed network among multiple Docker daemon hosts.</li> <li>This network sits on top of (overlays) the host-specific networks, allowing containers connected to it to communicate securely, especially when encryption is enabled.</li> <li>Most frequently, this type of network is used with Docker Swarm , but it is also possible to connect individual containers.</li> <li>Comment from youtuber: to manage containers at scale, especially in production, consider using Kubernetes.</li> <li>need to disable</li> <li>Example of usage of overlay network:</li> <li><code>2 ubuntu VMs</code></li> <li>find network interfaces on both VMs, e.g. ens33 on both VMs -&gt; disable this on the network interface on both VMs</li> <li><code>sudo ethtool -K ens33 tx-checksum-ip-generic off</code> (not persistent across restarts so might need to use something like a script using systemd service to automatically run it on boot)</li> <li>Even if you want to connect individual containers to overlay network, we still need to initialize the Docker Swarm</li> <li>on the first VM, run <code>docker swarm init</code></li> <li>will give a command you can execute on other VMs to join the Docker Swarm</li> <li>each VM must have docker installed</li> <li>manager and worker nodes (the VMs)</li> <li>on manager, create an overlay network and an attachable flag for individual containers to be able to use this network. Otherwise only swarm services will be able to use it.</li> <li>on the worker node (VM) if you do <code>docker network ls</code>, you will only see the overlay network when you start a container that uses the network (can start on the manager node)</li> <li>to verify that we can access containers on that overlay network deployed on different VMs, can SSH to the second container and use curl to send a request to the first container on the manager node</li> <li></li> </ul>"},{"location":"Docker_Networks/#overlay-networks","title":"Overlay Networks","text":"<ul> <li>Allow containers running on different Docker hosts (machines) to communicate with each other as if they are on the same local network, while keeping the communcation secure and isolated.</li> </ul> <p>Why normal Docker networks is not enough? - By default, Docker creates a bridge network on a single host (like bridge0 on Linux) - Containers connected to this bridge can talk to each other, but only on the same host.</p> <p>If you want:  - multiple physical/virtual servers - running containers that need to talk to each other - Need something that span across hosts</p>"},{"location":"Docker_Networks/#how-overlay-networks-work","title":"How overlay networks work?","text":"<ul> <li>Docker creates a virtual distributed network that spans all participating nodes (hosts).</li> <li>uses VXLAN tunneling (encapsulating packets inside UDP) so that container-to-container traffic can move across hosts transparently.</li> <li>network is managed by Docker swarm or Docker engine in swarm mode</li> </ul> <p>Example: - Host A has a container running web. - Host B has a container running db. - Both containers join the same overlay network called backend-net. - Now web can talk to db using the container name (db:3306), even though they are on different machines.</p>"},{"location":"Docker_Volumes/","title":"Docker Volumes","text":""},{"location":"Docker_Volumes/#what-is-a-volume-in-docker","title":"What is a Volume in Docker","text":"<p>Think of Docker container as a small isolated computer - when you stop or delete it, everything inside disappears (its 'disk' is temporary).</p> <p>A Docker volume is like pluggin in an external hard drive or shared folder to that container.</p> <p>It lets you:</p> <ol> <li>Keep data even after container is stopped or removed.</li> <li>Share files between real machine (the host) and the container.</li> </ol> <p>There are 2 main types of volume mounts:</p> <ul> <li>Bind mount (links a folder from your host machine to the container)</li> <li>Named volume (managed entirely by Docker - stored somewhere under <code>/var/lib/docker/volumes</code>)</li> </ul> <p>Explanation for use of volumes:</p> <ul> <li>Left side (<code>./sql</code>) -&gt; a folder on your computer at the same level as your docker-compose.yml file</li> <li>Right side (<code>/docker-entrypoint-initdb.d</code>) -&gt; a special folder inside the container that MySQL checks when it first starts.</li> </ul> <p>When MySQL starts for the first time, it looks in <code>/docker-entrypoint-initdb.d</code> and automatically runs any <code>.sql</code> files it finds there - this is how it initializes your database.</p> <p>When Docker starts the MySQL container, it copies that folder into the container.</p> <p>MySQL executes your CCP_data.sql automatically - creating the tables, inserting initial data, etc.</p> <p>This mount does not store your database data (the rows &amp; tables you later insert). It only gives MySQL initialization scripts.</p> <p>Once MySQL is running, the database data is stored inside the container's temporary filesystem. If you delete the container, the data is lost.</p> <p>This is called a bind mount (it directly links your folder <code>./sql</code> from the host to the continer).</p> <p>Question:  does the special folder docker-entrypoint-initdb.d exist in every docker container ? if not where does it come from ?</p> <p>No, the folder /docker-entrypoint-initdb.d does not exist in every Docker container. It exists only in certain official database images (like MySQL, PostgreSQL, MongoDB, etc.) because the image creator chose to include it in the image\u2019s startup logic. </p> <p>Each image defines its own conventions about where to put configuration or initialization files.</p> <p>So when you use:</p> <pre><code>    volumes:\n      - ./sql:/docker-entrypoint-initdb.d\n</code></pre> <p>you are not telling Docker anything special - you are telling the MySQL container: When you run your startup script, here is the folder where you can find initialization files.</p>"},{"location":"Docker_Volumes/#content-of-docker-composeyml-file-to-use-postgresql-database-as-docker-container","title":"Content of docker-compose.yml file to use postgresql database as docker container","text":"<p>This version does not persist volume and data will vanish if you remove the container.</p> <pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: ccp-multi-ied-multi-plc\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: powerplant\n    ports:\n      - \"5432:5432\"\n    volumes:\n    - ./postgres-sql:/docker-entrypoint-initdb.d\n\n</code></pre> <p>Use of  - volumes: - ./sql:/docker-entrypoint-initdb.d</p> <ul> <li> <p>This mounts your host directory ./sql into the container\u2019s /docker-entrypoint-initdb.d directory.</p> </li> <li> <p>docker-entrypoint-initdb.d is a special folder: on first startup of the container, MySQL runs all .sql or .sh scripts there to initialize the DB.</p> </li> <li> <p>After initialization, new runs don\u2019t automatically re-run scripts unless you remove the container and volume.</p> </li> </ul> <p>\u2757 This is not where live database data is stored. Live data is in /var/lib/mysql. Since you\u2019re not mounting that, the data lives in a Docker-managed anonymous volume.</p> <p>So: Changing this bind mount will not remove your MySQL data. It only affects which init scripts would run on container creation.</p>"},{"location":"Docker_Volumes/#content-of-docker-composeyml-file-to-use-postgresql-database-as-docker-container-with-persistent-volume","title":"Content of docker-compose.yml file to use postgresql database as docker container (with persistent volume)","text":"<pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: ccp-multi-ied-multi-plc-pg\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: powerplant\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data   # &lt;-- persistent volume for DB data\n      - ./postgres-sql:/docker-entrypoint-initdb.d  # &lt;-- init scripts\n\nvolumes:\n  postgres_data:   # &lt;-- named volume definition\n</code></pre>"},{"location":"Docker_Volumes/#use-of-volumes-inside-postgresql-docker-composeyml","title":"Use of Volumes inside PostgreSQL docker-compose.yml","text":"<pre><code>volumes:\n- postgres_data:/var/lib/postgresql/data # persistent volume for DB data\n- ./postgres-sql:/docker-entrypoint-initdb.d # init scripts\n\nvolumes:\n  postgres_data: # named volume definition \n</code></pre> <p>Explanation of each line:</p> <ol> <li> <p><code>-postgres_data:/var/lib/postgresql/data</code></p> </li> <li> <p>left side (postgres_data) -&gt; a named volume, managed by Docker</p> </li> <li> <p>you don't see it as a folder on your host - Docker stores it internally (under /var/lib/docker/volumes/...)</p> </li> <li> <p>right side (/var/lib/postgresql/data) -&gt; the main folder inside the container where the PostgreSQL stores all its databases.</p> </li> </ol> <p>This means your PostgreSQL data (tables, records, etc.) is stored in the <code>postgres_data</code> volume not inside the temporary cotainer filesystem.</p> <p>Even if you:</p> <ul> <li> <p>stop the container (docker compose down)</p> </li> <li> <p>rebuild it</p> </li> <li> <p>restart it</p> </li> </ul> <p>the data remains intact, because the volume is persistent.</p> <p>If you ever want to delete the data, you explicitly run: <code>docker compose down -v</code></p> <ol> <li> <p><code>- postgres-sql:/docker-entrypoint-initdb.d</code></p> </li> <li> <p>it mounts your host folder <code>postgres-sql/</code> (next to your <code>docker-compose.yml</code>) into the container's <code>/docker-entrypoint-initdb.d</code> folder</p> </li> <li> <p>PostgreSQL automatically runs any <code>.sql</code> files there the first time the database is initialized (before any data exists).</p> </li> </ol> <p>So if you have <code>postgres-sql/CCP_data.sql</code>, it will create your tables and insert the initial values when the container is started for the first time.</p> <ol> <li>section at the bottom:</li> </ol> <pre><code>volumes:\n  postgres_data:\n</code></pre> <p>This declares the named volume used above, so Docker knows it should create and manage it.</p>"},{"location":"Docker_tgz/","title":"Using tarballs (.tgz)","text":""},{"location":"Docker_tgz/#what-is-a-docker-tgz-file","title":"What is a Docker .tgz file ?","text":"<p>In the Docker context, a <code>.tgz</code> file is a compressed archive of a Docker image created with:</p> <pre><code>  docker save -o image.tar &lt;image:tag&gt;\n\n  # then gzip it\n\n  tar czf image.tgz image.tar\n</code></pre> <p>Or directly:</p> <p><code>docker save &lt;image:tag&gt; | gzip &gt; image.tgz</code></p> <p>Inside the <code>.tgz</code>: - it is basically a tarball of JSON metadata and filesystem layer tarballs</p>"},{"location":"Docker_tgz/#to-make-modifications-to-docker-tgz-file","title":"To make modifications to Docker .tgz file","text":"<ul> <li>load the <code>.tgz</code> file into Docker (<code>docker load</code>)</li> <li>run a container from it</li> <li>make changes</li> <li><code>docker commit</code></li> <li><code>docker save</code></li> </ul>"},{"location":"Docker_tgz/#do-changes-to-tgz-persist","title":"Do changes to .tgz persist?","text":"<ul> <li>if you only change inside the running container from image, these changes don't go back into the .tgz. They live in the container's writable layer.</li> <li>To persist changes into a new <code>.tgz</code>:</li> <li>Run a container from the image</li> <li>Make modifications</li> <li>Commit the container -&gt; new image   <code>docker commit &lt;container-id&gt; myimage:modified</code></li> <li>Save it back:   <code>docker save myimage:modified | gzip &gt; myimage-modified.tgz</code></li> </ul> <p>## How .tgz files work in practice  - <code>.tgz</code> = portable snapshot of a Docker image  - you can transfer it, then use docker load &lt; my image.tgz to load it into Docker on another host  -  mostly for air-gapped environments (no Docker Hub access) or backups.</p>"},{"location":"Docker_tgz/#to-load-the-tgz-into-docker","title":"To load the .tgz into Docker","text":"<p><code>docker load &lt; myimage.tgz</code></p> <ul> <li>makes the image available locally so you can run containers from it</li> </ul>"},{"location":"Docker_tgz/#to-create-a-tgz-from-an-existing-image","title":"To create a .tgz from an existing image","text":"<p><code>docker save myimage:tag | gzip &gt; myimage.tgz</code> - now you can copy over myimage.tgz to another machine and docker load it there</p> <p>sreedevirajavelu@srees-MacBook-Air pgtwin_7ss % docker load -i 7ss_db_docker_20250218.tgz  unrecognized image format</p> <p>That error means the file 7ss_db_docker_20250218.tgz is not in the expected Docker save/load format. Let\u2019s debug step by step:</p> <ol> <li>What docker load expects <code>docker load -i file.tgz</code> only works if the archive was created using:</li> </ol> <p><code>docker save -o image_name.tgz image:tag</code></p> <p>This produces a tarball with manifest.json, repositories, and layer directories inside.</p> <p>If your .tgz was created another way (e.g., tar czf on a Docker context or folder), docker load will reject it.</p> <ol> <li>Check the contents of your .tgz</li> </ol> <p>Run on your MacBook: tar -tzf 7ss_db_docker_20250218.tgz | head -20 If you see manifest.json and layer directories \u2192 \u2705 this is a docker save image, and something else is wrong. If you see Dockerfile, requirements.txt, Panda-db/, etc. \u2192 \u274c this is just the build context, not an image.</p> <ol> <li>If it\u2019s a Docker context (not an image)</li> </ol> <p>You cannot load it directly. Instead you need to build: tar -xzf 7ss_db_docker_20250218.tgz -C ./extracted/ cd extracted docker build -t my-image:latest .</p> <ol> <li>If it is a valid docker save image</li> </ol> <p>Try re-loading: gunzip -c 7ss_db_docker_20250218.tgz | docker load (some tools produce gzip-compressed vs tar-compressed files differently).</p> <p>sreedevirajavelu@srees-MacBook-Air pgtwin_7ss % tar -tzf 7ss_db_docker_20250218.tgz | head -20</p> <p>7ss_db/</p> <p>7ss_db/docker-sql/</p> <p>7ss_db/docker-sql/pgtv4_pp_7ss_db_20240302.sql</p> <p>7ss_db/docker-sql/my.cnf</p> <p>7ss_db/docker-sql/build_fixed_max_conn.sh</p> <p>7ss_db/docker-sql/build.sh</p> <p>7ss_db/docker-sql/pandapower_db_initial.sql</p> <p>7ss_db/docker-sql/prepare_max_connections.py</p> <p>7ss_db/docker-sql/README.md</p> <p>7ss_db/docker-sql/pandapower_db_structure.sql</p> <p>7ss_db/docker-sql/Dockerfile</p> <p>7ss_db/docker-sql/run_db.sh</p> <p>sreedevirajavelu@srees-MacBook-Air pgtwin_7ss % </p>"},{"location":"File_Transfer_Host_To_Remote/","title":"File Transfer Host To Remote","text":""},{"location":"File_Transfer_Host_To_Remote/#question","title":"Question:","text":"<p>I have ssh-ed into an ubuntu vm at 192.168.30.60 using vpn tunnel I want to transfer a folder from my local host into the ubuntu vm, how can I do this?</p> <p>Perfect \u2014 since you already SSH-ed into the Ubuntu VM at <code>192.168.30.60</code> via a VPN tunnel, there are several reliable ways to transfer a folder from your local host (Mac) to that remote Ubuntu VM.</p>"},{"location":"File_Transfer_Host_To_Remote/#option-1-use-scp-simple-and-recommended","title":"\ud83d\udfe2 Option 1 \u2014 Use <code>scp</code> (Simple and Recommended)","text":"<p><code>scp</code> (secure copy) is the easiest way to transfer files or folders over SSH.</p>"},{"location":"File_Transfer_Host_To_Remote/#to-copy-a-folder-from-your-local-mac-ubuntu-vm","title":"\ud83d\udcc1 To copy a folder from your local Mac \u2192 Ubuntu VM","text":"<pre><code>scp -r /path/to/local/folder ubuntu@192.168.30.60:/home/ubuntu/\n</code></pre> <p>Example:</p> <pre><code>scp -r ~/Documents/myproject ubuntu@192.168.30.60:/home/ubuntu/\n</code></pre> <ul> <li><code>-r</code> \u2192 recursive (for directories)</li> <li><code>ubuntu</code> \u2192 username on the VM</li> <li><code>/home/ubuntu/</code> \u2192 destination path inside the VM</li> </ul> <p>It will prompt for your SSH password or use your SSH key automatically if already set up.</p>"},{"location":"File_Transfer_Host_To_Remote/#option-2-use-rsync-faster-and-resumable","title":"\ud83d\udfe3 Option 2 \u2014 Use <code>rsync</code> (faster and resumable)","text":"<p>If the folder is large or you might re-transfer updates, use <code>rsync</code>:</p> <pre><code>rsync -avz ~/Documents/myproject ubuntu@192.168.30.60:/home/ubuntu/\n</code></pre> <ul> <li><code>-a</code> \u2192 archive mode (preserve permissions, etc.)</li> <li><code>-v</code> \u2192 verbose</li> <li><code>-z</code> \u2192 compress during transfer (good for VPNs)</li> </ul> <p>If interrupted, you can rerun the same command \u2014 it resumes efficiently.</p>"},{"location":"File_Transfer_Host_To_Remote/#option-3-if-youre-already-sshed-in-using-terminal","title":"\ud83d\udfe0 Option 3 \u2014 If you\u2019re already SSH\u2019ed in (using terminal)","text":"<p>If you are inside the SSH session and want to pull files from your local machine, you need to open a new terminal on your local Mac and run the <code>scp</code> or <code>rsync</code> command from your Mac, not from within SSH.</p> <p>(SSH connections are one-way for file transfer; the copy command must be run from the source side.)</p>"},{"location":"File_Transfer_Host_To_Remote/#option-4-using-visual-tools-optional","title":"\ud83d\udd35 Option 4 \u2014 Using Visual Tools (Optional)","text":"<p>If you prefer GUI methods:</p> <ul> <li>Use VS Code Remote SSH extension \u2192 open the Ubuntu VM and drag files.</li> <li> <p>Or use FileZilla:</p> </li> <li> <p>Protocol: SFTP</p> </li> <li>Host: <code>192.168.30.60</code></li> <li>Username: <code>ubuntu</code></li> <li>Password: (your SSH password)</li> <li>Port: <code>22</code></li> </ul>"},{"location":"File_Transfer_Host_To_Remote/#tip","title":"\ud83e\udde0 Tip","text":"<p>You can verify the transfer:</p> <pre><code>ssh ubuntu@192.168.30.60\nls -l /home/ubuntu/\n</code></pre> <p>You should see your folder there.</p> <p>Would you like me to show you how to copy the folder in reverse (from the Ubuntu VM \u2192 your Mac) as well?</p>"},{"location":"File_Transfer_Host_To_Remote/#question_1","title":"Question:","text":"<p>Based on the details below, how should I transfer the files from the local folder to the remote VM at  192.168.30.60</p> <p>path to local folder :</p> <p>/Users/sreedevirajavelu/Desktop/POWER PLANT TWIN/ccp_multi_IED_multi_plc_database_pg</p> <p>at remote VM:</p> <p>ubuntu@pgt:~$ whoami</p> <p>ubuntu</p> <p>Perfect \ud83d\udc4d You\u2019ve already got all the info needed. Here\u2019s exactly how to transfer your local folder to your remote Ubuntu VM at <code>192.168.30.60</code>.</p>"},{"location":"File_Transfer_Host_To_Remote/#command-to-use-from-your-local-host-terminal","title":"\u2705 Command to use (from your local host terminal):","text":"<pre><code>scp -r \"/Users/sreedevirajavelu/Desktop/POWER PLANT TWIN/ccp_multi_IED_multi_plc_database_pg\" ubuntu@192.168.30.60:/home/ubuntu/\n</code></pre>"},{"location":"File_Transfer_Host_To_Remote/#explanation","title":"\ud83e\udde0 Explanation:","text":"<ul> <li><code>scp</code> \u2192 Secure copy over SSH</li> <li><code>-r</code> \u2192 Recursively copy directories and their contents</li> <li><code>\"...\"</code> \u2192 Quotes around the path because your folder name contains spaces (<code>POWER PLANT TWIN</code>)</li> <li><code>ubuntu@192.168.30.60</code> \u2192 Username and target host</li> <li><code>/home/ubuntu/</code> \u2192 Destination path on the VM (your home directory)</li> </ul>"},{"location":"File_Transfer_Host_To_Remote/#to-verify-after-copying","title":"\ud83d\udee0 To verify after copying:","text":"<p>Once the command completes, SSH into your VM (if you aren\u2019t already) and run:</p> <pre><code>ls /home/ubuntu/\n</code></pre> <p>You should see:</p> <pre><code>ccp_multi_IED_multi_plc_database_pg\n</code></pre>"},{"location":"File_Transfer_Host_To_Remote/#if-youre-connected-via-vpn","title":"\u26a0\ufe0f If you\u2019re connected via VPN:","text":"<p>Make sure your VPN tunnel allows SSH/SCP traffic to <code>192.168.30.60</code>. If <code>scp</code> hangs or times out, you can test connectivity first:</p> <pre><code>ssh ubuntu@192.168.30.60\n</code></pre> <p>If SSH works, <code>scp</code> will work too.</p> <p>Would you like to copy it to a specific directory inside the VM (e.g., <code>/opt/projects/</code> or <code>/srv/plc/</code>) instead of your home folder? I can adjust the command accordingly.</p>"},{"location":"Fix_Slow_UbuntuVM_Boot/","title":"Fix slow booting for ubuntu vm based on output of these commands to find what was slowing down the booting time","text":"<pre><code> systemd-analyze\n systemd-analyze blame\n</code></pre> <p>Top delay:</p> <p>2min 258ms systemd-networkd-wait-online.service</p> <p>This service alone adds over 2 minutes to your boot. That\u2019s practically your entire userspace delay.</p> <p>\ud83d\udd0d What is systemd-networkd-wait-online.service?</p> <p>It waits for the network to be fully up (like getting a DHCP lease or full connection). Used mostly on servers to ensure critical services only start after network is available. In most VMs and desktops, it's not necessary and can be safely disabled.</p> <p>\u2705 Fix: Disable the Wait-Online Service</p> <p>Run: - sudo systemctl disable systemd-networkd-wait-online.service - sudo systemctl mask systemd-networkd-wait-online.service</p> <p>\ud83d\udca1 mask ensures it cannot be accidentally started again.</p> <p>If you also see or use NetworkManager, you can disable its wait service too:</p> <p>sudo systemctl disable NetworkManager-wait-online.service</p> <p>sudo systemctl mask NetworkManager-wait-online.service</p> <p>\ud83d\udd01 Then reboot your VM: sudo reboot \ud83e\uddea After reboot:</p> <p>Check again: systemd-analyze You should see your boot time drop from 2+ minutes \u2192 ~15 seconds or less.</p> <p>\ud83d\udd01 To Undo the Effects of Disabling and Masking the Wait-Online Service If you previously ran:</p> <ul> <li> <p>sudo systemctl disable systemd-networkd-wait-online.service</p> </li> <li> <p>sudo systemctl mask systemd-networkd-wait-online.service</p> </li> </ul> <p>You can restore the service like this:</p> <p>\u2705 Step 1: Unmask the service</p> <ul> <li>sudo systemctl unmask systemd-networkd-wait-online.service</li> </ul> <p>\u2705 Step 2: Re-enable the service at boot</p> <ul> <li>sudo systemctl enable systemd-networkd-wait-online.service \u2705 Step 3 (Optional): Start it immediately (not needed for reboot use)</li> </ul> <p>sudo systemctl start systemd-networkd-wait-online.service</p> <p>\ud83d\udd01 Same steps apply if you disabled NetworkManager-wait-online.service: Just replace the service name in each command. Example:</p> <p>sudo systemctl unmask NetworkManager-wait-online.service</p> <p>sudo systemctl enable NetworkManager-wait-online.service</p> <p>\ud83e\uddea Confirm it's active again:</p> <p>systemctl status systemd-networkd-wait-online.service or systemctl is-enabled systemd-networkd-wait-online.service</p> <p>You should see:</p> <p><code>enabled</code></p> <p>\u2705 Summary: Rollback Commands</p>"},{"location":"Fix_Slow_UbuntuVM_Boot/#unmask-and-re-enable-systemd-networkd-wait-online","title":"Unmask and re-enable systemd-networkd-wait-online","text":"<p>sudo systemctl unmask systemd-networkd-wait-online.service</p> <p>sudo systemctl enable systemd-networkd-wait-online.service</p>"},{"location":"Fix_Slow_UbuntuVM_Boot/#optional-also-for-networkmanager-wait-online-if-previously-masked","title":"Optional: Also for NetworkManager-wait-online (if previously masked)","text":"<p>sudo systemctl unmask NetworkManager-wait-online.service</p> <p>sudo systemctl enable NetworkManager-wait-online.service</p>"},{"location":"Mac/","title":"Mac Notes","text":""},{"location":"Mac/#setting-up-ubuntu-vm-on-virtualbox-on-macbook-m4","title":"Setting up Ubuntu VM on VirtualBox on MacBook M4 :","text":"<p>https://www.youtube.com/watch?v=LjL_N0OZxvY</p>"},{"location":"Mac/#make-ubuntu-full-screen-in-virtualbox","title":"Make Ubuntu full screen in VirtualBox:","text":"<p>https://youtu.be/Fw8ppXeJ_GY?si=qu0ehtCFotNpaLcB - this tutorial also shows how to install guest additions</p>"},{"location":"Mac/#how-to-create-shared-folder-between-host-and-ubuntu-vm-on-virtualbox","title":"How to create shared folder between host and Ubuntu VM on VirtualBox:","text":"<p>https://youtu.be/j8Ne96h8UDg?si=aBwXLuINWmvPNp1l</p> <p>Fix slow booting for ubuntu vm based on output of these commands to find what was slowing down the booting time</p> <pre><code> systemd-analyze\n systemd-analyze blame\n</code></pre> <p></p> <p>Top delay:</p> <p>2min 258ms systemd-networkd-wait-online.service</p> <p>This service alone adds over 2 minutes to your boot. That\u2019s practically your entire userspace delay.</p> <p>\ud83d\udd0d What is systemd-networkd-wait-online.service?</p> <p>It waits for the network to be fully up (like getting a DHCP lease or full connection). Used mostly on servers to ensure critical services only start after network is available. In most VMs and desktops, it's not necessary and can be safely disabled.</p> <p>\u2705 Fix: Disable the Wait-Online Service</p> <p>Run: - sudo systemctl disable systemd-networkd-wait-online.service - sudo systemctl mask systemd-networkd-wait-online.service</p> <p>\ud83d\udca1 mask ensures it cannot be accidentally started again.</p> <p>If you also see or use NetworkManager, you can disable its wait service too:</p> <p>sudo systemctl disable NetworkManager-wait-online.service</p> <p>sudo systemctl mask NetworkManager-wait-online.service</p> <p>\ud83d\udd01 Then reboot your VM: sudo reboot \ud83e\uddea After reboot:</p> <p>Check again: systemd-analyze You should see your boot time drop from 2+ minutes \u2192 ~15 seconds or less.</p> <p>\ud83d\udd01 To Undo the Effects of Disabling and Masking the Wait-Online Service If you previously ran:</p> <ul> <li> <p>sudo systemctl disable systemd-networkd-wait-online.service</p> </li> <li> <p>sudo systemctl mask systemd-networkd-wait-online.service</p> </li> </ul> <p>You can restore the service like this:</p> <p>\u2705 Step 1: Unmask the service</p> <ul> <li>sudo systemctl unmask systemd-networkd-wait-online.service</li> </ul> <p>\u2705 Step 2: Re-enable the service at boot</p> <ul> <li>sudo systemctl enable systemd-networkd-wait-online.service \u2705 Step 3 (Optional): Start it immediately (not needed for reboot use)</li> </ul> <p>sudo systemctl start systemd-networkd-wait-online.service</p> <p>\ud83d\udd01 Same steps apply if you disabled NetworkManager-wait-online.service: Just replace the service name in each command. Example:</p> <p>sudo systemctl unmask NetworkManager-wait-online.service</p> <p>sudo systemctl enable NetworkManager-wait-online.service</p> <p>\ud83e\uddea Confirm it's active again:</p> <p>systemctl status systemd-networkd-wait-online.service or systemctl is-enabled systemd-networkd-wait-online.service</p> <p>You should see:</p> <p><code>enabled</code></p> <p>\u2705 Summary: Rollback Commands</p>"},{"location":"Mac/#unmask-and-re-enable-systemd-networkd-wait-online","title":"Unmask and re-enable systemd-networkd-wait-online","text":"<p>sudo systemctl unmask systemd-networkd-wait-online.service</p> <p>sudo systemctl enable systemd-networkd-wait-online.service</p>"},{"location":"Mac/#optional-also-for-networkmanager-wait-online-if-previously-masked","title":"Optional: Also for NetworkManager-wait-online (if previously masked)","text":"<p>sudo systemctl unmask NetworkManager-wait-online.service</p> <p>sudo systemctl enable NetworkManager-wait-online.service</p>"},{"location":"Matlab/","title":"Matlab","text":"<p>Install this Java Runtime before installing Matlab</p> <p>Java Runtime Required</p> <p>Native Apple silicon MATLAB requires a Java runtime be installed on your Mac. A supported Java 11 JRE is available free of charge with Amazon Corretto 11</p> <p>https://ww2.mathworks.cn/en/support/requirements/apple-silicon.html</p> <p>Had to install Xcode from App store</p>"},{"location":"Matlab_Data_Sources/","title":"Matlab Data Sources","text":""},{"location":"Matlab_Data_Sources/#matlab-data-sources-sql-vs-postgresql","title":"Matlab Data Sources - SQL vs PostgreSQL","text":""},{"location":"Matlab_Data_Sources/#postgresql-jdbc-for-macos","title":"PostgreSQL JDBC for macOS:","text":"<p>https://ww2.mathworks.cn/help/database/ug/postgresql-jdbc-for-macos.html</p>"},{"location":"Matlab_Data_Sources/#postgresql-jdbc-for-windows","title":"PostgreSQL JDBC for Windows:","text":"<p>https://ww2.mathworks.cn/help/database/ug/postgresql-jdbc-windows.html</p> <p>I have configured a JDBC data source on matlab using mysql-connector-j-9.3.0 as the driver , and connecting to a dockerized version of a mysql database, how can I change this to postgresql database instead with minimal changes?</p> <ol> <li>Replace JDBC Driver MySQL and PostgreSQL require different JDBC drivers: Current (MySQL): mysql-connector-j-9.3.0.jar Required (Postgres): postgresql-.jar (latest is 42.x) \ud83d\udc49 Download the .jar and place it in the same folder where MATLAB can access JDBC drivers (often under matlabroot/toolbox/local or a custom path you\u2019ve set). <p>Link to download JDBC Drivers</p> <p>https://ww2.mathworks.cn/en/products/database/driver-installation.html</p> <ul> <li>choose PostgreSQL JDBC driver</li> </ul> <p>\u2705 Which JDBC Driver to Use F rom the PostgreSQL JDBC site:</p> <p>You are on Postgres 16 \u2192 supported by all recent drivers.</p> <p>You are likely running Java 8+ (MATLAB since R2020a bundles Java 8, and newer releases may bundle Java 11).</p> <p>\ud83d\udc49 So you should use the JDBC 4.2 driver (version 42.7.8, latest stable). That means: <code>postgresql-42.7.8.jar</code> is the right driver for you.</p> <ul> <li> <p>check version of Java on Matlab</p> </li> <li> <p>check version of PostgreSQL used as JDBC data source</p> </li> </ul> <p>\ud83d\udd0d Things to Check Before Connecting</p> <p>Java Version in MATLAB</p> <p>In MATLAB, check which JVM it\u2019s using:</p> <p>version -java</p> <p>If it shows <code>Java 8 or higher</code>, you\u2019re good with <code>postgresql-42.7.8.jar</code>. If it shows Java 7 \u2192 use postgresql-42.2.29.jar. If Java 6 (unlikely unless MATLAB is very old) \u2192 use postgresql-42.2.27.jar.</p>"},{"location":"Matlab_Data_Sources/#content-of-docker-composeyml-file-to-use-postgresql-database-as-docker-container","title":"Content of docker-compose.yml file to use postgresql database as docker container","text":"<p>This version does not persist volume and data will vanish if you remove the container.</p> <pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: ccp-multi-ied-multi-plc\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: powerplant\n    ports:\n      - \"5432:5432\"\n    volumes:\n    - ./postgres-sql:/docker-entrypoint-initdb.d\n\n</code></pre> <p>Use of  -  volumes:       - ./sql:/docker-entrypoint-initdb.d</p> <pre><code>- This mounts your host directory ./sql into the container\u2019s /docker-entrypoint-initdb.d directory.\n\n- docker-entrypoint-initdb.d is a special folder: on first startup of the container, MySQL runs all .sql or .sh scripts there to initialize the DB.\n\n- After initialization, new runs don\u2019t automatically re-run scripts unless you remove the container and volume.\n\n\u2757 This is not where live database data is stored. Live data is in /var/lib/mysql. Since you\u2019re not mounting that, the data lives in a Docker-managed anonymous volume.\n\nSo: Changing this bind mount will not remove your MySQL data. It only affects which init scripts would run on container creation.\n</code></pre>"},{"location":"Matlab_Data_Sources/#content-of-docker-composeyml-file-to-use-postgresql-database-as-docker-container-with-persistent-volume","title":"Content of docker-compose.yml file to use postgresql database as docker container (with persistent volume)","text":"<pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: ccp-multi-ied-multi-plc-pg\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: powerplant\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data   # &lt;-- persistent volume for DB data\n      - ./postgres-sql:/docker-entrypoint-initdb.d  # &lt;-- init scripts\n\nvolumes:\n  postgres_data:   # &lt;-- named volume definition\n</code></pre>"},{"location":"Matlab_Data_Sources/#steps-to-use-postgresql-database-with-matlab","title":"Steps to use PostgreSQL database with MATLAB:","text":"<ol> <li>Download JDBC driver from PostgreSQL JDBC site (check version of Java on MATLAB &amp; Postgres version)</li> <li></li> </ol>"},{"location":"Matlab_Docker/","title":"Matlab Docker","text":"<p>Summarise steps to run the matlab docker container, requirements to run the matlab docker container and how I can check that my license fulfils the requirements. After running the docker container, how can i access the matlab docker container to add simulation files, configure data source, run simulation? Note that I will be running on a remote ubuntu vm that i have ssh-ed into using VPN tunnel and i can only view the browser on my local machine so what options do i have to access and use the matlab docker container and which is best ?</p> <p>Here\u2019s a complete, structured summary of what you need to know and do to run and access a MATLAB Docker container \u2014 tailored to your setup (remote Ubuntu VM accessed via SSH and VPN, with browser access only on your local machine).</p>"},{"location":"Matlab_Docker/#1-requirements-to-run-matlab-docker-container","title":"\ud83e\udde9 1. Requirements to Run MATLAB Docker Container","text":""},{"location":"Matlab_Docker/#software-requirements","title":"\u2705 Software Requirements","text":"<ul> <li> <p>Docker installed on your remote Ubuntu VM.</p> </li> <li> <p>Check: <code>docker --version</code></p> </li> <li> <p>If missing:</p> <p><code>bash sudo apt update sudo apt install docker.io -y sudo systemctl enable docker sudo systemctl start docker</code></p> </li> <li> <p>MATLAB container image from Docker Hub:</p> </li> </ul> <p><code>bash   docker pull mathworks/matlab:r2025a   # version I used on Macbook</code></p>"},{"location":"Matlab_Docker/#license-requirements","title":"\u2705 License Requirements","text":"<p>Your MATLAB license must:</p> <ol> <li>Be valid for all MathWorks products you want to use.</li> <li>Be linked to your MathWorks account.</li> <li>Be configured for cloud use.</li> </ol> <p>License types that already support cloud use:</p> <ul> <li>Individual license \u2705</li> <li>Campus-Wide license \u2705</li> </ul> <p>For Concurrent (Network) licenses:</p> <ul> <li>You need to specify the license server:</li> </ul> <p><code>bash   -e MLM_LICENSE_FILE=27000@MyLicenseServer</code></p>"},{"location":"Matlab_Docker/#how-to-check-if-your-license-meets-the-requirements","title":"\ud83d\udd0d How to Check if Your License Meets the Requirements","text":"<ol> <li>Go to your MathWorks Account page.</li> <li> <p>Under Licenses \u2192 Manage Licenses, check:</p> </li> <li> <p>License type (Individual, Campus-Wide, or Concurrent).</p> </li> <li>If Concurrent, note down the license server details (port and DNS address).</li> <li>Confirm with your license administrator or MathWorks support if it is enabled for cloud use.</li> </ol>"},{"location":"Matlab_Docker/#2-steps-to-run-the-matlab-docker-container","title":"\u2699\ufe0f 2. Steps to Run the MATLAB Docker Container","text":""},{"location":"Matlab_Docker/#option-1-run-matlab-in-browser-mode-simpler-recommended","title":"\ud83e\udded Option 1: Run MATLAB in Browser Mode (Simpler, Recommended)","text":"<p>Run this on your remote Ubuntu VM:</p> <pre><code>docker run -it --rm -p 8888:8888 --shm-size=512M mathworks/matlab:r2024b -browser\n</code></pre> <ul> <li><code>-p 8888:8888</code> \u2192 exposes port for browser connection</li> <li><code>--shm-size=512M</code> \u2192 required for MATLAB GUI stability</li> <li><code>-browser</code> \u2192 enables web-based MATLAB interface</li> </ul> <p>After running, Docker will print a URL like:</p> <pre><code>http://localhost:8888/?token=abcd1234\n</code></pre> <p>Since your browser is on your local machine, you must forward this port through SSH.</p>"},{"location":"Matlab_Docker/#ssh-port-forwarding-from-local-remote","title":"\ud83d\udedc SSH Port Forwarding (from Local \u2192 Remote)","text":"<p>Run this on your local machine (not inside SSH):</p> <pre><code>ssh -L 8888:localhost:8888 username@remote_vm_ip\n</code></pre> <p>Then open in your local browser:</p> <pre><code>http://localhost:8888\n</code></pre> <p>Login with your MathWorks credentials when prompted.</p>"},{"location":"Matlab_Docker/#option-2-run-matlab-in-vnc-mode-for-full-desktop-access","title":"\ud83e\udded Option 2: Run MATLAB in VNC Mode (for full desktop access)","text":"<p>Run this on your remote Ubuntu VM:</p> <pre><code>docker run --init -it --rm -p 5901:5901 -p 6080:6080 --shm-size=512M mathworks/matlab:r2024b -vnc\n</code></pre> <p>Then:</p> <ul> <li>Via VNC client: connect to <code>remote_vm_ip:5901</code> (password: <code>matlab</code>)</li> <li>OR via browser (no VNC client): open</li> </ul> <p><code>http://localhost:6080</code></p> <p>\u2192 use SSH port forwarding:</p> <p><code>bash   ssh -L 6080:localhost:6080 username@remote_vm_ip</code></p>"},{"location":"Matlab_Docker/#3-accessing-and-using-matlab-container","title":"\ud83e\uddf0 3. Accessing and Using MATLAB Container","text":"<p>Once the container is running:</p>"},{"location":"Matlab_Docker/#add-simulation-files","title":"\ud83d\uddc2 Add Simulation Files","text":"<p>Option 1: Mount a directory from your remote VM into the container:</p> <pre><code>docker run -it --rm \\\n  -p 8888:8888 \\\n  -v /path/to/your/files:/home/matlab/projects \\\n  --shm-size=512M mathworks/matlab:r2024b -browser\n</code></pre> <p>Inside MATLAB, access your files under <code>/home/matlab/projects</code>.</p> <p>Option 2: Copy files into container (temporary)</p> <pre><code>docker cp localfile.m container_id:/home/matlab/\n</code></pre>"},{"location":"Matlab_Docker/#configure-data-source","title":"\u2699\ufe0f Configure Data Source","text":"<p>If your simulation depends on external data:</p> <ul> <li>Mount the data directory the same way using <code>-v</code>.</li> <li>If using a database or sensor stream, ensure the container has network access (use <code>--network host</code> if necessary).</li> </ul>"},{"location":"Matlab_Docker/#run-simulation","title":"\u25b6\ufe0f Run Simulation","text":"<p>Once inside MATLAB (browser or VNC):</p> <ul> <li>Open <code>.slx</code> or <code>.m</code> files from your mounted directory.</li> <li>Run simulations normally using the GUI or MATLAB command line.</li> </ul>"},{"location":"Matlab_Docker/#4-access-options-from-your-local-machine","title":"\ud83c\udf10 4. Access Options from Your Local Machine","text":"Access Method Description Pros Cons Recommended When Browser Mode (<code>-browser</code>) Use MATLAB Web interface through port 8888 Easiest, lightweight, no VNC setup Slightly limited desktop features \u2705 Best for general use VNC Mode (<code>-vnc</code>) Full MATLAB desktop via port 6080 or 5901 Supports toolboxes/add-ons GUI Slightly slower over VPN When full GUI features needed Batch Mode (no GUI) Run scripts or simulations directly Fastest, low resource use No graphical interface Automated simulations"},{"location":"Matlab_Docker/#best-setup-for-your-case-remote-ubuntu-vm-over-vpn","title":"\u2705 Best Setup for Your Case (Remote Ubuntu VM over VPN)","text":"<p>Given that:</p> <ul> <li>You SSH into the VM.</li> <li>You can only use your local browser (not remote desktop).</li> </ul> <p>\ud83d\udc49 Recommended Setup:</p> <ol> <li>Use Browser Mode (<code>-browser</code>).</li> <li>Use SSH port forwarding to map <code>8888</code> from remote to local.</li> <li>Mount your project directory via <code>-v</code>.</li> <li>Access MATLAB via <code>http://localhost:8888</code> in your local browser.</li> </ol> <p>Example final command:</p> <pre><code>ssh -L 8888:localhost:8888 user@remote_vm_ip\n</code></pre> <p>Then on the VM:</p> <pre><code>docker run -it --rm \\\n  -p 8888:8888 \\\n  -v ~/matlab_projects:/home/matlab/projects \\\n  --shm-size=512M mathworks/matlab:r2024b -browser\n</code></pre> <p>Access locally via: \ud83d\udc49 <code>http://localhost:8888</code></p> <p>Would you like me to include steps for saving MATLAB session data and simulations persistently (so it\u2019s not lost when the container stops)?</p>"},{"location":"Matlab_Docker/#question-i-need-to-install-the-database-toolbox-so-will-it-be-better-to-use-the-browser-or-vnc-option-to-run-and-access-the-matlab-docker-container","title":"Question: I need to install the database toolbox, so will it be better to use the browser or vnc option to run and access the matlab docker container","text":"<p>Excellent question \u2014 and you\u2019re absolutely right to pause and think about this before running the container.</p> <p>When you need to install toolboxes like the Database Toolbox, the VNC mode is the better and more reliable option.</p> <p>Let\u2019s break down why and how \ud83d\udc47</p>"},{"location":"Matlab_Docker/#1-key-difference-between-browser-and-vnc-modes","title":"\ud83e\udde9 1. Key Difference Between Browser and VNC Modes","text":"Mode Interface Can Install Toolboxes? When to Use Browser Mode (<code>-browser</code>) MATLAB Web Desktop (lightweight interface) \u274c No \u2013 the installer UI and Add-On Explorer are not supported When you just want to run existing MATLAB scripts or simulations VNC Mode (<code>-vnc</code>) Full MATLAB Desktop via virtual display \u2705 Yes \u2013 supports Toolbox installation, updates, Add-On Explorer When you need to install or manage MATLAB toolboxes"},{"location":"Matlab_Docker/#2-how-to-run-matlab-in-vnc-mode-recommended-for-installing-toolboxes","title":"\ud83e\uddf0 2. How to Run MATLAB in VNC Mode (Recommended for Installing Toolboxes)","text":"<p>Run this on your remote Ubuntu VM:</p> <pre><code>docker run --init -it --name matlab_container \\\n  -p 5901:5901 -p 6080:6080 \\\n  --shm-size=512M \\\n  mathworks/matlab:r2024b -vnc\n</code></pre>"},{"location":"Matlab_Docker/#explanation","title":"Explanation:","text":"<ul> <li><code>--init</code> \u2192 handles clean process shutdown</li> <li><code>-p 5901:5901</code> \u2192 for VNC client connection</li> <li><code>-p 6080:6080</code> \u2192 for browser-based VNC (no separate client needed)</li> <li><code>--shm-size=512M</code> \u2192 required shared memory</li> <li><code>-vnc</code> \u2192 launches MATLAB with a full GUI desktop</li> </ul> <p>Default VNC password: <code>matlab</code></p>"},{"location":"Matlab_Docker/#3-how-to-access-matlab-gui-from-your-local-machine","title":"\ud83c\udf10 3. How to Access MATLAB GUI from Your Local Machine","text":"<p>Since you\u2019re on a remote Ubuntu VM (accessed via SSH + VPN) and can only view browsers locally:</p>"},{"location":"Matlab_Docker/#option-1-use-browser-to-access-via-port-6080-simplest","title":"\u2705 Option 1: Use Browser to Access via Port 6080 (Simplest)","text":"<ol> <li>On your local machine, forward the VNC web port:</li> </ol> <p><code>bash    ssh -L 6080:localhost:6080 username@remote_vm_ip</code> 2. Open in your local browser:</p> <p><code>http://localhost:6080</code> 3. Login with password <code>matlab</code>.    You\u2019ll see a full Ubuntu desktop running MATLAB.</p> <p>\u2192 From there, open MATLAB \u2192 Home \u2192 Add-Ons \u2192 Get Add-Ons, search for Database Toolbox, and install.</p>"},{"location":"Matlab_Docker/#option-2-use-a-vnc-client-tigervnc-realvnc-etc","title":"\ud83e\uddd1\u200d\ud83d\udcbb Option 2: Use a VNC Client (TigerVNC, RealVNC, etc.)","text":"<ol> <li>Forward the VNC port:</li> </ol> <p><code>bash    ssh -L 5901:localhost:5901 username@remote_vm_ip</code> 2. Connect using a VNC viewer to:</p> <p><code>localhost:5901</code> 3. Password: <code>matlab</code></p> <p>This gives a smoother experience (less lag, clipboard support).</p>"},{"location":"Matlab_Docker/#4-keep-the-container-persistent","title":"\ud83d\udcbe 4. Keep the Container Persistent","text":"<p>When installing toolboxes, you\u2019ll definitely want to keep the container persistent so your installation isn\u2019t lost:</p> <pre><code>docker run --init -it --name matlab_container \\\n  -p 5901:5901 -p 6080:6080 \\\n  -v ~/matlab_projects:/home/matlab/projects \\\n  --shm-size=512M \\\n  mathworks/matlab:r2024b -vnc\n</code></pre> <p>Then after installation:</p> <pre><code>docker stop matlab_container      # stop container\ndocker start -ai matlab_container # restart with toolboxes intact\n</code></pre>"},{"location":"Matlab_Docker/#summary","title":"\ud83e\udde0 Summary","text":"Task Best Mode Why Running scripts/simulations Browser Lighter, easier to access Installing toolboxes / Add-Ons VNC Full desktop needed Updating MATLAB / managing licenses VNC GUI access required Automated jobs / headless execution Batch mode No GUI needed <p>\u2705 Recommendation for you Since you need to install the Database Toolbox, start MATLAB in VNC mode (<code>-vnc</code>) and access it through your local browser (port 6080) or a VNC viewer. Once installed, you can later switch back to browser mode (<code>-browser</code>) for daily use.</p> <p>Would you like me to show the exact steps to install the Database Toolbox inside the VNC MATLAB container, including how to make sure the toolbox persists across container restarts?</p>"},{"location":"NAT_Adapter_Iptables_Forwarding/","title":"NAT Adapter Iptables Fowarding","text":"<p><code>docker run -d --name eg_plc --net plc-macvlan --ip 192.168.40.202 --privileged openplc:v3</code></p> <p>ubuntu@pgt:~$ ip addr show enp0s3 2: enp0s3:  mtu 1500 qdisc fq_codel state UP group default qlen 1000     link/ether 08:00:27:62:7e:2a brd ff:ff:ff:ff:ff:ff     inet 192.168.30.60/24 brd 192.168.30.255 scope global enp0s3        valid_lft forever preferred_lft forever     inet6 fe80::a00:27ff:fe62:7e2a/64 scope link        valid_lft forever preferred_lft forever <p>Perfect \ud83d\udc4d \u2014 your VM\u2019s bridged IP is 192.168.30.60, so that\u2019s exactly the address to use in your forwarding rules. Here\u2019s the exact set of commands you can run inside your Ubuntu VM to make your 4 PLC Docker containers accessible from:</p> <ul> <li>the Remote Desktop host where the VM runs, and</li> <li>your own machine (the one you use to SSH into the VM).</li> </ul> <p>\ud83e\udde9 Step 1 \u2014 Run these commands inside the Ubuntu VM</p> <pre><code>sudo iptables -t nat -A PREROUTING -p tcp -d 192.168.30.60 --dport 8081 -j DNAT --to-destination 192.168.40.201:8080\nsudo iptables -t nat -A PREROUTING -p tcp -d 192.168.30.60 --dport 8082 -j DNAT --to-destination 192.168.40.202:8080\nsudo iptables -t nat -A PREROUTING -p tcp -d 192.168.30.60 --dport 8083 -j DNAT --to-destination 192.168.40.203:8080\nsudo iptables -t nat -A PREROUTING -p tcp -d 192.168.30.60 --dport 8084 -j DNAT --to-destination 192.168.40.204:8080\nsudo iptables -t nat -A POSTROUTING -j MASQUERADE\n</code></pre> <p>These rules tell Ubuntu: \"When traffic comes to 192.168.30.60:8081, forward it internally to 192.168.40.201:8080 (and so on for the others).\" \ud83e\udde9 Step 2 \u2014 Verify it\u2019s working From your Remote Desktop host, open: http://192.168.30.60:8081/login \u2192 PLC1 http://192.168.30.60:8082/login \u2192 PLC2 http://192.168.30.60:8083/login \u2192 PLC3 http://192.168.30.60:8084/login \u2192 PLC4 From your own computer (the one you use to SSH in), you can use the same URLs \u2014 as long as you can ping 192.168.30.60. You do not need to use localhost or SSH port forwarding anymore. \ud83e\udde9 Step 3 \u2014 (Optional but recommended) Make the rules persistent To make sure the forwarding stays active after reboot: sudo apt install iptables-persistent -y sudo netfilter-persistent save sudo netfilter-persistent enable  Now the rules you just added will automatically reapply every time the VM restarts. \u2705 Quick access summary PLC Container IP    Browser Access URL  Target (inside VM) PLC1    192.168.40.201  http://192.168.30.60:8081/login 192.168.40.201:8080 PLC2    192.168.40.202  http://192.168.30.60:8082/login 192.168.40.202:8080 PLC3    192.168.40.203  http://192.168.30.60:8083/login 192.168.40.203:8080 PLC4    192.168.40.204  http://192.168.30.60:8084/login 192.168.40.204:8080 Would you like me to modify your existing /usr/local/sbin/setup-macvlan.sh so it automatically sets up these forwarding rules too, so you only manage one script?</p> <p>TO DO: need to continue for other 3 plc containers, persist the rules</p> <p>make sure to docker start [plc_name]</p> <p>st_plc -&gt; http://192.168.30.60:8081/login </p> <p>Can be accessed on remote desktop (base machine of cyberX) or on your own host machine where you login to the VM</p> <p>After setting the DNAT rule:</p> <pre><code>sudo iptables -t nat -A PREROUTING -p tcp -d 192.168.30.60 --dport 8081 -j DNAT --to-destination 192.168.40.201:8080\nsudo iptables -t nat -A POSTROUTING -j MASQUERADE\n</code></pre> <p>Able to access http://192.168.30.60:8081/dashboard from the host machine (host machine that has ssh connection to ubuntu VM on remote host &amp; docker container is on this ubuntu vm) but NOT able to ping 192.168.30.60 </p> <p>\u2699\ufe0f Why http works but ping doesn\u2019t</p> <p><code>-p tcp --dport 8081</code></p> <p>So it matches only TCP packets going to port 8081. ping uses ICMP, not TCP. Therefore, your DNAT rule does not apply to ICMP at all. The host (192.168.30.60) has no direct route to 192.168.40.0/24 The internal Docker network (192.168.40.0/24) is isolated by default. Containers can reach out to the internet via NAT, but the host or external machines cannot directly reach them unless: You create a bridge network and expose it, Or add specific iptables / routing rules to forward ICMP (rarely done). HTTP works because of port forwarding (DNAT) When your browser connects to port 8081 on 192.168.30.60, iptables rewrites the packet\u2019s destination to 192.168.40.201:8080. Docker or the VM\u2019s networking stack then handles the return packets (via MASQUERADE). So HTTP works perfectly through this NAT.</p>"},{"location":"PostgreSQL/","title":"PostgreSQL","text":""},{"location":"PostgreSQL/#using-postgresql-db-from-docker-composeyml","title":"Using Postgresql DB from docker-compose.yml","text":"<pre><code>services:\n  postgres:\n    image: postgres:16\n    container_name: ccp-multi-ied-multi-plc-pg\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: powerplant\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data # persistent volume for DB data\n      - ./postgres-sql:/docker-entrypoint-initdb.d # init scripts\n\nvolumes:\n  postgres_data: # named volume definition\n</code></pre> <p>Got it \u2705 \u2014 you\u2019ve already defined your PostgreSQL service with user root, password root, and database powerplant.</p> <p>Once you get a bash shell inside the container, you can access PostgreSQL using the psql client. Here\u2019s how:</p> <ol> <li>Get into the container\u2019s shell <code>docker exec -it ccp-multi-ied-multi-plc-pg bash</code></li> </ol> <p>To enter PostgreSQL:</p> <p><code>psql -U root -d powerplant</code></p> <p>will prompt for a password -&gt; type <code>root</code> based on docker-compose.yml used</p> <p>Once insde psql, you will see something like:</p> <p><code>powerplant=#</code></p> <p>Now you can run SQL commands:</p> <pre><code>\\l           -- list databases\n\\c powerplant -- connect to powerplant DB\n\\dt          -- list tables\nSELECT * FROM ref;\nSELECT * FROM ccp_values;\n</code></pre> <p>\ud83d\udca1 Optional tips and customizations 1\ufe0f\u20e3 Disable the pager temporarily</p> <p>If you just want to see everything inline for one session:</p> <p>\\pset pager off</p> <p></p> <ol> <li>run postgresql db as a docker container : inside ccp_multi_IED_multi_plc_database_pg folder : docker compose up -d</li> <li>go into postgresql db : docker exec -it ccp-multi-ied-multi-plc-pg bash psql -U root -d powerplant password: root to show tables : \\dt SELECT * FROM ccp_values;</li> <li>figure out how to connect to matlab using JDBC data source -&gt; configure JDBC data source ...connector for postgresql (driver) </li> </ol> <p>data source name : Postgres_CCP database name : powerplant server: localhost port number : 5432 </p> <p>currently on postgres: 16 and java 8+ </p> <p>\ud83d\udc49 So you should use the JDBC 4.2 driver (version 42.7.8, latest stable). That means: postgresql-42.7.8.jar is the right driver for you. check what the xsteam is for ... and whether it needs to be changed try running with the postgresql db -&gt; what is the next part that needs to be connected check whether IED scripts need to change part at establish_connection() ... conn = mysqlconnector.connect( ....</p> <p>XSteam.m , XSteamUS.m , license.txt , X Steam for Matlab.pdf </p> <p>previously for a JDBC data source on matlab that used mysql database , on the HOME tab, I had done these: I selected 'Set Path' , 'Add Folder' and then selected xsteam folder  which contained these 4 files , will it make sense to add this same path to the same folder after I changed database to postgresql but im using it for the same functionality ? what is the xsteam folder for and does any of the files in it need to be modified ?</p> <p>XSteam folder and files belong to an independent MATLAB library called XSteam, used to calculate thermodynamic properties of water and steam (e.g. enthalpy, entropy, temperature, pressure-volume relationships, steam quality etc. )</p> <ol> <li>in stored_procedure_pg_GT.sql -&gt; timestamp was changed to ts as timestamp is a reserved keyword in postgresql</li> </ol> <p>need to check if it affects anything in the IEDs or PLCs for GT</p> <p>check the changes needed for the IED files</p> <p>Since you\u2019ve switched from MySQL to PostgreSQL, you can\u2019t use mysql.connector anymore. You need to use a PostgreSQL driver, like psycopg2 or psycopg. Here\u2019s how you can modify your function:</p> <pre><code>import psycopg2  # PostgreSQL driver\n\ndef establish_connection():\n    conn = psycopg2.connect(\n        host=\"10.34.11.15\",        # your PostgreSQL host\n        port=5432,                  # default PostgreSQL port\n        user=\"root\",                # your PostgreSQL username\n        password=\"root\",            # your PostgreSQL password\n        database=\"powerplant\"       # your PostgreSQL database name\n    )\n    return conn\n</code></pre> <p>Key Changes:</p> <ol> <li>Driver: Use psycopg2 instead of mysql.connector.</li> <li>Port: PostgreSQL default is 5432 (MySQL default is 3306).</li> <li>SQL syntax: Some queries may need slight changes if they use MySQL-specific syntax (e.g., backticks <code>table</code> vs. double quotes \"table\" in PostgreSQL), but most simple SELECT statements are fine.</li> <li>Stored procedures/functions:</li> </ol> <p>In PostgreSQL, your stored procedures are functions, so you\u2019ll call them differently:</p> <pre><code>cur = conn.cursor()\ncur.execute(\"SELECT * FROM get_gt_ied_data();\")  # note the parentheses\nrows = cur.fetchall()\n</code></pre> <p>Excellent \u2014 yes, you\u2019re thinking of <code>psql</code>'s display and output behavior settings, and there is a way to make it show query results, then immediately return to the prompt without odd pauses or output paging.</p> <p>Let\u2019s go through this cleanly \ud83d\udc47</p>"},{"location":"PostgreSQL/#problem","title":"\ud83c\udfaf Problem","text":"<p>When you run a command in <code>psql</code>, sometimes:</p> <ul> <li>It pauses after showing output (especially for long results).</li> <li>Or it pipes output through a pager like <code>less</code>, requiring you to press <code>q</code> to get back.</li> <li>Or you want it to just print everything directly and go back to the prompt.</li> </ul> <p>Excellent \u2014 yes, you\u2019re thinking of psql's display and output behavior settings, and there is a way to make it show query results, then immediately return to the prompt without odd pauses or output paging.</p> <p>Let\u2019s go through this cleanly \ud83d\udc47</p>"},{"location":"PostgreSQL/#solution-disable-the-pager-and-control-display-settings","title":"\u2705 Solution \u2014 Disable the pager and control display settings","text":"<p>You can use these <code>psql</code> internal commands:</p>"},{"location":"PostgreSQL/#1-disable-the-pager-so-output-prints-immediately","title":"1. Disable the pager (so output prints immediately)","text":"<pre><code>\\pset pager off\n</code></pre> <p>This ensures query results are shown directly in the terminal, and you return immediately to the <code>powerplant=#</code> prompt after the output.</p> <p>If you want to re-enable it later:</p> <pre><code>\\pset pager on\n</code></pre> <p>You can check its status:</p> <pre><code>\\pset\n</code></pre>"},{"location":"PostgreSQL/#2-disable-formatting-for-simpler-plain-output","title":"2. Disable formatting for simpler plain output","text":"<p>If you want more compact output (no table borders or alignment):</p> <pre><code>\\pset format unaligned\n\\pset border 0\n</code></pre> <p>Then results will appear like plain text (useful for scripts or logs).</p> <p>Restore normal table output with:</p> <pre><code>\\pset format aligned\n\\pset border 1\n</code></pre>"},{"location":"PostgreSQL/#3-disable-headers-or-footers-optional","title":"3. Disable headers or footers (optional)","text":"<p>If you don\u2019t want to see the <code>(3 rows)</code> line:</p> <pre><code>\\pset footer off\n</code></pre> <p>To bring it back:</p> <pre><code>\\pset footer on\n</code></pre>"},{"location":"PostgreSQL/#4-make-settings-permanent","title":"4. Make settings permanent","text":"<p>To persist these preferences, add them to your <code>~/.psqlrc</code> file:</p> <pre><code>echo '\\pset pager off' &gt;&gt; ~/.psqlrc\necho '\\pset format aligned' &gt;&gt; ~/.psqlrc\necho '\\pset border 1' &gt;&gt; ~/.psqlrc\n</code></pre> <p>Then they\u2019ll apply automatically every time you start <code>psql</code>.</p>"},{"location":"PostgreSQL/#example-session","title":"\ud83e\udde0 Example session","text":"<pre><code>psql -U root -d powerplant\n</code></pre> <p>Inside <code>psql</code>:</p> <pre><code>\\pset pager off\n\\pset format aligned\nselect * from ccp_values limit 5;\n</code></pre> <p>\u27a1 Output prints immediately \u27a1 Cursor returns cleanly to <code>powerplant=#</code></p> <p>Would you like me to suggest an optimal <code>.psqlrc</code> configuration for smooth day-to-day work (no pauses, clean formatting, and instant results)?</p>"},{"location":"Recovering_VM/","title":"Recovering VM","text":""},{"location":"Recovering_VM/#at-grub-shell","title":"At GRUB Shell:","text":"<p>\ud83d\udd27 Steps to boot into text console At the grub&gt; prompt, run these exact commands:</p> <pre><code>set root=(hd0,gpt2)\n\nlinux /vmlinuz root=/dev/ubuntu-vg/ubuntu-lv ro systemd.unit=multi-user.target\n\ninitrd /initrd.img\n\nboot\n</code></pre> <p>\ud83d\udccc Explanation</p> <ul> <li> <p>set root=(hd0,gpt2) \u2192 tells GRUB where /boot is.</p> </li> <li> <p>linux /vmlinuz root=/dev/ubuntu-vg/ubuntu-lv ro systemd.unit=multi-user.target</p> </li> <li> <p>/vmlinuz \u2192 your kernel (symlink to latest: vmlinuz-6.8.0-71-generic).</p> </li> <li> <p>root=/dev/ubuntu-vg/ubuntu-lv \u2192 because your root filesystem is inside LVM (lvm/ubuntu--vg-ubuntu--lv showed up in your ls).</p> </li> <li> <p>systemd.unit=multi-user.target \u2192 boots into text console instead of GUI.</p> </li> <li>initrd /initrd.img \u2192 loads your initramfs.</li> <li>boot \u2192 starts the kernel.</li> </ul> <p>\ud83d\udda5\ufe0f After booting You\u2019ll land in a text login prompt. Login with your username/password. Then fix the problematic services:</p> <pre><code>sudo systemctl disable NetworkManager-wait-online.service\nsudo systemctl mask NetworkManager-wait-online.service\nsudo apt update\nsudo apt install --reinstall gdm3 virtualbox-guest-utils virtualbox-guest-x11 virtualbox-guest-dkms\nsudo systemctl set-default multi-user.target\n</code></pre> <p>Reboot:</p> <pre><code>sudo reboot\n</code></pre> <p>You should now get a stable text login every time. Once everything is fixed, you can switch back to GUI with:</p> <pre><code>sudo systemctl set-default graphical.target\nsudo systemctl isolate graphical.target\n</code></pre>"},{"location":"Recovering_VM/#checking-space-left-on-vm","title":"Checking space left on VM:","text":"<pre><code>\u2191 Share se into data and signature falled \nUbuntu 24.04.2 Clone [HSRG_PLC PPTwin] [Running] \nW: Failed to fetch http://ports.ubuntu.com/ubuntu-ports/dists/noble-updates/InRelease \nError writing to file - write (28: No space left on device) [IP: 185.12 190.36 80] \nW: Failed to fetch http://ports.ubuntu.com/ubuntu-ports/dists/noble-backports/InRelease Error writing to file write (28: No space left on device) [IP: 185. 5.190.36 80] \nW: Failed to fetch http://ports.ubuntu.com/ubuntu-ports/dists/noble-security/InRelease Error writing to file - write (28: No space left on device) [IP: 185.1 .190.36 80] \nW: Some index files failed to download. They have been ignored, or old ones used instead. \nsreedevi@sreedevi:~$ dpkg -1 'linux-image*' | grep ^ii ii \nlinux-image-6.8.0-71-generic ii  6.8.0-71.71 arm64 Signed kernel image generic\nlinux-image-generic 6.8.0-71.71 arm64 Generic Linux kernel image\nsreedevi@sreedevi :~$ sudo apt clean      \nsreedevi@sreedevi:~$ sudo journalctl -vacuum-time=2d \njournalctl: invalid option -- 'v' \nsreedevi@sreedevi :~$ sudo journalctl --vacuum-time=2d \nVacuuming done, freed B of archived journals from /var/log/journal/ef 121b3f41774ea893499e6942bb2ab8. \nVacuuming done, freed B of archived journals from /var/log/journal. \nVacuuming done, freed B of archived journals from /run/log/journal. \nsreedevi@sreedevi:~$ sudo rm -rf /var/lib/apt/lists/* \nsreedevi@sreedevi: \"$ df -h \nFilesystem Size Used Avail Use% Mounted on \ntmpfs 391M 1.3M 389M 1% /run \nefivarfs 256K 54K 203K 21% /sys/firmware/efi/efivars \n/dev/ubuntu-vg/ubuntu-lv 16G 15G \u00d8 100% / \ntmpfs 2.0G \u04e8 2.0G 0% /dev/shm\ntmpfs 5.0M 8.0K 5.0M 1% /run/lock \n/dev/sda2 2.0G 106M 1.7G 6% /boot \n/dev/sda1 1.1G 6.4M 1.1G 1% /boot/efi \nPOWER PLANT_TWIN 927G 421G 506G 46% /media/sf_POWER_PLANT_TWIN \nubuntu24_shared_folder 927G 421G 506G 46% /media/sf_ubuntu24_shared_folder \ntmpfs 391M 88K 390M 1% /run/user/1000 \nsreedevi@sreedevi:~$\n</code></pre> <ul> <li>Your root filesystem is: /dev/ubuntu-vg/ubuntu-lv   16G   15G   0   100%   / So your root volume is 16 GB and completely full. That\u2019s why apt fails and the VM hangs. Your /boot and /boot/efi partitions have space, but they don\u2019t help since all system packages/logs live under /.</li> </ul>"},{"location":"Recovering_VM/#free-some-emergency-space","title":"Free some emergency space","text":"<p>Just enough so apt and the system can breathe:</p> <pre><code>\n# Clear apt cache (already done, but run again just in case)\nsudo apt clean\n\n# Remove old logs\nsudo rm -rf /var/log/journal/*\n\n# Truncate big logs if they exist\nsudo truncate -s 0 /var/log/*.log\nsudo truncate -s 0 /var/log/*/*.log\n</code></pre>"},{"location":"Recovering_VM/#check-if-there-is-free-space-in-the-volume-group","title":"Check if there is free space in the volume group","text":"<pre><code>sreedevi@sreedevi:~$ sudo vgdisplay ubuntu-vg \n\nVolume group \n\nVG Name ubuntu-vg \nSystem ID \nFormat 1vm2 \nMetadata Areas 1 \nMetadata Sequence No 2 \nVG Access read/write \nVG Status resizable \nMAX LV \u04e8 \nCur LV 1 \nOpen LV 1 \nMax PV \u04e8 \nCur PV 1 \nAct PV 1 \nVG Size &lt;31.95 GiB \nPE Size 4.00 MiB \nTotal PE 8178 \nAlloc PE Size 4089 / 15.97 GiB \nFree PE Size 4089/15.97 GiB \nVG UUID TkZXdq-3dSV-izeB-Ewns-UJJn-1VaN-FLMNwc \nsreedevi@sreedevi:~$\n</code></pre> <p>This means the volume group <code>ubuntu-vg</code> is ~32 GB total, but only 16 GB is allocated to <code>/</code> (<code>/dev/ubuntu-vg/ubuntu-lv</code>). So this means you already have ~16GB free inside the VG ready to use - you don't even need to resize the VirtualBox disk.</p>"},{"location":"Recovering_VM/#expand-your-root-filesystem","title":"Expand your root filesystem","text":"<p>Run these 2 commmands:</p> <pre><code># Extend root LV to use all free space\nsudo lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv\n\n# Resize filesystem to fill LV\nsudo resize2fs /dev/ubuntu-vg/ubuntu-lv\n</code></pre> <pre><code>sreedevi@sreedevi:~$ sudo lvextend -1 +100%FREE /dev/ubuntu-vg/ubuntu-lv \n[sudo] password for sreedevi: \nSize of logical volume ubuntu-vg/ubuntu-lv changed from 15.97 GiB (4089 extents) to &lt;31.95 GiB (8178 extents). \nLogical volume ubuntu-vg/ubuntu-lv successfully resized. \nsreedevi@sreedevi :~$ sudo resize2fs /dev/ubuntu-vg/ubuntu-lv resize2fs 1.47.0 (5-Feb-2023) \nFilesystem at /dev/ubuntu-vg/ubuntu-ly is mounted on ; on-line resizing required old_desc_blocks = 2, new_desc_blocks = 4 \nThe filesystem on /dev/ubuntu-vg/ubuntu-lv is now 8374272 (4k) blocks long. \nsreedevi@sreedevi: df -h \nFilesystem Size Used Avail Use% Mounted on \ntmpfs 391M 1.3M 389M 1% /run \nefivarfs 256K 54K 203K 21% /sys/firmware/efi/efivars \n/dev/ubuntu-vg/ubuntu-lv 32G 15G 16G 50% / \ntmpfs 2.0G \u04e8 2.0G 0% /dev/shm \ntmpfs 5.0\u043c 8.\u043e\u043a 5.0\u043c 1% /run/lock \n/dev/sda2 2.0G 106M 1.7G 6% /boot \n/dev/sda1 1.1G 6.4M 1.1G 1% /boot/efi \nPOWER PLANT_TWIN 927G 422G 506G 46% /media/sf_POWER_PLANT_TWIN \nubuntu24_shared_folder 927G 422G 506G 46% /media/sf_ubuntu24_shared_folder \ntmpfs 391M 88K 390M 1% /run/user/1000 \nsreedevi@sreedevi:~$\n</code></pre> <p>issue faced when trying steps </p> <pre><code>sreedevi@sreedevi :~$ sudo apt install --reinstall gdm3 virtualbox-guest-utils virtualbox-guest-x11 virtualbox-guest-dkms. \nReading package lists... \nDone Building dependency tree... \nDone Reading state information... \nDone E: Unable to locate package virtualbox-guest-utils \nE: Unable to locate package virtualbox-guest-x11 \nE: Unable to locate package virtualbox-guest-dkms \nsreedevi@sreedevi: \"$ \n</code></pre> <p>Steps to fix boot delay from \"waiting for network online\" and reconfigure GDM for next boot: </p> <p>sudo systemctl disable NetworkManager-wait-online.service  sudo systemctl mask NetworkManager-wait-online.service  sudo apt update  sudo apt install spice-vdagent virtualbox-guest-additions-iso sudo systemctl set-default multi-user.target </p> <p>sudo systemctl set-default graphical.target # switching to GUI</p> <p>Prompt from me: After Select Advanced options for Ubuntu \u2192 recovery mode. Choose Root \u2013 Drop to root shell prompt. the prompt says enter for maintenance and press control D to continue after I pressed control d I see <code>root@sreedevi</code> how should I proceed to do these : sudo systemctl disable NetworkManager-wait-online.service sudo systemctl mask NetworkManager-wait-online.service sudo apt update sudo apt install virtualbox-guest-utils virtualbox-guest-x11 virtualbox-guest-dkms sudo dpkg-reconfigure gdm3</p> <p>If root filesystem is mounted as read-only : <code>mount -o remount,rw /</code></p> <p>After Select Advanced options for Ubuntu -&gt; recovery mode </p>"},{"location":"Remote_PortForwarding_SSH/","title":"Remote Port Fowarding SSH","text":"<p>The ubuntu vm is on a Remote Desktop, docker container is running on the ubuntu vm which is on the Remote Desktop, and I want to be able to access the docker container from my own host machine, not on the Remote Desktop, are there additional steps to be done if I want to access the plc browser of the docker container</p> <p>\ud83e\udde9 Your Current Setup (summarized)</p> <p>Let\u2019s restate the network chain: Your Laptop (Host)    \u2193 Remote Desktop Server (Hypervisor or another host)    \u2193 Ubuntu VM (IP: 192.168.30.60)    \u2193 Docker Container (OpenPLC, IP: 192.168.30.201 on macvlan)</p> <p>You already did: The macvlan network setup \u2705 The container is running and reachable from inside the Ubuntu VM \u2705 (ping -I macvlan0 192.168.30.201)</p> <p>The OpenPLC web UI runs on port 8080 inside the container \u2705 (default)</p> <p>Now the problem: You cannot reach http://192.168.30.201:8080 from your own laptop, only from within the Ubuntu VM. \ud83e\udde0 Why This Happens</p> <p>A macvlan network gives containers their own MAC addresses and connects them directly to the physical network \u2014 but only within the same Layer 2 domain (LAN). If your laptop and the Ubuntu VM\u2019s network (192.168.30.x) are not on the same LAN, or if the Remote Desktop host doesn\u2019t bridge your VM\u2019s NIC properly, your laptop won\u2019t be able to reach that container\u2019s IP directly.</p> <p>So let\u2019s go through possible solutions.</p> <p>\ud83d\udee0\ufe0f Option 1: Access via Port Forwarding (simplest &amp; most reliable) You can forward the container\u2019s port (8080) to the Ubuntu VM\u2019s IP, which you already can reach through Remote Desktop.</p> <p>\ud83e\ude9c Steps: 1\ufe0f\u20e3 Stop and remove the container that\u2019s using macvlan: docker stop st_plc docker rm st_plc</p> <p>2\ufe0f\u20e3 Run it again using bridge network (not macvlan), and expose port 8080: docker run -d --name st_plc -p 8080:8080 --privileged openplc:v3</p> <p>3\ufe0f\u20e3 Check it\u2019s running: docker ps</p> <p>4\ufe0f\u20e3 From your laptop, open: http://:8080 <p>Example: http://192.168.30.60:8080</p> <p>\u2705 You\u2019ll see the OpenPLC web interface.</p> <p>\ud83d\udca1 Why this works</p> <p>The port mapping (-p 8080:8080) tells Docker:</p> <p>\u201cForward traffic from the VM\u2019s port 8080 \u2192 to the container\u2019s port 8080.\u201d</p> <p>Your laptop can reach the Ubuntu VM\u2019s IP.</p> <p>So you can reach the container UI directly.</p> <p>\ud83e\uddf1 Option 2: Keep macvlan and bridge traffic from outside (advanced)</p> <p>If you must keep macvlan (for PLC simulation realism or LAN integration), then your VM\u2019s NIC must be in the same broadcast domain as your laptop.</p> <p>That means:</p> <p>The 192.168.30.x network of your container and VM must be accessible from your laptop\u2019s network.</p> <p>If your Remote Desktop VM is NATed, your laptop can\u2019t reach it directly \u2014 you\u2019ll need to set up a route or bridge on the hypervisor level.</p> <p>For example, on the Remote Desktop host (if you control it):</p> <p>sudo ip route add 192.168.30.0/24 via  <p>And ensure the hypervisor\u2019s virtual switch is in bridged mode (not NAT).</p> <p>But this setup requires admin access to the Remote Desktop host \u2014 not always possible.</p> <p>\u2699\ufe0f Option 3: SSH Tunnel (secure workaround)</p> <p>If you cannot change network bridging but can SSH into the Ubuntu VM, you can tunnel the OpenPLC port to your local machine.</p> <p>\ud83e\ude9c Steps from your laptop:</p> <p><code>ssh -L 8080:192.168.30.201:8080 ubuntu@&lt;Ubuntu_VM_IP&gt;</code></p> <p>Then, on your laptop, visit:</p> <p>http://localhost:8080</p> <p>\u2705 SSH securely forwards port 8080 from your local laptop to the container inside the VM, even through NAT or Remote Desktop.</p> <p>\ud83e\udde9 Summary Table Goal    Method  Pros    Cons</p> <p>\u2705 Access from your laptop easily    Port forwarding (-p 8080:8080)  Easiest, works anywhere Container no longer uses macvlan</p> <p>\u2699\ufe0f Keep macvlan realism Bridge networking on host   Same subnet realism Needs admin control on Remote Desktop host</p> <p>\ud83d\udd12 Secure remote access  SSH tunnel  No reconfiguration, encrypted   Needs SSH access every time</p> <p>\ud83d\ude80 Recommended for You</p> <p>Since your VM is inside a Remote Desktop environment, the simplest and most stable method is: \u27a4 Use Option 1 (Port Forwarding):</p> <p><code>docker run -d --name st_plc -p 8080:8080 --privileged openplc:v3</code></p> <p>Then access from your host browser:</p> <p><code>http://&lt;Ubuntu_VM_IP&gt;:8080</code></p> <p>That\u2019ll reliably work regardless of NAT or RDP network complexity.</p> <p>Would you like me to show how you can keep both macvlan and port forwarding simultaneously \u2014 so you can test PLC communications and still access the UI from your host machine?</p>"},{"location":"Remote_PortForwarding_SSH/#question","title":"Question","text":"<p>I have multiple plc containers running on different ip addresses on the docker macvlan network  If I use localhost to access it will not be possible to access different containers on the browser separately so I need to use the ip address to be unique and use that to access the plc runtime </p> <p>ssh -L 8080:192.168.30.201:8080 ubuntu@192.168.30.60</p> <p>http://localhost:8080</p>"},{"location":"Remote_PortForwarding_SSH/#answer","title":"Answer","text":"<p>Excellent observation \u2014 you\u2019re absolutely right \u2705</p> <p>If you have multiple OpenPLC containers, each running on a different IP (for example 192.168.30.201, 192.168.30.202, 192.168.30.203, etc.) in your Docker macvlan network, then mapping them all to localhost:8080 would overlap \u2014 only one tunnel could use that local port at a time. Let\u2019s go over how to handle this cleanly and flexibly.</p> <p>\ud83e\udde0 Why localhost won\u2019t scale</p> <p>If you tunnel all of them like:</p> <p><code>ssh -L 8080:192.168.30.201:8080 ubuntu@192.168.30.60</code></p> <p>then you\u2019d always have to reuse localhost:8080 \u2014 so only one can be accessed at once.</p> <p>You need different local ports for each tunnel. \u2699\ufe0f The Correct Way (multiple tunnels, unique local ports) Use different local ports per container:</p>"},{"location":"Remote_PortForwarding_SSH/#for-first-plc","title":"For first PLC","text":"<p>ssh -L 8081:192.168.30.201:8080 ubuntu@192.168.30.60</p>"},{"location":"Remote_PortForwarding_SSH/#for-second-plc","title":"For second PLC","text":"<p>ssh -L 8082:192.168.30.202:8080 ubuntu@192.168.30.60</p>"},{"location":"Remote_PortForwarding_SSH/#for-third-plc","title":"For third PLC","text":"<p>ssh -L 8083:192.168.30.203:8080 ubuntu@192.168.30.60</p> <p>Then on your host browser, you can open: PLC Browser URL PLC #1  http://localhost:8081 PLC #2  http://localhost:8082 PLC #3  http://localhost:8083 Each one will map to a different container, through the SSH tunnel.</p> <p>\ud83e\udde0 What\u2019s happening behind the scenes</p> <p>SSH on your host forwards:</p> <p>localhost:8081 \u2192 (SSH tunnel) \u2192 192.168.30.201:8080 localhost:8082 \u2192 (SSH tunnel) \u2192 192.168.30.202:8080 localhost:8083 \u2192 (SSH tunnel) \u2192 192.168.30.203:8080</p> <p>over the single SSH connection to your Ubuntu VM.</p> <p>\ud83d\ude80 Optional: Single SSH session, multiple tunnels Instead of opening multiple SSH windows, you can define multiple -L options in one command: ssh \\ -L 8081:192.168.30.201:8080 \\ -L 8082:192.168.30.202:8080 \\ -L 8083:192.168.30.203:8080 \\ ubuntu@192.168.30.60</p> <p>Then open all three PLC interfaces from your host browser:</p> <p>http://localhost:8081 http://localhost:8082 http://localhost:8083</p> <p>\ud83e\udde9 Summary Goal    Command Browser URL Access PLC 1    ssh -L 8081:192.168.30.201:8080 ubuntu@192.168.30.60    http://localhost:8081 Access PLC 2    ssh -L 8082:192.168.30.202:8080 ubuntu@192.168.30.60    http://localhost:8082 Access PLC 3    ssh -L 8083:192.168.30.203:8080 ubuntu@192.168.30.60    http://localhost:8083 Would you like me to show you how to automate this SSH tunneling (e.g. using an .ssh/config file or startup script) so you don\u2019t need to type all these each time?</p>"},{"location":"SQL/","title":"SQL","text":"<p>To check if a column exists in table ccp_values;</p> <p><code>SHOW COLUMNS FROM ccp_values</code></p> <p>After confirming a column exists, to see all values from the column eg. st_steam_mass_flow_rate from the table:</p> <p><code>SELECT st_steam_mass_flow_rate FROM ccp_values;</code> </p> <p>To see recent values eg. last 10 rows only:</p> <p><code>SELECT timestamp, st_steam_mass_flow_rate FROM ccp_values ORDER BY timestamp, DESC LIMIT 10;</code></p>"},{"location":"SSH/","title":"SSH","text":"<p>keypair was created using command like below on windows :</p> <p>ssh-keygen-t ed25519 ...</p> <p>had to convert the private key that was generated on windows using dos2unix as the line endings of the text file generated on windows will be based on carriage return  can check using cat -v ~/.ssh/id_ed25519_private If you see ^M at the end of lines \u2192 that\u2019s a Windows CR (\\r).</p> <p>command to ssh to ncl node 10.13 this does not work (as need to create a config file ): ssh -i ~/.ssh/id_ed25519_private -J ADSC_SmartGrid_5@gateway.ncl.sg -i ~/.ssh/id_ed25519_private pgt@10.10.10.13</p> <p>Create config file first</p> <pre><code># Gateway host\nHost gateway\n    HostName gateway.ncl.sg\n    User ADSC_SmartGrid_5\n    IdentityFile ~/.ssh/id_ed25519_private\n\n# Intermediate host\nHost intermediate\n    HostName 172.18.178.17\n    User pgt\n    Port 4696\n    IdentityFile ~/.ssh/id_ed25519_private\n    ProxyJump gateway\n\n# Final internal host\nHost pgt10.13\n    HostName 10.10.10.13\n    User pgt\n    IdentityFile ~/.ssh/id_ed25519_private\n    ProxyJump intermediate\n\n</code></pre> <p><code>chmod 600 ~/.ssh/config</code> <code>ssh -v pgt10.13</code></p>"},{"location":"ScadaLTS/","title":"ScadaLTS","text":""},{"location":"ScadaLTS/#content-of-docker-composeyml","title":"Content of docker-compose.yml","text":"<p>For Powerplant Twin</p> <pre><code>version: '3.8'\nservices:\n  database:\n    container_name: mysql\n    image: mysql/mysql-server:5.7\n    environment:\n      - MYSQL_ROOT_PASSWORD=root\n      - MYSQL_USER=root\n      - MYSQL_PASSWORD=root\n      - MYSQL_DATABASE=scadalts\n    ports:\n      - \"3307:3306\"\n    volumes:\n      - mysql-data:/var/lib/mysql\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\", \"-uroot\", \"-proot\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n      start_period: 20s\n\n  scadalts:\n    container_name: scadalts\n    image: scadalts/scadalts:latest\n    depends_on:\n      database:\n        condition: service_healthy\n    ports:\n      - \"8080:8080\"\n    expose:\n      - \"8000\"\n\nvolumes:\n  mysql-data:\n</code></pre>"},{"location":"Systemctl_Systemd/","title":"Systemctl Systemd","text":"<p>Excellent question \u2014 this is a network/DNS resolution issue, not a Docker configuration problem. Let\u2019s break down what\u2019s happening and how to fix it properly.</p>"},{"location":"Systemctl_Systemd/#what-the-error-means","title":"\ud83e\udde9 What the error means","text":"<pre><code>Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 127.0.0.53:53: read udp 127.0.0.1:46176-&gt;127.0.0.53:53: i/o timeout\n</code></pre> <p>Explanation:</p> <ul> <li>Docker is trying to pull the image from Docker Hub (<code>registry-1.docker.io</code>).</li> <li>It\u2019s using the system DNS resolver (<code>127.0.0.53</code>, which is <code>systemd-resolved</code>).</li> <li>The lookup request timed out, meaning the VM can\u2019t resolve DNS names (even though it may have internet connectivity, e.g., <code>ping 8.8.8.8</code> works).</li> </ul>"},{"location":"Systemctl_Systemd/#step-1-test-dns-resolution-on-your-ubuntu-vm","title":"\u2705 Step 1: Test DNS resolution on your Ubuntu VM","text":"<p>Run:</p> <pre><code>ping -c 3 google.com\n</code></pre> <p>If it fails with \u201cTemporary failure in name resolution,\u201d DNS is the issue.</p>"},{"location":"Systemctl_Systemd/#step-2-fix-your-etcresolvconf","title":"\u2705 Step 2: Fix your <code>/etc/resolv.conf</code>","text":"<p>Run this command to check the current content:</p> <pre><code>cat /etc/resolv.conf\n</code></pre> <p>You\u2019ll likely see something like:</p> <pre><code>nameserver 127.0.0.53\noptions edns0 trust-ad\nsearch ...\n</code></pre> <p>That\u2019s not helpful for Docker containers when DNS forwarding breaks.</p> <p>Replace it with public DNS servers:</p> <pre><code>sudo bash -c 'cat &gt; /etc/resolv.conf &lt;&lt;EOF\nnameserver 8.8.8.8\nnameserver 1.1.1.1\nEOF'\n</code></pre>"},{"location":"Systemctl_Systemd/#step-3-restart-networking-and-docker","title":"\u2705 Step 3: Restart networking and Docker","text":"<pre><code>sudo systemctl restart systemd-resolved\nsudo systemctl restart docker\n</code></pre> <p>Then try again:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"Systemctl_Systemd/#optional-persist-dns-changes-if-using-cloud-init-or-netplan","title":"\u2699\ufe0f Optional: Persist DNS changes (if using cloud-init or Netplan)","text":"<p>Since your <code>/etc/netplan/50-cloud-init.yaml</code> file had:</p> <pre><code>nameservers:\n  addresses:\n    - 127.0.0.53\n    - 8.8.8.8\n    - 8.8.4.4\n</code></pre> <p>Update it to remove 127.0.0.53, like this:</p> <pre><code>nameservers:\n  addresses:\n    - 8.8.8.8\n    - 8.8.4.4\n</code></pre> <p>Then apply the changes:</p> <pre><code>sudo netplan apply\n</code></pre>"},{"location":"Systemctl_Systemd/#why-this-fixes-it","title":"\ud83e\udde0 Why this fixes it","text":"<p>Docker relies on the host\u2019s DNS resolution to fetch images. When Ubuntu uses <code>systemd-resolved</code> (127.0.0.53) and that resolver breaks (common on VMs), Docker can\u2019t resolve <code>registry-1.docker.io</code>. Setting <code>/etc/resolv.conf</code> to external DNS servers like Google or Cloudflare bypasses that problem.</p> <p>Would you like me to give you a version of this fix that persists across reboots (since cloud-init can overwrite <code>/etc/resolv.conf</code> each time)?</p> <p>Perfect \u2014 that\u2019s a very common situation on Ubuntu 20.04+ and 22.04 systems. You\u2019re correct not to directly edit <code>/etc/resolv.conf</code> because it\u2019s a symlink managed by <code>systemd-resolved</code>, not a static file.</p> <p>Here\u2019s exactly what you should do \ud83d\udc47</p>"},{"location":"Systemctl_Systemd/#step-by-step-fix-for-dns-resolution-recommended-way","title":"\ud83e\udded Step-by-step fix for DNS resolution (recommended way)","text":""},{"location":"Systemctl_Systemd/#step-1-confirm-the-symlink","title":"\ud83e\udde9 Step 1: Confirm the symlink","text":"<p>Run:</p> <pre><code>ls -l /etc/resolv.conf\n</code></pre> <p>You\u2019ll likely see something like:</p> <pre><code>/etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf\n</code></pre> <p>This confirms <code>systemd-resolved</code> is managing it.</p>"},{"location":"Systemctl_Systemd/#step-2-tell-systemd-resolved-to-use-public-dns-servers","title":"\ud83e\udde9 Step 2: Tell systemd-resolved to use public DNS servers","text":"<p>Create or edit this file:</p> <pre><code>sudo nano /etc/systemd/resolved.conf\n</code></pre> <p>Find or add these lines under <code>[Resolve]</code>:</p> <pre><code>[Resolve]\nDNS=8.8.8.8 8.8.4.4\nFallbackDNS=1.1.1.1\nDNSStubListener=no\n</code></pre> <p>\ud83d\udca1 The <code>DNSStubListener=no</code> line disables the local 127.0.0.53 stub that\u2019s currently breaking Docker lookups.</p> <p>Save and exit (<code>Ctrl+O</code>, <code>Enter</code>, then <code>Ctrl+X</code>).</p>"},{"location":"Systemctl_Systemd/#step-3-point-etcresolvconf-to-the-real-resolv-file","title":"\ud83e\udde9 Step 3: Point <code>/etc/resolv.conf</code> to the real resolv file","text":"<p>Replace the current symlink:</p> <pre><code>sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf\n</code></pre> <p>This file (<code>/run/systemd/resolve/resolv.conf</code>) contains the actual upstream DNS servers rather than the local stub.</p>"},{"location":"Systemctl_Systemd/#step-4-restart-systemd-resolved","title":"\ud83e\udde9 Step 4: Restart systemd-resolved","text":"<pre><code>sudo systemctl restart systemd-resolved\n</code></pre> <p>Then verify:</p> <pre><code>resolvectl status\n</code></pre> <p>You should now see:</p> <pre><code>DNS Servers: 8.8.8.8 8.8.4.4 1.1.1.1\n</code></pre>"},{"location":"Systemctl_Systemd/#step-5-test-dns-resolution","title":"\ud83e\udde9 Step 5: Test DNS resolution","text":"<pre><code>ping -c 3 google.com\n</code></pre> <p>If that works, try:</p> <pre><code>docker compose up -d\n</code></pre> <p>It should now be able to pull from Docker Hub successfully.</p>"},{"location":"Systemctl_Systemd/#why-this-method-is-best","title":"\ud83e\udde0 Why this method is best","text":"<p>This approach:</p> <ul> <li>Keeps <code>systemd-resolved</code> in control (so no conflicts with Ubuntu updates).</li> <li>Permanently replaces the stub DNS (127.0.0.53) with direct DNS servers.</li> <li>Persists across reboots and cloud-init updates.</li> </ul> <p>Would you like me to show you the persistent fix that also survives a full VM reboot or cloud-init regeneration (so it\u2019s never reset back to 127.0.0.53)?</p>"},{"location":"Systemctl_Systemd/#persistent-fix","title":"Persistent Fix","text":"<p>Excellent \u2014 let\u2019s make your DNS fix fully persistent, even across reboots and <code>cloud-init</code> regenerations.</p> <p>Here\u2019s the robust setup that ensures your VM always keeps working DNS (and Docker can always pull images).</p>"},{"location":"Systemctl_Systemd/#goal","title":"\ud83e\udded Goal","text":"<p>Stop Ubuntu\u2019s <code>cloud-init</code> and <code>systemd-resolved</code> from resetting <code>/etc/resolv.conf</code> to <code>127.0.0.53</code> \u2705 Survives reboots \u2705 Survives <code>cloud-init</code> reconfiguration \u2705 Keeps Docker name resolution reliable</p>"},{"location":"Systemctl_Systemd/#step-1-disable-cloud-init-network-management-optional-but-recommended","title":"\ud83d\udd27 Step 1: Disable <code>cloud-init</code> network management (optional but recommended)","text":"<p>Cloud-init regenerates <code>/etc/netplan/50-cloud-init.yaml</code> and resets DNS. To prevent that:</p> <pre><code>sudo mkdir -p /etc/cloud/cloud.cfg.d/\necho \"network: {config: disabled}\" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n</code></pre> <p>This stops cloud-init from overwriting your network configuration or DNS settings.</p>"},{"location":"Systemctl_Systemd/#step-2-configure-permanent-dns-in-systemd-resolved","title":"\ud83d\udd27 Step 2: Configure permanent DNS in <code>systemd-resolved</code>","text":"<p>Edit the config file:</p> <pre><code>sudo nano /etc/systemd/resolved.conf\n</code></pre> <p>Update it to include your preferred DNS servers:</p> <pre><code>[Resolve]\nDNS=8.8.8.8 8.8.4.4\nFallbackDNS=1.1.1.1\nDNSStubListener=no\n</code></pre> <p>Save and exit (<code>Ctrl+O</code>, <code>Enter</code>, <code>Ctrl+X</code>).</p>"},{"location":"Systemctl_Systemd/#step-3-replace-etcresolvconf-with-the-real-resolver-file","title":"\ud83d\udd27 Step 3: Replace <code>/etc/resolv.conf</code> with the real resolver file","text":"<p>Right now <code>/etc/resolv.conf</code> likely points to:</p> <pre><code>/run/systemd/resolve/stub-resolv.conf\n</code></pre> <p>\u2026which uses the stub <code>127.0.0.53</code> listener.</p> <p>Replace it permanently:</p> <pre><code>sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf\n</code></pre> <p>That file (<code>/run/systemd/resolve/resolv.conf</code>) always reflects the real upstream DNS servers (like 8.8.8.8).</p>"},{"location":"Systemctl_Systemd/#step-4-restart-services","title":"\ud83d\udd27 Step 4: Restart services","text":"<pre><code>sudo systemctl restart systemd-resolved\n</code></pre> <p>Then check status:</p> <pre><code>resolvectl status\n</code></pre> <p>You should now see:</p> <pre><code>DNS Servers: 8.8.8.8 8.8.4.4 1.1.1.1\nDNS Stub Listener: no\n</code></pre>"},{"location":"Systemctl_Systemd/#step-5-verify-dns","title":"\ud83d\udd27 Step 5: Verify DNS","text":"<p>Run:</p> <pre><code>ping -c 3 google.com\n</code></pre> <p>and:</p> <pre><code>docker pull hello-world\n</code></pre> <p>Both should now succeed instantly.</p>"},{"location":"Systemctl_Systemd/#why-this-is-persistent-and-safe","title":"\ud83e\udde0 Why this is persistent and safe","text":"Component Problem What we did <code>cloud-init</code> Rewrites network config Disabled network reconfiguration <code>systemd-resolved</code> Uses stub resolver (127.0.0.53) Pointed it to public DNS directly <code>/etc/resolv.conf</code> Symlinked to stub file Re-linked to the real resolver Reboot resets Temporary files regenerated <code>/etc/systemd/resolved.conf</code> persists"},{"location":"Systemctl_Systemd/#optional-make-sure-etcnetplan-matches-your-setup","title":"\ud83e\udde9 Optional: make sure <code>/etc/netplan</code> matches your setup","text":"<p>If your <code>/etc/netplan/50-cloud-init.yaml</code> defines DNS like:</p> <pre><code>nameservers:\n  addresses:\n    - 127.0.0.53\n</code></pre> <p>You can safely change it to:</p> <pre><code>nameservers:\n  addresses:\n    - 8.8.8.8\n    - 8.8.4.4\n</code></pre> <p>Then apply it:</p> <pre><code>sudo netplan apply\n</code></pre> <p>\u2705 After doing all this: Your DNS resolution (and Docker access) will remain correct permanently \u2014 even after reboot, <code>apt upgrade</code>, or VM image regeneration.</p> <p>Would you like me to show how to verify after reboot that your changes persist (a quick 2-command check)?</p>"},{"location":"Systemctl_Systemd/#persistent-dns-network-configuration-fix-across-reboots-and-cloud-init-resets","title":"Persistent DNS + network configuration fix across reboots and cloud-init resets.","text":"<p>''' explain each of these commands :</p> <p>sudo mkdir -p /etc/cloud/cloud.cfg.d/ echo \"network: {config: disabled}\" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg</p> <p>this is the content of /etc/netplan/50-cloud-init.yaml , and there is a comment that says any changes will not persist across an instance reboot  , what should I do to persist changes on this file too? summarise and consolidate all changes that I need to do to resolve the error here , to persist and to verify it after reboot </p> <p>ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ sudo cat /etc/netplan/50-cloud-init.yaml  [sudo] password for ubuntu: </p>"},{"location":"Systemctl_Systemd/#this-file-is-generated-from-information-provided-by-the-datasource-changes","title":"This file is generated from information provided by the datasource.  Changes","text":""},{"location":"Systemctl_Systemd/#to-it-will-not-persist-across-an-instance-reboot-to-disable-cloud-inits","title":"to it will not persist across an instance reboot.  To disable cloud-init's","text":""},{"location":"Systemctl_Systemd/#network-configuration-capabilities-write-a-file","title":"network configuration capabilities, write a file","text":""},{"location":"Systemctl_Systemd/#etccloudcloudcfgd99-disable-network-configcfg-with-the-following","title":"/etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:","text":""},{"location":"Systemctl_Systemd/#network-config-disabled","title":"network: {config: disabled}","text":"<p>network:     ethernets:         enp0s3:             dhcp4: false             addresses:               - 192.168.30.60/24             nameservers:               addresses:                 - 127.0.0.53                 - 8.8.8.8                 - 8.8.4.4             routes:               - to: default                 via: 192.168.30.1     version: 2 ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ </p> <p>drwxr-xr-x 6 ubuntu ubuntu 4096 Oct 21 02:20 ccp_multi_IED_multi_plc_database_pg ubuntu@pgt:~$ cd ccp_multi_IED_multi_plc_database_pg/ ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ docker compose up -d [+] Running 1/1  \u2718 postgres Error Get \"https://registry-1.docker.io/v2/...                10.2s  Error response from daemon: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 127.0.0.53:53: read udp 127.0.0.1:46176-&gt;127.0.0.53:53: i/o timeout ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ ping -c 3 google.com ping: google.com: Temporary failure in name resolution ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ ping -c 3 google.com ping: google.com: Temporary failure in name resolution ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ cat /etc/resolv.conf</p>"},{"location":"Systemctl_Systemd/#this-is-runsystemdresolvestub-resolvconf-managed-by-mansystemd-resolved8","title":"This is /run/systemd/resolve/stub-resolv.conf managed by man:systemd-resolved(8).","text":""},{"location":"Systemctl_Systemd/#do-not-edit","title":"Do not edit.","text":""},{"location":"Systemctl_Systemd/#_1","title":"Systemctl Systemd","text":""},{"location":"Systemctl_Systemd/#this-file-might-be-symlinked-as-etcresolvconf-if-youre-looking-at","title":"This file might be symlinked as /etc/resolv.conf. If you're looking at","text":""},{"location":"Systemctl_Systemd/#etcresolvconf-and-seeing-this-text-you-have-followed-the-symlink","title":"/etc/resolv.conf and seeing this text, you have followed the symlink.","text":""},{"location":"Systemctl_Systemd/#_2","title":"Systemctl Systemd","text":""},{"location":"Systemctl_Systemd/#this-is-a-dynamic-resolvconf-file-for-connecting-local-clients-to-the","title":"This is a dynamic resolv.conf file for connecting local clients to the","text":""},{"location":"Systemctl_Systemd/#internal-dns-stub-resolver-of-systemd-resolved-this-file-lists-all","title":"internal DNS stub resolver of systemd-resolved. This file lists all","text":""},{"location":"Systemctl_Systemd/#configured-search-domains","title":"configured search domains.","text":""},{"location":"Systemctl_Systemd/#_3","title":"Systemctl Systemd","text":""},{"location":"Systemctl_Systemd/#run-resolvectl-status-to-see-details-about-the-uplink-dns-servers","title":"Run \"resolvectl status\" to see details about the uplink DNS servers","text":""},{"location":"Systemctl_Systemd/#currently-in-use","title":"currently in use.","text":""},{"location":"Systemctl_Systemd/#_4","title":"Systemctl Systemd","text":""},{"location":"Systemctl_Systemd/#third-party-programs-should-typically-not-access-this-file-directly-but-only","title":"Third party programs should typically not access this file directly, but only","text":""},{"location":"Systemctl_Systemd/#through-the-symlink-at-etcresolvconf-to-manage-manresolvconf5-in-a","title":"through the symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a","text":""},{"location":"Systemctl_Systemd/#different-way-replace-this-symlink-by-a-static-file-or-a-different-symlink","title":"different way, replace this symlink by a static file or a different symlink.","text":""},{"location":"Systemctl_Systemd/#_5","title":"Systemctl Systemd","text":""},{"location":"Systemctl_Systemd/#see-mansystemd-resolvedservice8-for-details-about-the-supported-modes-of","title":"See man:systemd-resolved.service(8) for details about the supported modes of","text":""},{"location":"Systemctl_Systemd/#operation-for-etcresolvconf","title":"operation for /etc/resolv.conf.","text":"<p>nameserver 127.0.0.53 options edns0 trust-ad search . ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ ls -l /etc/resolv.conf lrwxrwxrwx 1 root root 39 Sep 11  2024 /etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf ubuntu@pgt:~/ccp_multi_IED_multi_plc_database_pg$ ls /etc/netplan/ 50-cloud-init.yaml content of file /etc/netplan/50-cloud-init.yaml:^C '''</p> <p>Perfect \u2014 you\u2019ve given me everything I need. Let\u2019s walk through what\u2019s happening, what those commands do, and then I\u2019ll give you a complete, persistent DNS + network configuration fix that survives reboots and cloud-init resets.</p>"},{"location":"Systemctl_Systemd/#whats-happening","title":"\ud83e\udde9 What\u2019s happening","text":"<ul> <li>Your VM\u2019s <code>/etc/resolv.conf</code> is symlinked to a dynamically managed file:</li> </ul> <p><code>/etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf</code></p> <p>This is controlled by systemd-resolved.   So even if you edit it manually, it gets overwritten after reboot or service restart.</p> <ul> <li>The file <code>/etc/netplan/50-cloud-init.yaml</code> is auto-generated by cloud-init.   The comment:</li> </ul> <p>\u201cChanges to it will not persist across an instance reboot\u201d means cloud-init rewrites it every time the VM boots.</p> <ul> <li>Your DNS (<code>127.0.0.53</code>) is a local DNS stub resolver, which is failing to resolve queries.</li> </ul>"},{"location":"Systemctl_Systemd/#step-1-explanation-of-the-commands-you-mentioned","title":"\ud83e\udde0 Step 1: Explanation of the commands you mentioned","text":"<pre><code>sudo mkdir -p /etc/cloud/cloud.cfg.d/\n</code></pre> <ul> <li>Creates the directory <code>/etc/cloud/cloud.cfg.d/</code> if it doesn\u2019t exist.</li> <li>This directory stores override configurations for cloud-init.</li> </ul> <pre><code>echo \"network: {config: disabled}\" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n</code></pre> <ul> <li>This disables cloud-init\u2019s automatic network configuration, preventing it from regenerating <code>/etc/netplan/50-cloud-init.yaml</code> at each reboot.</li> <li>Once disabled, you control networking manually via Netplan.</li> </ul>"},{"location":"Systemctl_Systemd/#step-2-apply-persistent-dns-network-fix","title":"\u2699\ufe0f Step 2: Apply persistent DNS + network fix","text":"<p>Follow these steps exactly in order:</p>"},{"location":"Systemctl_Systemd/#1-disable-cloud-inits-network-regeneration","title":"1\ufe0f\u20e3 Disable cloud-init\u2019s network regeneration","text":"<pre><code>sudo mkdir -p /etc/cloud/cloud.cfg.d/\necho \"network: {config: disabled}\" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n</code></pre>"},{"location":"Systemctl_Systemd/#2-create-your-own-persistent-netplan-file","title":"2\ufe0f\u20e3 Create your own persistent netplan file","text":"<p>Create <code>/etc/netplan/01-netcfg.yaml</code> (custom, not cloud-init generated):</p> <pre><code>sudo nano /etc/netplan/01-netcfg.yaml\n</code></pre> <p>Paste the following content:</p> <pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp0s3:\n      dhcp4: no\n      addresses:\n        - 192.168.30.60/24\n      gateway4: 192.168.30.1\n      nameservers:\n        addresses:\n          - 8.8.8.8\n          - 8.8.4.4\n</code></pre> <p>\u2705 Explanation:</p> <ul> <li><code>renderer: networkd</code> ensures systemd-networkd handles the interface.</li> <li>We\u2019ve removed 127.0.0.53 so DNS goes directly to Google\u2019s servers.</li> <li>This file is not managed by cloud-init, so it persists.</li> </ul>"},{"location":"Systemctl_Systemd/#3-apply-and-verify","title":"3\ufe0f\u20e3 Apply and verify","text":"<pre><code>sudo netplan apply\n</code></pre> <p>Then test:</p> <pre><code>ping -c 3 google.com\n</code></pre> <p>If ping works \u2014 DNS is fixed.</p>"},{"location":"Systemctl_Systemd/#step-3-make-etcresolvconf-static-and-persistent","title":"\ud83d\udd01 Step 3: Make <code>/etc/resolv.conf</code> static and persistent","text":"<p>To break the dynamic symlink and use a static resolv.conf:</p> <pre><code>sudo rm /etc/resolv.conf\nsudo bash -c 'cat &gt; /etc/resolv.conf &lt;&lt;EOF\nnameserver 8.8.8.8\nnameserver 8.8.4.4\nEOF'\n</code></pre> <p>Then make sure it isn\u2019t replaced by systemd-resolved:</p> <pre><code>sudo systemctl disable systemd-resolved.service\nsudo systemctl stop systemd-resolved.service\n</code></pre> <p>Now <code>/etc/resolv.conf</code> will remain static and persistent.</p>"},{"location":"Systemctl_Systemd/#step-4-verify-persistence-after-reboot","title":"\u2705 Step 4: Verify persistence after reboot","text":"<p>After reboot:</p> <pre><code>cat /etc/resolv.conf\nls -l /etc/netplan/\ncat /etc/netplan/01-netcfg.yaml\n</code></pre> <p>You should see:</p> <ul> <li><code>resolv.conf</code> no longer symlinked.</li> <li>DNS pointing to 8.8.8.8 and 8.8.4.4.</li> <li><code>netplan</code> using your custom YAML.</li> <li>Network still working (<code>ping google.com</code> succeeds).</li> </ul>"},{"location":"Systemctl_Systemd/#summary-of-all-changes-to-persist-dns-fix-error","title":"\ud83e\uddfe Summary of all changes (to persist DNS &amp; fix error)","text":"Step Action Purpose Persistent? 1 <code>sudo mkdir -p /etc/cloud/cloud.cfg.d/</code> Ensure override directory exists \u2705 2 <code>echo \"network: {config: disabled}\" &gt; /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg</code> Disable cloud-init network resets \u2705 3 Create <code>/etc/netplan/01-netcfg.yaml</code> Manual static IP + DNS setup \u2705 4 <code>sudo netplan apply</code> Apply new config \u2705 5 Remove symlink <code>/etc/resolv.conf</code> and create static file Fix DNS permanently \u2705 6 <code>sudo systemctl disable systemd-resolved</code> Prevent it from overwriting resolv.conf \u2705 <p>Would you like me to show a minimal \u201ccopy-paste-ready\u201d version of all commands you can run in sequence (no explanations, just working setup)?</p>"},{"location":"Systemctl_Systemd/#after-sudo-netplan-apply-in-the-above-steps-still-got-error-in-dns-name-resolution","title":"After sudo netplan apply in the above steps, still got error in DNS name resolution","text":"<p>After doing sudo netplan apply, below is the output I get , why do I still get failure in name resolution and how to resolve</p> <pre><code>ubuntu@pgt:~$ ping -c 3 8.8.8.8\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=115 time=2.22 ms\n64 bytes from 8.8.8.8: icmp_seq=2 ttl=115 time=2.30 ms\n64 bytes from 8.8.8.8: icmp_seq=3 ttl=115 time=2.43 ms\n\n--- 8.8.8.8 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2002ms\nrtt min/avg/max/mdev = 2.224/2.316/2.425/0.082 ms\nubuntu@pgt:~$ ping -c 3 google.com\nping: google.com: Temporary failure in name resolution\nubuntu@pgt:~$ \n</code></pre> <p>Good observation \u2014 this tells us exactly what\u2019s working and what\u2019s not \ud83d\udc47</p> <ul> <li>\u2705 <code>ping 8.8.8.8</code> works \u2192 your network routing and internet connectivity are fine.</li> <li>\u274c <code>ping google.com</code> fails \u2192 your DNS resolution is broken.</li> </ul> <p>Let\u2019s go step-by-step to fix and persist it properly.</p>"},{"location":"Systemctl_Systemd/#step-1-confirm-what-etcresolvconf-points-to","title":"\ud83e\udde9 Step 1: Confirm what <code>/etc/resolv.conf</code> points to","text":"<p>Run:</p> <pre><code>ls -l /etc/resolv.conf\ncat /etc/resolv.conf\n</code></pre> <p>You will almost certainly see this:</p> <pre><code>/etc/resolv.conf -&gt; ../run/systemd/resolve/stub-resolv.conf\nnameserver 127.0.0.53\n</code></pre> <p>That means DNS queries are still going through the systemd-resolved stub (<code>127.0.0.53</code>), which is not using your Netplan <code>8.8.8.8</code> entries.</p>"},{"location":"Systemctl_Systemd/#step-2-point-resolvconf-to-the-real-dns-configuration","title":"\ud83e\uddf0 Step 2: Point resolv.conf to the real DNS configuration","text":"<p>You can safely replace that symlink with one that points to the correct file managed by <code>systemd-resolved</code>:</p> <pre><code>sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf\n</code></pre> <p>That file (<code>/run/systemd/resolve/resolv.conf</code>) contains the real upstream nameservers that systemd-resolved learned from Netplan.</p> <p>Now check again:</p> <pre><code>cat /etc/resolv.conf\n</code></pre> <p>\u2705 Expected:</p> <pre><code>nameserver 8.8.8.8\nnameserver 8.8.4.4\n</code></pre> <p>Then test:</p> <pre><code>ping -c3 google.com\n</code></pre> <p>If it resolves, DNS is now fixed.</p>"},{"location":"Systemctl_Systemd/#step-3-make-sure-this-survives-reboot","title":"\ud83e\udde9 Step 3: Make sure this survives reboot","text":"<p>You\u2019ve already disabled Cloud-Init network overwriting with:</p> <pre><code>echo \"network: {config: disabled}\" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n</code></pre> <p>and moved <code>50-cloud-init.yaml</code>, so this DNS configuration will persist.</p> <p>The <code>/etc/resolv.conf</code> symlink fix also persists automatically because it\u2019s a filesystem change, not a runtime file.</p>"},{"location":"Systemctl_Systemd/#step-4-optional-verification-after-reboot","title":"\ud83e\udde9 Step 4: Optional verification after reboot","text":"<p>After you reboot:</p> <pre><code>cat /etc/resolv.conf\nresolvectl status\nping -c3 google.com\n</code></pre> <p>All should show Google\u2019s DNS and successful resolution.</p> <p>\u2705 Summary of what was wrong and what fixed it</p> <ul> <li>Problem: <code>/etc/resolv.conf</code> still pointed to the stub at <code>127.0.0.53</code>, ignoring your Netplan DNS.</li> <li>Fix: Replace that symlink to point to <code>/run/systemd/resolve/resolv.conf</code>, which contains your correct DNS servers.</li> </ul> <p>After that, Docker and all other applications should be able to resolve hostnames normally.</p>"},{"location":"VM-to-VM-Communication/","title":"VM-to-VM Communication","text":""},{"location":"VM-to-VM-Communication/#for-vm-to-vm-communication-on-virtualbox","title":"FOR VM TO VM COMMUNICATION ON VIRTUALBOX:","text":"<p>Had to use 2 adapters: NAT - enables internet connection for that VM Host only network - make sure to use the SAME host network for both </p>"},{"location":"Wine/","title":"Wine","text":"<p>How Linux WINE Works Under the Hood :</p> <p>https://www.youtube.com/watch?v=XSZ3UJbHqAk</p> <ul> <li>check about file sharing between wine -&gt; ubuntu -&gt; local pc (eg. MacBook)</li> <li>heck about file sharing between wine -&gt; local pc (eg. MacBook)</li> </ul> <p>https://unix.stackexchange.com/questions/344795/is-there-a-program-like-mobaxterm-for-linux-systems-sftp-gui-browser-termina</p>"},{"location":"Wine/#how-to-install-wine-on-ubuntu-2404-lts-linux-running-windows-programs-on-linux","title":"How to Install Wine on Ubuntu 24.04 LTS Linux | Running Windows Programs on Linux","text":"<p>https://youtu.be/J6EkEdQCNaQ?si=1QM9hzJSIiUMwCYW</p>"}]}